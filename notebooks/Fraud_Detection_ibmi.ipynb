{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b4b46d-c570-4d24-9566-2b60c0bc813d",
   "metadata": {},
   "source": [
    "# 0.) Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad62a8a9-2f39-45b4-a28b-50a10be51393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "from kubernetes.client.models import V1Volume, V1PersistentVolumeClaimVolumeSource\n",
    "import os\n",
    "from pydoc import importfile\n",
    "import requests\n",
    "from tensorflow import keras\n",
    "from typing import List\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f6c05c-583d-42f0-bc43-3b3a8891ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fd6e7\n"
     ]
    }
   ],
   "source": [
    "def generate_unique_hash():\n",
    "    current_time = str(time.time())\n",
    "    m = hashlib.sha256()\n",
    "    m.update(current_time.encode(\"utf-8\"))\n",
    "    return m.hexdigest()[:5]\n",
    "\n",
    "\n",
    "# use static hash for demo purposes\n",
    "unique_hash = \"fd6e7\"\n",
    "print(unique_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "151dd754-277c-4560-867a-03bf54727b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blackboard': 'artefacts',\n",
       " 'model_name': 'fraud-detection-fd6e7',\n",
       " 'cluster_configuration_secret': '',\n",
       " 'training_gpus': '1',\n",
       " 'training_node_selector': ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\"\n",
    "COMPONENT_CATALOG_GIT = \"https://github.com/lehrig/kubeflow-ppc64le-components.git\"\n",
    "COMPONENT_CATALOG_RELEASE = \"main\"\n",
    "\n",
    "load_dataframe_via_trino_comp = kfp.components.load_component_from_file(\n",
    "    os.path.join(\"trino\", \"component.yaml\")\n",
    ")\n",
    "deploy_model_with_kserve_comp = kfp.components.load_component_from_file(\n",
    "    os.path.join(\"kserve\", \"component.yaml\")\n",
    ")\n",
    "\n",
    "ARGUMENTS = {\n",
    "    \"blackboard\": \"artefacts\",\n",
    "    \"model_name\": \"fraud-detection-\" + unique_hash,\n",
    "    \"cluster_configuration_secret\": os.getenv(\n",
    "        \"CLUSTER_CONFIGURATION_SECRET\", default=\"\"\n",
    "    ),\n",
    "    \"training_gpus\": os.getenv(\"TRAINING_GPUS\", default=\"1\"),\n",
    "    \"training_node_selector\": os.getenv(\"TRAINING_NODE_SELECTOR\", default=\"\"),\n",
    "}\n",
    "MODEL_NAME = ARGUMENTS[\"model_name\"]\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "    NAMESPACE = f.read()\n",
    "\n",
    "ARGUMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5a63f-387e-4539-b268-6851eb1e8ab8",
   "metadata": {},
   "source": [
    "# 1.) Load Catalog with Reusable KF components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cffdb8d1-bdac-4cd9-bf9b-6320ea2a03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone --branch $COMPONENT_CATALOG_RELEASE $COMPONENT_CATALOG_GIT $COMPONENT_CATALOG_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28becbc4-ec16-41b1-befc-635e4c11b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = importfile(f\"{COMPONENT_CATALOG_FOLDER}/catalog.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec27553-57ff-4bdb-8f7f-a69719018843",
   "metadata": {},
   "source": [
    "# 2.) Create custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da1582-1fe4-47b0-a602-fdb2f2b1d5b8",
   "metadata": {},
   "source": [
    "## 2.1) Component: Preprocess data (dataset loading, rebalancing & splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b777d42-e9a6-4ea3-89f3-2db8768c0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(\n",
    "    dataframe: InputPath(str),\n",
    "    validation_dataset_dir: OutputPath(str),\n",
    "    train_dataset_dir: OutputPath(str),\n",
    "    dataset_encoder_dir: OutputPath(str),\n",
    ") -> List[str]:\n",
    "\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import dill\n",
    "\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import (\n",
    "        LabelEncoder,\n",
    "        OneHotEncoder,\n",
    "        FunctionTransformer,\n",
    "        MinMaxScaler,\n",
    "        LabelBinarizer,\n",
    "    )\n",
    "    from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "    def save_to_dir(x, y, directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        np.savez(os.path.join(directory, \"data.npz\"), x=x, y=y)\n",
    "\n",
    "    def split_dataset(n, df):\n",
    "        test = df.iloc[:n, :]\n",
    "        train = df.iloc[n:, :]\n",
    "        return test, train\n",
    "\n",
    "    def merge_splits(frauds, non_frauds, split):\n",
    "        print(\n",
    "            f\"{split} ratio fraud ({len(frauds)}) / non-fraud ({len(non_frauds)}):\",\n",
    "            len(frauds) / len(non_frauds),\n",
    "        )\n",
    "        df = pd.concat([frauds, non_frauds])\n",
    "        df.sort_values(\"year_month_day_time\", inplace=True)\n",
    "\n",
    "        x, y = df.drop([\"is fraud?\"], axis=1), df[\"is fraud?\"]\n",
    "        min_ind = math.floor(len(x) / 128)\n",
    "        x, y = x[-min_ind * 128 :], y[-min_ind * 128 :]\n",
    "        y = y.astype(\"int\")\n",
    "        return x, y\n",
    "\n",
    "    def timeEncoder(X):\n",
    "        X_hm = X[\"time\"].str.split(\":\", expand=True)\n",
    "        d = pd.to_datetime(\n",
    "            dict(\n",
    "                year=X[\"year\"],\n",
    "                month=X[\"month\"],\n",
    "                day=X[\"day\"],\n",
    "                hour=X_hm[0],\n",
    "                minute=X_hm[1],\n",
    "            )\n",
    "        ).astype(int)\n",
    "        return pd.DataFrame(d)\n",
    "\n",
    "    def amtEncoder(X):\n",
    "        amt = (\n",
    "            X.apply(lambda x: x[1:])\n",
    "            .astype(float)\n",
    "            .map(lambda amt: max(1, amt))\n",
    "            .map(math.log)\n",
    "        )\n",
    "        return pd.DataFrame(amt)\n",
    "\n",
    "    def decimalEncoder(X, length=5):\n",
    "        dnew = pd.DataFrame()\n",
    "        for i in range(length):\n",
    "            dnew[i] = np.mod(X, 10)\n",
    "            X = np.floor_divide(X, 10)\n",
    "        return dnew\n",
    "\n",
    "    def fraudEncoder(X):\n",
    "        return np.where(X == \"Yes\", 1, 0).astype(int)\n",
    "\n",
    "    def whitespace_remover(dataframe):\n",
    "        # Identify columns with data type 'object'\n",
    "        str_columns = dataframe.select_dtypes([\"object\"]).columns\n",
    "\n",
    "        # Apply strip operation only to string objects, skip others\n",
    "        dataframe[str_columns] = dataframe[str_columns].applymap(\n",
    "            lambda x: x.strip() if isinstance(x, str) else x\n",
    "        )\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    # df_nf = pd.read_csv(f\"{os.getenv('HOME')}/card_transactions_non-frauds.csv\")\n",
    "    # df_f = pd.read_csv(f\"{os.getenv('HOME')}/card_transactions_frauds.csv\")\n",
    "    # tdf = pd.concat([df_nf, df_f])\n",
    "    print(\"read in raw data\")\n",
    "    tdf = pd.read_feather(dataframe).iloc[20:]\n",
    "    tdf.columns = map(str.lower, tdf.columns)\n",
    "    tdf[\"merchant name\"] = tdf[\"merchant name\"].astype(str)\n",
    "    tdf.drop([\"mcc\", \"zip\", \"merchant state\"], axis=1, inplace=True)\n",
    "    tdf.sort_values(by=[\"user\", \"card\"], inplace=True)\n",
    "    tdf.reset_index(inplace=True, drop=True)\n",
    "    whitespace_remover(tdf)\n",
    "\n",
    "    encoders = {\n",
    "        \"merchant_name\": LabelEncoder().fit(tdf[\"merchant name\"]),\n",
    "        \"merchant_city\": LabelEncoder().fit(tdf[\"merchant city\"]),\n",
    "    }\n",
    "\n",
    "    mapper = DataFrameMapper(\n",
    "        [\n",
    "            (\"is fraud?\", FunctionTransformer(fraudEncoder)),\n",
    "            (\n",
    "                \"merchant name\",\n",
    "                [\n",
    "                    encoders[\"merchant_name\"],\n",
    "                    FunctionTransformer(decimalEncoder),\n",
    "                    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "                ],\n",
    "            ),\n",
    "            (\n",
    "                \"merchant city\",\n",
    "                [\n",
    "                    encoders[\"merchant_city\"],\n",
    "                    FunctionTransformer(decimalEncoder),\n",
    "                    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "                ],\n",
    "            ),\n",
    "            ([\"use chip\"], [SimpleImputer(strategy=\"constant\"), LabelBinarizer()]),\n",
    "            ([\"errors?\"], [SimpleImputer(strategy=\"constant\"), LabelBinarizer()]),\n",
    "            (\n",
    "                [\"year\", \"month\", \"day\", \"time\"],\n",
    "                [FunctionTransformer(timeEncoder), MinMaxScaler()],\n",
    "            ),\n",
    "            (\"amount\", [FunctionTransformer(amtEncoder), MinMaxScaler()]),\n",
    "        ],\n",
    "        input_df=True,\n",
    "        df_out=True,\n",
    "    )\n",
    "\n",
    "    print(\"fit and transform dataframe\")\n",
    "    mapper.fit(tdf)\n",
    "    tdf = mapper.transform(tdf)\n",
    "\n",
    "    # save encoders to disk\n",
    "    if not os.path.exists(dataset_encoder_dir):\n",
    "        print(\"creating encoder dir\")\n",
    "        os.makedirs(dataset_encoder_dir)\n",
    "    with open(os.path.join(dataset_encoder_dir, \"mapper.pkl\"), \"wb\") as f:\n",
    "        dill.dump(mapper, f)\n",
    "\n",
    "    print(\"encoders successfully archived\")\n",
    "\n",
    "    dataset = tdf\n",
    "    dataset = dataset.sample(frac=1)  # shuffle randomly\n",
    "\n",
    "    frauds = dataset[dataset[\"is fraud?\"] == 1]\n",
    "    non_frauds = dataset[dataset[\"is fraud?\"] == 0]\n",
    "    ratio = len(frauds) / len(non_frauds)\n",
    "    print(\n",
    "        f\"{len(frauds)} Frauds ({len(frauds)/len(dataset)*100}%) and {len(non_frauds)} Non-Frauds ({len(non_frauds)/len(dataset)*100}%) - ratio: {ratio}).\"\n",
    "    )\n",
    "\n",
    "    test_ratio = 0.1\n",
    "    n_test_frauds = int(test_ratio * len(frauds))\n",
    "    n_test_non_frauds = int(test_ratio * len(non_frauds))\n",
    "    n_train_frauds = len(frauds) - n_test_frauds\n",
    "    n_train_non_frauds = len(non_frauds) - n_test_non_frauds\n",
    "    # n_frauds = int(0.001 * len(dataset))\n",
    "    # n_non_frauds = int(len(dataset) * 0.2 - n_frauds)\n",
    "\n",
    "    print(f\"Frauds in test split: {n_test_frauds}\")\n",
    "    test_frauds, train_frauds = split_dataset(n_test_frauds, frauds)\n",
    "\n",
    "    print(f\"Non-Frauds in test split: {n_test_non_frauds}\")\n",
    "    test_non_frauds, train_non_frauds = split_dataset(n_test_non_frauds, non_frauds)\n",
    "\n",
    "    x_train, y_train = merge_splits(train_frauds, train_non_frauds, \"Train\")\n",
    "    x_test, y_test = merge_splits(test_frauds, test_non_frauds, \"Test\")\n",
    "    print(\n",
    "        f\"Using the following y-label: {y_train.name} and x-features: {x_train.columns}\"\n",
    "    )\n",
    "\n",
    "    over_sampler = RandomOverSampler(random_state=37, sampling_strategy=0.1)\n",
    "    train_input, train_target = over_sampler.fit_resample(x_train, y_train)\n",
    "    # train_input, train_target = x_train, y_train # use this if you don't want to oversample\n",
    "    print(\n",
    "        sum(train_target == 0),\n",
    "        \"negative &\",\n",
    "        sum(train_target == 1),\n",
    "        \"positive training samples (after upsampling)\",\n",
    "    )\n",
    "    print(\n",
    "        sum(y_test == 0),\n",
    "        \"negative &\",\n",
    "        sum(y_test == 1),\n",
    "        \"positive test samples\",\n",
    "    )\n",
    "    train = pd.concat([pd.DataFrame(train_target), pd.DataFrame(train_input)], axis=1)\n",
    "    train.columns = dataset.columns\n",
    "    train.sort_values(\"year_month_day_time\", inplace=True)\n",
    "    train_input, train_target = train.drop([\"is fraud?\"], axis=1), train[\"is fraud?\"]\n",
    "\n",
    "    train_target = train_target.to_numpy().reshape(len(train_target), 1)\n",
    "    y_test = y_test.to_numpy().reshape(len(y_test), 1)\n",
    "\n",
    "    save_to_dir(train_input.to_numpy(), train_target, train_dataset_dir)\n",
    "    save_to_dir(x_test.to_numpy(), y_test, validation_dataset_dir)\n",
    "\n",
    "    print(f\"Pre-processed train dataset saved. Contents of '{train_dataset_dir}':\")\n",
    "    print(os.listdir(\"/\".join(str(train_dataset_dir).split(\"/\")[:-1])))\n",
    "    print(f\"Pre-processed test dataset saved. Contents of '{validation_dataset_dir}':\")\n",
    "    print(os.listdir(\"/\".join(str(validation_dataset_dir).split(\"/\")[:-1])))\n",
    "\n",
    "    print(train_input.columns)\n",
    "    return list(train_input.columns)\n",
    "\n",
    "\n",
    "preprocess_dataset_comp = kfp.components.create_component_from_func(\n",
    "    func=preprocess_dataset,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"imbalanced-learn\", \"scikit-learn\", \"sklearn-pandas\", \"dill\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7979b7-cda8-4ab5-aa07-611c72968b93",
   "metadata": {},
   "source": [
    "## 2.2) Specification: model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa39bb6c-3e8a-47f6-9366-fd089ad97098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model_dir: OutputPath(str),\n",
    "    train_dataset_dir: InputPath(str),\n",
    "    validation_dataset_dir: InputPath(str),\n",
    "    epochs: int = 10,\n",
    "    seqlen: int = 7,\n",
    "):\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.callbacks import (\n",
    "        EarlyStopping,\n",
    "        ModelCheckpoint,\n",
    "        ReduceLROnPlateau,\n",
    "        TensorBoard,\n",
    "    )\n",
    "    from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "    from tensorflow.keras.metrics import (\n",
    "        TruePositives,\n",
    "        FalsePositives,\n",
    "        FalseNegatives,\n",
    "        TrueNegatives,\n",
    "    )\n",
    "\n",
    "    def load_dataset(path):\n",
    "        data = np.load(os.path.join(path, \"data.npz\"), allow_pickle=True)\n",
    "        x, y = data[\"x\"], data[\"y\"]\n",
    "        x = np.asarray(x).astype(np.float32)\n",
    "        y = np.asarray(y).astype(np.int_)\n",
    "        dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            x, y, sequence_length=seqlen, batch_size=128\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    train_dataset = load_dataset(train_dataset_dir)\n",
    "    test_dataset = load_dataset(validation_dataset_dir)\n",
    "\n",
    "    for batch in train_dataset.take(1):\n",
    "        input_d, targets = batch\n",
    "    print(\"Input shape:\", input_d.numpy().shape, \"Target shape:\", targets.numpy().shape)\n",
    "\n",
    "    input_shape = (input_d.shape[1], input_d.shape[2])\n",
    "    inputs = Input(shape=input_shape)\n",
    "    lstm_in = LSTM(100, batch_size=7, return_sequences=True)(inputs)\n",
    "    lstm_out = LSTM(100, batch_size=7)(lstm_in)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(lstm_out)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        TruePositives(name=\"tp\"),\n",
    "        FalsePositives(name=\"fp\"),\n",
    "        FalseNegatives(name=\"fn\"),\n",
    "        TrueNegatives(name=\"tn\"),\n",
    "    ]\n",
    "    # loss = keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=metrics)\n",
    "    print(model.summary())\n",
    "\n",
    "    print(\"Initializing training callbacks...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"loss\", patience=20, verbose=0, mode=\"min\"),\n",
    "        ModelCheckpoint(\n",
    "            f\"{model_dir}/best_model.keras\",\n",
    "            monitor=\"loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.1,\n",
    "            patience=7,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=os.environ[\"TENSORBOARD_S3_ADDRESS\"],\n",
    "            histogram_freq=1,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        verbose=3,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    results = model.evaluate(test_dataset)\n",
    "    print(\"Evaluation Loss, Accuracy, TP, FP, FN, TN:\", results)\n",
    "    TP, FP, FN, TN = results[2:]\n",
    "    if TP != 0:\n",
    "        PR = TP / (FP + TP)\n",
    "        RE = TP / (FN + TP)\n",
    "        print(\"F1 Measure:\", 2 * (PR * RE / (PR + RE)))\n",
    "\n",
    "    model.save(model_dir)\n",
    "\n",
    "\n",
    "train_specification = kfp.components.func_to_component_text(func=train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130c6d0-8e82-4b18-8c10-98b141be4dc4",
   "metadata": {},
   "source": [
    "## 2.3) Component: Prediction on Test Set for Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04224364-f14b-4266-a53c-0439958a5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model_dir: InputPath(str),\n",
    "    test_dataset_dir: InputPath(str),\n",
    "    predictions_dir: OutputPath(str),\n",
    "    seq_len: int = 7,\n",
    "):\n",
    "\n",
    "    from tensorflow import keras\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    data = np.load(os.path.join(test_dataset_dir, \"data.npz\"), allow_pickle=True)\n",
    "    x, y = data[\"x\"], data[\"y\"]\n",
    "    x = np.asarray(x).astype(np.float32)\n",
    "    y = np.asarray(y).astype(np.int_)\n",
    "    test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x, y, sequence_length=seq_len, batch_size=128\n",
    "    )\n",
    "    y = np.concatenate([b for a, b in test_dataset], axis=0)\n",
    "    model = keras.models.load_model(model_dir)\n",
    "    preds = model.predict(test_dataset)\n",
    "    preds = [str(int(pred[0] > 0.5)) + \"\\n\" for pred in preds]\n",
    "    y = [str(x[0].item()) + \"\\n\" for x in y]\n",
    "\n",
    "    if not os.path.exists(predictions_dir):\n",
    "        os.makedirs(predictions_dir)\n",
    "    with open(os.path.join(predictions_dir, \"ytrue.txt\"), \"w\") as f:\n",
    "        f.writelines(y)\n",
    "    with open(os.path.join(predictions_dir, \"ypred.txt\"), \"w\") as f:\n",
    "        f.writelines(preds)\n",
    "\n",
    "\n",
    "predict_comp = kfp.components.create_component_from_func(\n",
    "    func=predict, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d346be3-9c32-4488-99f3-1ad7312c586e",
   "metadata": {},
   "source": [
    "## 2.4) Component: Deploy to Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89954dc9-a320-44a4-aa1a-e9e3bd61bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_to_aix(\n",
    "    model_version: int = 1,\n",
    "):\n",
    "    import requests\n",
    "\n",
    "    print(\"Updating model at AIX...\")\n",
    "    response = requests.get(\n",
    "        f\"http://p114oracle.pbm.ihost.com:3000/update?model_version={model_version}\"\n",
    "    )\n",
    "    print(response.text)\n",
    "\n",
    "\n",
    "deploy_to_aix_comp = kfp.components.create_component_from_func(\n",
    "    func=deploy_to_aix, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427ce9a-4ce3-4f65-bb62-e2d5d6a8a8c9",
   "metadata": {},
   "source": [
    "# 3.) Create the actual pipeline by combining the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2d06d93-2fea-466a-b491-fb1e4c83fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"IBMI-Fraud-Detection\"\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"Fraud detection\",\n",
    "    description=\"An example pipeline that tries to predict fraudulent credit card transactions\",\n",
    ")\n",
    "def fraud_pipeline(\n",
    "    blackboard: str,\n",
    "    model_name: str,\n",
    "    cluster_configuration_secret: str,\n",
    "    training_gpus: int,\n",
    "    training_node_selector: str,\n",
    "):\n",
    "    create_blackboard = dsl.VolumeOp(\n",
    "        name=\"Create Artefacts Blackboard\",\n",
    "        resource_name=blackboard,\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    load_dataframe_via_trino_task = load_dataframe_via_trino_comp(\n",
    "        query=\"SELECT * FROM  jtopen.demo.fraud limit 1000000\",\n",
    "        columns=None,\n",
    "        columns_query=\"SHOW COLUMNS FROM jtopen.demo.fraud\",\n",
    "    )\n",
    "    load_dataframe_via_trino_task.after(create_blackboard)\n",
    "\n",
    "    CATALOG.create_dataset_quality_report(\n",
    "        dataset_dir=load_dataframe_via_trino_task.outputs[\"dataframe\"],\n",
    "        dataset_type=\"df/feather\",\n",
    "    )\n",
    "\n",
    "    preprocess_dataset_task = preprocess_dataset_comp(\n",
    "        dataframe=load_dataframe_via_trino_task.outputs[\"dataframe\"]\n",
    "    )\n",
    "\n",
    "    monitor_training_task = CATALOG.monitor_training_comp()\n",
    "\n",
    "    train_parameters = {\n",
    "        \"train_dataset_dir\": \"train_dataset_dir\",\n",
    "        \"validation_dataset_dir\": \"validation_dataset_dir\",\n",
    "        \"model_dir\": \"model_dir\",\n",
    "        \"epochs\": \"2\",\n",
    "        \"seqlen\": \"4\",\n",
    "    }\n",
    "\n",
    "    train_model_task = CATALOG.train_model_comp(\n",
    "        preprocess_dataset_task.outputs[\"train_dataset_dir\"],\n",
    "        preprocess_dataset_task.outputs[\"validation_dataset_dir\"],\n",
    "        train_specification,\n",
    "        train_parameters,\n",
    "        model_name=model_name,\n",
    "        gpus=training_gpus,\n",
    "        node_selector=training_node_selector,\n",
    "        tensorboard_s3_address=monitor_training_task.outputs[\"tensorboard_s3_address\"],\n",
    "        cluster_configuration_secret=cluster_configuration_secret,\n",
    "    )\n",
    "\n",
    "    # plot_confusion_matrix_task = CATALOG.plot_confusion_matrix_comp(\n",
    "    #    input_columns=preprocess_dataset_task.outputs[\"output\"],\n",
    "    #    label_columns={\"is fraud\": [0, 1]},\n",
    "    #    test_dataset_dir=preprocess_dataset_task.outputs[\"validation_dataset_dir\"],\n",
    "    #    model_dir=train_model_task.outputs[\"model_dir\"],\n",
    "    #    seq_len=int(train_parameters[\"seqlen\"]),\n",
    "    # )\n",
    "\n",
    "    predict_task = predict_comp(\n",
    "        test_dataset_dir=preprocess_dataset_task.outputs[\"validation_dataset_dir\"],\n",
    "        model_dir=train_model_task.outputs[\"model_dir\"],\n",
    "    )\n",
    "\n",
    "    CATALOG.plot_confusion_matrix_predictions_comp(\n",
    "        predictions_dir=predict_task.outputs[\"predictions_dir\"]\n",
    "    )\n",
    "\n",
    "    convert_model_to_onnx_task = CATALOG.convert_model_to_onnx_comp(\n",
    "        train_model_task.outputs[\"model_dir\"]\n",
    "    )\n",
    "\n",
    "    upload_model_task = CATALOG.upload_model_comp(\n",
    "        file_dir=convert_model_to_onnx_task.outputs[\"onnx_model_dir\"],\n",
    "        project_name=model_name,\n",
    "    )\n",
    "\n",
    "    deploy_model_with_kserve_comp(\n",
    "        project_name=model_name,\n",
    "        model_version=upload_model_task.outputs[\"model_version\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# deploy_to_aix_comp(upload_model_task.outputs[\"model_version\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b2ffe-b2b5-4972-a645-a041e8162161",
   "metadata": {},
   "source": [
    "# 4.) Run the pipeline within an experiment\n",
    "reate a pipeline run, using a pipeline configuration that:\n",
    "\n",
    "- enables data passing via persistent volumes (faster than the default MinIO-based passing)\n",
    "- disables caching (which currently is not supported for data passing via volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7296f15-54c9-436e-bda7-32b9db101840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_cache_transformer(op):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf = PipelineConf()\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n",
    "# pipeline_conf.data_passing_method = data_passing_methods.KubernetesVolume(\n",
    "#     volume=V1Volume(\n",
    "#         name=ARGUMENTS[\"blackboard\"],\n",
    "#         persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "#             \"{{workflow.name}}-%s\" % ARGUMENTS[\"blackboard\"]\n",
    "#         ),\n",
    "#     ),\n",
    "#     path_prefix=f'{ARGUMENTS[\"blackboard\"]}/',\n",
    "# )\n",
    "\n",
    "# kfp.Client().create_run_from_pipeline_func(\n",
    "#     fraud_pipeline,\n",
    "#     arguments=ARGUMENTS,\n",
    "#     namespace=NAMESPACE,\n",
    "#     pipeline_conf=pipeline_conf,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df65011-6a1d-4d11-853a-01d6b159a667",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa809d8a-3cd3-4383-81c9-cb69bb139d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=fraud_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad8d62-2313-4719-809f-8bb9b38d5149",
   "metadata": {},
   "source": [
    "## Upload Pipeline\n",
    "\n",
    "We can only have one pipeline with a specific name; in case we ran this script multiple times we will delete the pipeline first if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "441d1608-178c-4ffe-8141-65478c368ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e56ef688-9285-40f9-afb9-c3f9396019ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/6df2c704-8e98-4150-85f1-118d916b48a3>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a27451-b24b-4350-aca6-02f4f05866ac",
   "metadata": {},
   "source": [
    "## Create Pipeline Run\n",
    "\n",
    "Create a pipeline run\n",
    "\n",
    "This creates a run of the pipeline. As with uploading, we could do this manually using the UI.\n",
    "\n",
    "Runnning a pipeline requires a KFP experiment. This code will create the experiment if it does not already exist.\n",
    "\n",
    "Grouping runs into an experiment is advantageous, since it allows us to compare metrics between pipeline runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ec8f2eb-d2d0-48f7-b020-fba5c352795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eba9006c-41bc-40ec-9291-5a14e0de99e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/4b1d5850-eb8f-4e43-8664-71cedd80bd26\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"ibmi-fraud\"),\n",
    "    job_name=\"ibmi-fraud\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    "    params=ARGUMENTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e8158-24f0-4ed9-ab64-99f16c5c3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp_server_api import ApiException\n",
    "\n",
    "\n",
    "def get_experiment_id(client, experiment_name):\n",
    "    \"\"\"Helper function to get the experiment ID given an experiment name.\"\"\"\n",
    "    try:\n",
    "        experiment = client.get_experiment(experiment_name=experiment_name)\n",
    "        return experiment.id\n",
    "    except ApiException as e:\n",
    "        print(f\"Error obtaining experiment ID for {experiment_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_latest_run_id(client, experiment_id):\n",
    "    \"\"\"Retrieve the latest run ID from an experiment.\"\"\"\n",
    "    try:\n",
    "        # Fetch all runs for the specified experiment ID\n",
    "        runs = client.list_runs(\n",
    "            experiment_id=experiment_id, sort_by=\"created_at desc\", page_size=1\n",
    "        )\n",
    "        if runs.runs:\n",
    "            latest_run = runs.runs[0]\n",
    "            return latest_run.id\n",
    "        else:\n",
    "            print(\"No runs found for this experiment.\")\n",
    "            return None\n",
    "    except ApiException as e:\n",
    "        print(f\"Exception when calling KfpApi->list_runs: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_pipeline_output_artifacts(client, run_id):\n",
    "    \"\"\"Fetch and print pipeline output artifacts given a run ID.\"\"\"\n",
    "    try:\n",
    "        run_detail = client.get_run(run_id).run.to_dict()\n",
    "        if run_detail:\n",
    "\n",
    "            workflow_manifest = json.loads(\n",
    "                run_detail[\"pipeline_spec\"][\"workflow_manifest\"]\n",
    "            )\n",
    "            print(workflow_manifest[\"spec\"])\n",
    "\n",
    "    except ApiException as e:\n",
    "        print(f\"Exception when calling KfpApi->get_run: {e}\")\n",
    "\n",
    "\n",
    "experiment_id = get_experiment_id(client, \"ibmi-fraud\")\n",
    "# Get the latest run ID for the experiment\n",
    "latest_run_id = get_latest_run_id(client, experiment_id)\n",
    "# Get the latest run ID for the experiment\n",
    "latest_run_id = get_latest_run_id(client, experiment_id)\n",
    "if latest_run_id:\n",
    "    # Retrieve artifacts from the latest run\n",
    "    get_pipeline_output_artifacts(client, latest_run_id)\n",
    "else:\n",
    "    print(\"Failed to retrieve the latest run ID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7e4f0-2fe4-42ce-9565-5d44361ab424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
