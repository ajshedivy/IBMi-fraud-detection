apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: fraud-detection-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2024-04-23T18:13:50.012579',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example pipeline that
      tries to predict fraudulent credit card transactions", "inputs": [{"name": "blackboard",
      "type": "String"}, {"name": "model_name", "type": "String"}, {"name": "cluster_configuration_secret",
      "type": "String"}, {"name": "training_gpus", "type": "Integer"}, {"name": "training_node_selector",
      "type": "String"}], "name": "Fraud detection"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: fraud-detection
  templates:
  - name: convert-model-to-onnx
    container:
      args: [--model-dir, /tmp/inputs/model_dir/data, --onnx-model-dir, /tmp/outputs/onnx_model_dir/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def Convert_Model_to_ONNX(model_dir, onnx_model_dir):
            """Converts a model to ONNX format. Supported input formats: Keras."""

            import logging
            import onnx
            import sys
            import tensorflow as tf
            import tf2onnx

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )
            logger = logging.getLogger()

            logger.info(f"Loading model from '{model_dir}'...")
            keras_model = tf.keras.models.load_model(model_dir)

            logger.info("Converting model to ONNX...")
            converted_model, _ = tf2onnx.convert.from_keras(keras_model)

            logger.info(f"Saving ONNX model to '{onnx_model_dir}'...")
            onnx.save_model(converted_model, onnx_model_dir)

            logger.info("Finished.")

        import argparse
        _parser = argparse.ArgumentParser(prog='Convert Model to ONNX', description='Converts a model to ONNX format. Supported input formats: Keras.')
        _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--onnx-model-dir", dest="onnx_model_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = Convert_Model_to_ONNX(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      artifacts:
      - {name: train-model-job-model_dir, path: /tmp/inputs/model_dir/data}
    outputs:
      artifacts:
      - {name: convert-model-to-onnx-onnx_model_dir, path: /tmp/outputs/onnx_model_dir/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Converts
          a model to ONNX format. Supported input formats: Keras.", "implementation":
          {"container": {"args": ["--model-dir", {"inputPath": "model_dir"}, "--onnx-model-dir",
          {"outputPath": "onnx_model_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef Convert_Model_to_ONNX(model_dir,
          onnx_model_dir):\n    \"\"\"Converts a model to ONNX format. Supported input
          formats: Keras.\"\"\"\n\n    import logging\n    import onnx\n    import
          sys\n    import tensorflow as tf\n    import tf2onnx\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    logger.info(f\"Loading
          model from ''{model_dir}''...\")\n    keras_model = tf.keras.models.load_model(model_dir)\n\n    logger.info(\"Converting
          model to ONNX...\")\n    converted_model, _ = tf2onnx.convert.from_keras(keras_model)\n\n    logger.info(f\"Saving
          ONNX model to ''{onnx_model_dir}''...\")\n    onnx.save_model(converted_model,
          onnx_model_dir)\n\n    logger.info(\"Finished.\")\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Convert Model to ONNX'', description=''Converts
          a model to ONNX format. Supported input formats: Keras.'')\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--onnx-model-dir\",
          dest=\"onnx_model_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = Convert_Model_to_ONNX(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}},
          "inputs": [{"name": "model_dir", "type": "String"}], "name": "Convert Model
          to ONNX", "outputs": [{"name": "onnx_model_dir", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "eac66176d38b100b351b461d5829c35958553462575514adfe08910bf7a85598", "url":
          "/home/jovyan/components/model-building/convert-to-onnx/component.yaml"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: create-artefacts-blackboard
    resource:
      action: create
      setOwnerReference: true
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-{{inputs.parameters.blackboard}}'
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 4Gi
    inputs:
      parameters:
      - {name: blackboard}
    outputs:
      parameters:
      - name: create-artefacts-blackboard-manifest
        valueFrom: {jsonPath: '{}'}
      - name: create-artefacts-blackboard-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: create-artefacts-blackboard-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
    metadata:
      annotations: {pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: create-data-quality-report-with-evidently
    container:
      args: [--dataset-dir, /tmp/inputs/dataset_dir/data, --dataset-type, df/feather,
        --max-rows, '10000', --output-dir, /tmp/outputs/output_dir/data, --mlpipeline-ui-metadata,
        /tmp/outputs/mlpipeline_ui_metadata/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'evidently==0.3.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'evidently==0.3.1' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def Create_Data_Quality_Report_with_Evidently(
            dataset_dir,
            output_dir,
            mlpipeline_ui_metadata_path,
            dataset_type="df",
            ref_dataset_dir = None,
            additional_args = None,
            column_mapping = None,
            max_rows = 10000,
        ):
            """
            Generate a data quality report for a given dataset using Evidently AI.

            Args:
                dataset_dir (str): Path to the directory containing the dataset.
                output_dir (str): Path to the directory where the report HTML file will be saved.
                mlpipeline_ui_metadata_path (str): Path to the file where the metadata for the ML Pipeline UI will be saved.
                dataset_type (str, optional): Type of the dataset. Must be one of 'csv', 'df' (dataframe via pickle), 'df/feather' (dataframe via feather), or 'huggingface'.
                    Defaults to 'df'.
                ref_dataset_dir (str, optional): Path to the directory containing a reference dataset for comparison.
                    Defaults to None.
                additional_args (dict, optional): Additional arguments to be passed to the dataset processing function.
                    Defaults to None.
                column_mapping (dict, optional): Mapping of columns between the current and reference datasets.
                    Defaults to None.
                max_rows (int, optional): Maximum number of rows to be used for producing the report. If dataset is larger, the report only selects the first max_rows rows of the dataset.

            Returns:
                None: The function saves the report HTML file and metadata to disk.

            Raises:
                KeyError: If the `dataset_type` argument is not one of the supported datatypes stated in the dataset_type parameter.
            """
            from datasets import Array2D, Image, load_from_disk, Value
            from evidently.metric_preset import DataQualityPreset
            from evidently.report import Report
            import hashlib
            import json
            import logging
            import pandas as pd
            from pathlib import Path
            import sys

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )

            def _process_dataframe(dataset_dir, opt_args=None):
                return pd.read_pickle(dataset_dir, **opt_args)

            def _process_dataframe_feather(dataset_dir, opt_args=None):
                return pd.read_feather(dataset_dir, **opt_args)

            def _process_csv(dataset_dir, opt_args=None):
                return pd.read_csv(dataset_dir, **opt_args)

            def _process_huggingface(dataset_dir, opt_args=None):
                dataset = load_from_disk(dataset_dir)

                if (opt_args is not None) and ("split" in opt_args):
                    logging.info(f"Selecting split '{opt_args['split']}'...")
                    dataset = dataset[opt_args["split"]]

                def to_hash(encoded_text):
                    hash_object = hashlib.md5(encoded_text)
                    return hash_object.hexdigest()

                def list_to_str(examples):
                    for key in arrays:
                        examples[key] = [
                            to_hash("".join(str(value) for value in a_list).encode())
                            for a_list in examples[key]
                        ]

                    for key in images:
                        examples[key] = [
                            to_hash(an_image.tobytes()) for an_image in examples[key]
                        ]
                    return examples

                arrays = set()
                images = set()
                for key, feature in dataset.features.items():
                    if isinstance(feature, Array2D):
                        arrays.add(key)
                    if isinstance(feature, Image):
                        images.add(key)

                copy_of_features = dataset.features.copy()
                for feature in arrays:
                    copy_of_features[feature] = Value(dtype="string", id=None)
                for feature in images:
                    copy_of_features[feature] = Value(dtype="string", id=None)

                string_dataset = dataset.map(
                    list_to_str,
                    batched=True,
                    batch_size=100,
                    num_proc=1,
                    keep_in_memory=True,
                    features=copy_of_features,
                )
                return string_dataset.to_pandas()

            DATA_TYPES = {
                "csv": _process_csv,
                "df": _process_dataframe,
                "df/feather": _process_dataframe_feather,
                "huggingface": _process_huggingface,
            }

            def process_dataset(dataset_dir, args=None):
                _dataset_type = dataset_type.lower()

                if dataset_dir is None:
                    return None
                if _dataset_type not in DATA_TYPES.keys():
                    raise KeyError(
                        f"Dataset type {_dataset_type} not supported by the data quality component"
                    )

                dataset = DATA_TYPES[_dataset_type](dataset_dir, (args or {}))

                if max_rows is not None:
                    dataset = dataset.head(max_rows)
                return dataset

            logging.info("Preparing datasets for data quality report...")
            df = process_dataset(dataset_dir, args=additional_args)
            ref_data = process_dataset(ref_dataset_dir, args=additional_args)

            report = Report(metrics=[DataQualityPreset()])

            logging.info("Generating report using Evidently...")
            report.run(current_data=df, reference_data=ref_data, column_mapping=column_mapping)

            logging.info("Saving report as HTML...")
            Path(output_dir).parent.mkdir(parents=True, exist_ok=True)
            report.save_html(output_dir)

            logging.info("Writing HTML content to Metadata UI...")
            html_content = open(output_dir, "r").read()
            metadata = {
                "outputs": [
                    {
                        "type": "web-app",
                        "storage": "inline",
                        "source": html_content,
                    }
                ]
            }

            with open(mlpipeline_ui_metadata_path, "w") as f:
                json.dump(metadata, f)

            logging.info("Finished.")

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Create Data Quality Report with Evidently', description='Generate a data quality report for a given dataset using Evidently AI.')
        _parser.add_argument("--dataset-dir", dest="dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--dataset-type", dest="dataset_type", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--ref-dataset-dir", dest="ref_dataset_dir", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--additional-args", dest="additional_args", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--column-mapping", dest="column_mapping", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--max-rows", dest="max_rows", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--output-dir", dest="output_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = Create_Data_Quality_Report_with_Evidently(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      artifacts:
      - {name: load-dataframe-via-trino-dataframe, path: /tmp/inputs/dataset_dir/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: create-data-quality-report-with-evidently-output_dir, path: /tmp/outputs/output_dir/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Generate
          a data quality report for a given dataset using Evidently AI.", "implementation":
          {"container": {"args": ["--dataset-dir", {"inputPath": "dataset_dir"}, {"if":
          {"cond": {"isPresent": "dataset_type"}, "then": ["--dataset-type", {"inputValue":
          "dataset_type"}]}}, {"if": {"cond": {"isPresent": "ref_dataset_dir"}, "then":
          ["--ref-dataset-dir", {"inputPath": "ref_dataset_dir"}]}}, {"if": {"cond":
          {"isPresent": "additional_args"}, "then": ["--additional-args", {"inputValue":
          "additional_args"}]}}, {"if": {"cond": {"isPresent": "column_mapping"},
          "then": ["--column-mapping", {"inputValue": "column_mapping"}]}}, {"if":
          {"cond": {"isPresent": "max_rows"}, "then": ["--max-rows", {"inputValue":
          "max_rows"}]}}, "--output-dir", {"outputPath": "output_dir"}, "--mlpipeline-ui-metadata",
          {"outputPath": "mlpipeline_ui_metadata"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''evidently==0.3.1''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''evidently==0.3.1'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef Create_Data_Quality_Report_with_Evidently(\n    dataset_dir,\n    output_dir,\n    mlpipeline_ui_metadata_path,\n    dataset_type=\"df\",\n    ref_dataset_dir
          = None,\n    additional_args = None,\n    column_mapping = None,\n    max_rows
          = 10000,\n):\n    \"\"\"\n    Generate a data quality report for a given
          dataset using Evidently AI.\n\n    Args:\n        dataset_dir (str): Path
          to the directory containing the dataset.\n        output_dir (str): Path
          to the directory where the report HTML file will be saved.\n        mlpipeline_ui_metadata_path
          (str): Path to the file where the metadata for the ML Pipeline UI will be
          saved.\n        dataset_type (str, optional): Type of the dataset. Must
          be one of ''csv'', ''df'' (dataframe via pickle), ''df/feather'' (dataframe
          via feather), or ''huggingface''.\n            Defaults to ''df''.\n        ref_dataset_dir
          (str, optional): Path to the directory containing a reference dataset for
          comparison.\n            Defaults to None.\n        additional_args (dict,
          optional): Additional arguments to be passed to the dataset processing function.\n            Defaults
          to None.\n        column_mapping (dict, optional): Mapping of columns between
          the current and reference datasets.\n            Defaults to None.\n        max_rows
          (int, optional): Maximum number of rows to be used for producing the report.
          If dataset is larger, the report only selects the first max_rows rows of
          the dataset.\n\n    Returns:\n        None: The function saves the report
          HTML file and metadata to disk.\n\n    Raises:\n        KeyError: If the
          `dataset_type` argument is not one of the supported datatypes stated in
          the dataset_type parameter.\n    \"\"\"\n    from datasets import Array2D,
          Image, load_from_disk, Value\n    from evidently.metric_preset import DataQualityPreset\n    from
          evidently.report import Report\n    import hashlib\n    import json\n    import
          logging\n    import pandas as pd\n    from pathlib import Path\n    import
          sys\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n\n    def _process_dataframe(dataset_dir,
          opt_args=None):\n        return pd.read_pickle(dataset_dir, **opt_args)\n\n    def
          _process_dataframe_feather(dataset_dir, opt_args=None):\n        return
          pd.read_feather(dataset_dir, **opt_args)\n\n    def _process_csv(dataset_dir,
          opt_args=None):\n        return pd.read_csv(dataset_dir, **opt_args)\n\n    def
          _process_huggingface(dataset_dir, opt_args=None):\n        dataset = load_from_disk(dataset_dir)\n\n        if
          (opt_args is not None) and (\"split\" in opt_args):\n            logging.info(f\"Selecting
          split ''{opt_args[''split'']}''...\")\n            dataset = dataset[opt_args[\"split\"]]\n\n        def
          to_hash(encoded_text):\n            hash_object = hashlib.md5(encoded_text)\n            return
          hash_object.hexdigest()\n\n        def list_to_str(examples):\n            for
          key in arrays:\n                examples[key] = [\n                    to_hash(\"\".join(str(value)
          for value in a_list).encode())\n                    for a_list in examples[key]\n                ]\n\n            for
          key in images:\n                examples[key] = [\n                    to_hash(an_image.tobytes())
          for an_image in examples[key]\n                ]\n            return examples\n\n        arrays
          = set()\n        images = set()\n        for key, feature in dataset.features.items():\n            if
          isinstance(feature, Array2D):\n                arrays.add(key)\n            if
          isinstance(feature, Image):\n                images.add(key)\n\n        copy_of_features
          = dataset.features.copy()\n        for feature in arrays:\n            copy_of_features[feature]
          = Value(dtype=\"string\", id=None)\n        for feature in images:\n            copy_of_features[feature]
          = Value(dtype=\"string\", id=None)\n\n        string_dataset = dataset.map(\n            list_to_str,\n            batched=True,\n            batch_size=100,\n            num_proc=1,\n            keep_in_memory=True,\n            features=copy_of_features,\n        )\n        return
          string_dataset.to_pandas()\n\n    DATA_TYPES = {\n        \"csv\": _process_csv,\n        \"df\":
          _process_dataframe,\n        \"df/feather\": _process_dataframe_feather,\n        \"huggingface\":
          _process_huggingface,\n    }\n\n    def process_dataset(dataset_dir, args=None):\n        _dataset_type
          = dataset_type.lower()\n\n        if dataset_dir is None:\n            return
          None\n        if _dataset_type not in DATA_TYPES.keys():\n            raise
          KeyError(\n                f\"Dataset type {_dataset_type} not supported
          by the data quality component\"\n            )\n\n        dataset = DATA_TYPES[_dataset_type](dataset_dir,
          (args or {}))\n\n        if max_rows is not None:\n            dataset =
          dataset.head(max_rows)\n        return dataset\n\n    logging.info(\"Preparing
          datasets for data quality report...\")\n    df = process_dataset(dataset_dir,
          args=additional_args)\n    ref_data = process_dataset(ref_dataset_dir, args=additional_args)\n\n    report
          = Report(metrics=[DataQualityPreset()])\n\n    logging.info(\"Generating
          report using Evidently...\")\n    report.run(current_data=df, reference_data=ref_data,
          column_mapping=column_mapping)\n\n    logging.info(\"Saving report as HTML...\")\n    Path(output_dir).parent.mkdir(parents=True,
          exist_ok=True)\n    report.save_html(output_dir)\n\n    logging.info(\"Writing
          HTML content to Metadata UI...\")\n    html_content = open(output_dir, \"r\").read()\n    metadata
          = {\n        \"outputs\": [\n            {\n                \"type\": \"web-app\",\n                \"storage\":
          \"inline\",\n                \"source\": html_content,\n            }\n        ]\n    }\n\n    with
          open(mlpipeline_ui_metadata_path, \"w\") as f:\n        json.dump(metadata,
          f)\n\n    logging.info(\"Finished.\")\n\nimport json\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Create Data Quality Report with Evidently'',
          description=''Generate a data quality report for a given dataset using Evidently
          AI.'')\n_parser.add_argument(\"--dataset-dir\", dest=\"dataset_dir\", type=str,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataset-type\",
          dest=\"dataset_type\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--ref-dataset-dir\",
          dest=\"ref_dataset_dir\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--additional-args\",
          dest=\"additional_args\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--column-mapping\",
          dest=\"column_mapping\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--max-rows\",
          dest=\"max_rows\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-dir\",
          dest=\"output_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = Create_Data_Quality_Report_with_Evidently(**_parsed_args)\n"], "image":
          "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}}, "inputs": [{"description":
          "Path to the directory containing the dataset.", "name": "dataset_dir",
          "type": "String"}, {"default": "df", "description": "Type of the dataset.
          Must be one of ''csv'', ''df'' (dataframe via pickle), ''df/feather'' (dataframe
          via feather), or ''huggingface''.\nDefaults to ''df''.", "name": "dataset_type",
          "optional": true}, {"description": "Path to the directory containing a reference
          dataset for comparison.\nDefaults to None.", "name": "ref_dataset_dir",
          "optional": true, "type": "String"}, {"description": "Additional arguments
          to be passed to the dataset processing function.\nDefaults to None.", "name":
          "additional_args", "optional": true, "type": "JsonObject"}, {"description":
          "Mapping of columns between the current and reference datasets.\nDefaults
          to None.", "name": "column_mapping", "optional": true, "type": "JsonObject"},
          {"default": "10000", "description": "Maximum number of rows to be used for
          producing the report. If dataset is larger, the report only selects the
          first max_rows rows of the dataset.", "name": "max_rows", "optional": true,
          "type": "Integer"}], "name": "Create Data Quality Report with Evidently",
          "outputs": [{"description": "Path to the directory where the report HTML
          file will be saved.", "name": "output_dir", "type": "String"}, {"description":
          "Path to the file where the metadata for the ML Pipeline UI will be saved.",
          "name": "mlpipeline_ui_metadata"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "1ccdc66464f5b7a71fbda408fe66cf0766dfc597173f5cb47884eccf634dc590", "url":
          "/home/jovyan/components/monitoring/create-data-quality-report-with-evidently/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"dataset_type": "df/feather",
          "max_rows": "10000"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: deploy-model-with-kserve
    container:
      args: [--project-name, '{{inputs.parameters.model_name}}', --model-version,
        '{{inputs.parameters.upload-model-model_version}}', --kserve-version, v1beta1,
        --s3-bucket, projects, '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def deploy_model_with_kserve(
            project_name,
            model_version,
            explainer_type = None,
            kserve_version = "v1beta1",
            s3_bucket = "projects",
        ):
            """
            Deploys a model using KServe and Trino as backend.

                    Parameters:
                            project_name: Name of the project. Must be unique for the targeted namespace and conform Kubernetes naming conventions. Example: my-model.
                            explainer_type: Type of Alibi explanation. If None, explanations are not provided. Example: AnchorTabular.
                            kserve_version: KServe API version. Example: v1beta1.
                            model_version: Version of the deployed model. Relevant to match explainer version to model version. Example: 1.
                            s3_bucket: Name of the s3 bucket in which model projects reside. Example: projects.
                    Returns:
                            endpoint: REST endpoint where the model can be queried. Example: https://my-model-user-example-com.apps.myorg.com.
            """
            from kubernetes import client, config
            from kserve import KServeClient
            from kserve import constants
            from kserve import utils
            from kserve import V1beta1AlibiExplainerSpec
            from kserve import V1beta1ExplainerSpec
            from kserve import V1beta1InferenceService
            from kserve import V1beta1InferenceServiceSpec
            from kserve import V1beta1PredictorSpec
            from kserve import V1beta1TritonSpec
            import logging
            import sys

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )

            try:
                model_version = int(model_version)
            except ValueError:
                logging.warning(
                    "Could not parse model version. Continuing with default value 1..."
                )
                model_version = 1

            # See: https://www.kubeflow.org/docs/external-add-ons/kserve/first_isvc_kserve/
            logging.info("Initializing environment...")
            config.load_incluster_config()
            namespace = utils.get_default_target_namespace()
            api_version = constants.KSERVE_GROUP + "/" + kserve_version
            storage_uri: str = f"s3://{s3_bucket}/{project_name}"

            logging.info("Initializing inference service specification...")
            resources_spec = client.V1ResourceRequirements(
                requests={"cpu": "1000m", "memory": "8Gi"},
                limits={"cpu": "2000m", "memory": "16Gi"},
            )

            # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1TritonSpec/
            triton_spec = V1beta1TritonSpec(
                args=["--strict-model-config=false"],
                runtime_version="22.03-py3",
                storage_uri=storage_uri,
                resources=resources_spec,
            )

            # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1PredictorSpec/
            predictor_spec = V1beta1PredictorSpec(
                service_account_name="kserve-inference-sa", triton=triton_spec
            )

            if explainer_type:
                print("Found an explainer, which will be co-deployed.")
                # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1AlibiExplainerSpec/
                alibi_spec = V1beta1AlibiExplainerSpec(
                    type=explainer_type,
                    storage_uri=f"{storage_uri}/explainer/{model_version}",  # /explainer.alibi",
                    resources=resources_spec,
                )

                # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1ExplainerSpec/
                explainer_spec = V1beta1ExplainerSpec(
                    min_replicas=1,
                    alibi=alibi_spec,
                )

            # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1InferenceServiceSpec/#properties
            inference_service_spec = V1beta1InferenceService(
                api_version=api_version,
                kind=constants.KSERVE_KIND,
                metadata=client.V1ObjectMeta(
                    name=project_name,
                    namespace=namespace,
                    annotations={"sidecar.istio.io/inject": "false"},
                ),
                spec=V1beta1InferenceServiceSpec(
                    predictor=predictor_spec,
                    explainer=explainer_spec if explainer_type else None,
                ),
            )

            kserve_client = KServeClient()

            logging.info("Checking for existing inference service...")
            try:
                inference_service = kserve_client.get(project_name, namespace=namespace)
                logging.info(f"Received: {inference_service}")

                if "status" in inference_service:
                    logging.info("Inference service already exists.")

                    logging.info("Patching inference service with new model version...")
                    kserve_client.patch(project_name, inference_service_spec)
                else:
                    logging.info("Creating inference service...")
                    kserve_client.create(inference_service_spec)
            except Exception:
                logging.info("Creating new inference service...")
                kserve_client.create(inference_service_spec)

            logging.info("Waiting for inference service to start...")
            kserve_client.get(
                project_name, namespace=namespace, watch=True, timeout_seconds=180
            )

            logging.info("Getting inference URL...")
            inference_response = kserve_client.get(project_name, namespace=namespace)
            inference_url = inference_response["status"]["address"]["url"]
            logging.info(f"inference URL: {inference_url}")

            logging.info("Finished.")
            return inference_url

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Deploy model with kserve', description='Deploys a model using KServe and Trino as backend.')
        _parser.add_argument("--project-name", dest="project_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-version", dest="model_version", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--explainer-type", dest="explainer_type", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--kserve-version", dest="kserve_version", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--s3-bucket", dest="s3_bucket", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = deploy_model_with_kserve(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: quay.io/ntlawrence/demo-kserve@sha256:6803755ecffe42bfb38a703e1d4e453c088bae64096fc8857baa663b526d8978
    inputs:
      parameters:
      - {name: model_name}
      - {name: upload-model-model_version}
    outputs:
      artifacts:
      - {name: deploy-model-with-kserve-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Deploys
          a model using KServe and Trino as backend.", "implementation": {"container":
          {"args": ["--project-name", {"inputValue": "project_name"}, "--model-version",
          {"inputValue": "model_version"}, {"if": {"cond": {"isPresent": "explainer_type"},
          "then": ["--explainer-type", {"inputValue": "explainer_type"}]}}, {"if":
          {"cond": {"isPresent": "kserve_version"}, "then": ["--kserve-version", {"inputValue":
          "kserve_version"}]}}, {"if": {"cond": {"isPresent": "s3_bucket"}, "then":
          ["--s3-bucket", {"inputValue": "s3_bucket"}]}}, "----output-paths", {"outputPath":
          "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          deploy_model_with_kserve(\n    project_name,\n    model_version,\n    explainer_type
          = None,\n    kserve_version = \"v1beta1\",\n    s3_bucket = \"projects\",\n):\n    \"\"\"\n    Deploys
          a model using KServe and Trino as backend.\n\n            Parameters:\n                    project_name:
          Name of the project. Must be unique for the targeted namespace and conform
          Kubernetes naming conventions. Example: my-model.\n                    explainer_type:
          Type of Alibi explanation. If None, explanations are not provided. Example:
          AnchorTabular.\n                    kserve_version: KServe API version.
          Example: v1beta1.\n                    model_version: Version of the deployed
          model. Relevant to match explainer version to model version. Example: 1.\n                    s3_bucket:
          Name of the s3 bucket in which model projects reside. Example: projects.\n            Returns:\n                    endpoint:
          REST endpoint where the model can be queried. Example: https://my-model-user-example-com.apps.myorg.com.\n    \"\"\"\n    from
          kubernetes import client, config\n    from kserve import KServeClient\n    from
          kserve import constants\n    from kserve import utils\n    from kserve import
          V1beta1AlibiExplainerSpec\n    from kserve import V1beta1ExplainerSpec\n    from
          kserve import V1beta1InferenceService\n    from kserve import V1beta1InferenceServiceSpec\n    from
          kserve import V1beta1PredictorSpec\n    from kserve import V1beta1TritonSpec\n    import
          logging\n    import sys\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n\n    try:\n        model_version =
          int(model_version)\n    except ValueError:\n        logging.warning(\n            \"Could
          not parse model version. Continuing with default value 1...\"\n        )\n        model_version
          = 1\n\n    # See: https://www.kubeflow.org/docs/external-add-ons/kserve/first_isvc_kserve/\n    logging.info(\"Initializing
          environment...\")\n    config.load_incluster_config()\n    namespace = utils.get_default_target_namespace()\n    api_version
          = constants.KSERVE_GROUP + \"/\" + kserve_version\n    storage_uri: str
          = f\"s3://{s3_bucket}/{project_name}\"\n\n    logging.info(\"Initializing
          inference service specification...\")\n    resources_spec = client.V1ResourceRequirements(\n        requests={\"cpu\":
          \"1000m\", \"memory\": \"8Gi\"},\n        limits={\"cpu\": \"2000m\", \"memory\":
          \"16Gi\"},\n    )\n\n    # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1TritonSpec/\n    triton_spec
          = V1beta1TritonSpec(\n        args=[\"--strict-model-config=false\"],\n        runtime_version=\"22.03-py3\",\n        storage_uri=storage_uri,\n        resources=resources_spec,\n    )\n\n    #
          See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1PredictorSpec/\n    predictor_spec
          = V1beta1PredictorSpec(\n        service_account_name=\"kserve-inference-sa\",
          triton=triton_spec\n    )\n\n    if explainer_type:\n        print(\"Found
          an explainer, which will be co-deployed.\")\n        # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1AlibiExplainerSpec/\n        alibi_spec
          = V1beta1AlibiExplainerSpec(\n            type=explainer_type,\n            storage_uri=f\"{storage_uri}/explainer/{model_version}\",  #
          /explainer.alibi\",\n            resources=resources_spec,\n        )\n\n        #
          See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1ExplainerSpec/\n        explainer_spec
          = V1beta1ExplainerSpec(\n            min_replicas=1,\n            alibi=alibi_spec,\n        )\n\n    #
          See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1InferenceServiceSpec/#properties\n    inference_service_spec
          = V1beta1InferenceService(\n        api_version=api_version,\n        kind=constants.KSERVE_KIND,\n        metadata=client.V1ObjectMeta(\n            name=project_name,\n            namespace=namespace,\n            annotations={\"sidecar.istio.io/inject\":
          \"false\"},\n        ),\n        spec=V1beta1InferenceServiceSpec(\n            predictor=predictor_spec,\n            explainer=explainer_spec
          if explainer_type else None,\n        ),\n    )\n\n    kserve_client = KServeClient()\n\n    logging.info(\"Checking
          for existing inference service...\")\n    try:\n        inference_service
          = kserve_client.get(project_name, namespace=namespace)\n        logging.info(f\"Received:
          {inference_service}\")\n\n        if \"status\" in inference_service:\n            logging.info(\"Inference
          service already exists.\")\n\n            logging.info(\"Patching inference
          service with new model version...\")\n            kserve_client.patch(project_name,
          inference_service_spec)\n        else:\n            logging.info(\"Creating
          inference service...\")\n            kserve_client.create(inference_service_spec)\n    except
          Exception:\n        logging.info(\"Creating new inference service...\")\n        kserve_client.create(inference_service_spec)\n\n    logging.info(\"Waiting
          for inference service to start...\")\n    kserve_client.get(\n        project_name,
          namespace=namespace, watch=True, timeout_seconds=180\n    )\n\n    logging.info(\"Getting
          inference URL...\")\n    inference_response = kserve_client.get(project_name,
          namespace=namespace)\n    inference_url = inference_response[\"status\"][\"address\"][\"url\"]\n    logging.info(f\"inference
          URL: {inference_url}\")\n\n    logging.info(\"Finished.\")\n    return inference_url\n\ndef
          _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(\n            str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Deploy
          model with kserve'', description=''Deploys a model using KServe and Trino
          as backend.'')\n_parser.add_argument(\"--project-name\", dest=\"project_name\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-version\",
          dest=\"model_version\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--explainer-type\",
          dest=\"explainer_type\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--kserve-version\",
          dest=\"kserve_version\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--s3-bucket\",
          dest=\"s3_bucket\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = deploy_model_with_kserve(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ntlawrence/demo-kserve@sha256:6803755ecffe42bfb38a703e1d4e453c088bae64096fc8857baa663b526d8978"}},
          "inputs": [{"description": "Name of the project. Must be unique for the
          targeted namespace and conform Kubernetes naming conventions. Example: my-model.",
          "name": "project_name", "type": "String"}, {"description": "Version of the
          deployed model. Relevant to match explainer version to model version. Example:
          1.", "name": "model_version", "type": "Integer"}, {"description": "Type
          of Alibi explanation. If None, explanations are not provided. Example: AnchorTabular.",
          "name": "explainer_type", "optional": true, "type": "String"}, {"default":
          "v1beta1", "description": "KServe API version. Example: v1beta1.", "name":
          "kserve_version", "optional": true, "type": "String"}, {"default": "projects",
          "description": "Name of the s3 bucket in which model projects reside. Example:
          projects.", "name": "s3_bucket", "optional": true, "type": "String"}], "name":
          "Deploy model with kserve", "outputs": [{"name": "Output", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "7b7b165fe04946cafacff83786f2e881c3a527d86335e22e37c0232848ab5414",
          "url": "kserve/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"kserve_version":
          "v1beta1", "model_version": "{{inputs.parameters.upload-model-model_version}}",
          "project_name": "{{inputs.parameters.model_name}}", "s3_bucket": "projects"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: fraud-detection
    inputs:
      parameters:
      - {name: blackboard}
      - {name: cluster_configuration_secret}
      - {name: model_name}
      - {name: training_gpus}
      - {name: training_node_selector}
    dag:
      tasks:
      - name: convert-model-to-onnx
        template: convert-model-to-onnx
        dependencies: [train-model-job]
        arguments:
          artifacts:
          - {name: train-model-job-model_dir, from: '{{tasks.train-model-job.outputs.artifacts.train-model-job-model_dir}}'}
      - name: create-artefacts-blackboard
        template: create-artefacts-blackboard
        arguments:
          parameters:
          - {name: blackboard, value: '{{inputs.parameters.blackboard}}'}
      - name: create-data-quality-report-with-evidently
        template: create-data-quality-report-with-evidently
        dependencies: [load-dataframe-via-trino]
        arguments:
          artifacts:
          - {name: load-dataframe-via-trino-dataframe, from: '{{tasks.load-dataframe-via-trino.outputs.artifacts.load-dataframe-via-trino-dataframe}}'}
      - name: deploy-model-with-kserve
        template: deploy-model-with-kserve
        dependencies: [upload-model]
        arguments:
          parameters:
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
          - {name: upload-model-model_version, value: '{{tasks.upload-model.outputs.parameters.upload-model-model_version}}'}
      - name: load-dataframe-via-trino
        template: load-dataframe-via-trino
        dependencies: [create-artefacts-blackboard]
      - {name: monitor-training, template: monitor-training}
      - name: plot-confusion-matrix
        template: plot-confusion-matrix
        dependencies: [predict]
        arguments:
          artifacts:
          - {name: predict-predictions_dir, from: '{{tasks.predict.outputs.artifacts.predict-predictions_dir}}'}
      - name: predict
        template: predict
        dependencies: [preprocess-dataset, train-model-job]
        arguments:
          artifacts:
          - {name: preprocess-dataset-validation_dataset_dir, from: '{{tasks.preprocess-dataset.outputs.artifacts.preprocess-dataset-validation_dataset_dir}}'}
          - {name: train-model-job-model_dir, from: '{{tasks.train-model-job.outputs.artifacts.train-model-job-model_dir}}'}
      - name: preprocess-dataset
        template: preprocess-dataset
        dependencies: [load-dataframe-via-trino]
        arguments:
          artifacts:
          - {name: load-dataframe-via-trino-dataframe, from: '{{tasks.load-dataframe-via-trino.outputs.artifacts.load-dataframe-via-trino-dataframe}}'}
      - name: train-model-job
        template: train-model-job
        dependencies: [monitor-training, preprocess-dataset]
        arguments:
          parameters:
          - {name: cluster_configuration_secret, value: '{{inputs.parameters.cluster_configuration_secret}}'}
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
          - {name: monitor-training-tensorboard_s3_address, value: '{{tasks.monitor-training.outputs.parameters.monitor-training-tensorboard_s3_address}}'}
          - {name: training_gpus, value: '{{inputs.parameters.training_gpus}}'}
          - {name: training_node_selector, value: '{{inputs.parameters.training_node_selector}}'}
          artifacts:
          - {name: preprocess-dataset-train_dataset_dir, from: '{{tasks.preprocess-dataset.outputs.artifacts.preprocess-dataset-train_dataset_dir}}'}
          - {name: preprocess-dataset-validation_dataset_dir, from: '{{tasks.preprocess-dataset.outputs.artifacts.preprocess-dataset-validation_dataset_dir}}'}
      - name: upload-model
        template: upload-model
        dependencies: [convert-model-to-onnx]
        arguments:
          parameters:
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
          artifacts:
          - {name: convert-model-to-onnx-onnx_model_dir, from: '{{tasks.convert-model-to-onnx.outputs.artifacts.convert-model-to-onnx-onnx_model_dir}}'}
  - name: load-dataframe-via-trino
    container:
      args: [--query, SELECT * FROM  jtopen.demo.fraud limit 1000000, --columns-query,
        SHOW COLUMNS FROM jtopen.demo.fraud, --host, trino.trino, --port, '8080',
        --user, anybody, --dataframe, /tmp/outputs/dataframe/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def Load_Dataframe_via_Trino(
            query,
            dataframe_file,
            columns=None,
            columns_query = None,
            host = "trino.trino",
            port = 8080,
            user = "anybody",
            catalog = None,
            schema = None,
        ):
            """
            Load a Pandas Dataframe using Trino as SQL client.

                    Parameters:
                            query: An ANSI SQL compliant query for data, as supported by Trino. Queries can either use explicit or implicit references to schemata and catalogs. In the implicit case, the parameters catalog and schema must be set. Example: "SELECT * FROM transactions OFFSET 20".
                            columns: List of column names of the resulting Dataframe.
                            columns_query: An ANSI SQL compliant "SHOW COLUMNS" query for data columns, as supported by Trino. Queries can either use explicit or implicit references to schemata and catalogs. In the implicit case, the parameters catalog and schema must be set. If not set, generic column names are used. Example: "SHOW COLUMNS FROM postgresql.public.transactions".
                            host: Host of the trino installation, typically the trino service in the trino namespace. Example:  "trino.trino".
                            port: Trino service port. Example: "8080".
                            user: Sets the query context to the given user. The user needs permissions to access the targeted catalog and schema. Example: "anybody".
                            catalog: Sets the query context to the given catalog. If None, the query must explicitly reference to schemata and catalogs. If set, also a schema must be set. Example: "postgresql".
                            schema: Sets the query context to the given schema. If None, the query must explicitly reference to schemata and catalogs. If set, also a catalog must be set. Example: "public".
                    Returns:
                            dataframe_file: A Pandas dataframe containing the query results.
            """
            import logging
            import pandas as pd
            import sys
            from trino.dbapi import Connection

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )

            if (catalog is not None and schema is None) or (
                catalog is None and schema is not None
            ):
                raise Exception(
                    f"If you set one, you need to set both: catalog={catalog} but schema={schema}!"
                )

            logging.info("Establishing Trino connection...")
            with Connection(
                host=host,
                port=port,
                user=user,
                catalog=catalog,
                schema=schema,
            ) as conn:
                cursor = conn.cursor()

                logging.info("Querying data...")
                cursor.execute(query)
                dataframe = pd.DataFrame(cursor.fetchall())
                logging.info(f"Retrieved {len(dataframe)} rows.")

                if columns is not None:
                    logging.info("Using given column names...")
                elif columns_query is not None:
                    logging.info("Querying column names...")
                    cursor.execute(columns_query)
                    columns_dataframe = pd.DataFrame(cursor.fetchall())
                    columns = columns_dataframe[0].values.tolist()
                else:
                    logging.info("Creating generic column names...")
                    columns = []
                    for column in range(dataframe.columns.size):
                        columns.append(f"column_{column}")

                dataframe.columns = columns
                logging.info(f"Using columns: {columns}")

            # Feather outperforms Pickle & Parquet
            # See https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d
            dataframe.to_feather(dataframe_file)
            logging.info("Finished.")

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Load Dataframe via Trino', description='Load a Pandas Dataframe using Trino as SQL client.')
        _parser.add_argument("--query", dest="query", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--columns", dest="columns", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--columns-query", dest="columns_query", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--host", dest="host", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--port", dest="port", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--user", dest="user", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--catalog", dest="catalog", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--schema", dest="schema", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--dataframe", dest="dataframe_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = Load_Dataframe_via_Trino(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      artifacts:
      - name: columns
        path: /tmp/inputs/columns/data
        raw: {data: None}
    outputs:
      artifacts:
      - {name: load-dataframe-via-trino-dataframe, path: /tmp/outputs/dataframe/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Load
          a Pandas Dataframe using Trino as SQL client.", "implementation": {"container":
          {"args": ["--query", {"inputValue": "query"}, {"if": {"cond": {"isPresent":
          "columns"}, "then": ["--columns", {"inputValue": "columns"}]}}, {"if": {"cond":
          {"isPresent": "columns_query"}, "then": ["--columns-query", {"inputValue":
          "columns_query"}]}}, {"if": {"cond": {"isPresent": "host"}, "then": ["--host",
          {"inputValue": "host"}]}}, {"if": {"cond": {"isPresent": "port"}, "then":
          ["--port", {"inputValue": "port"}]}}, {"if": {"cond": {"isPresent": "user"},
          "then": ["--user", {"inputValue": "user"}]}}, {"if": {"cond": {"isPresent":
          "catalog"}, "then": ["--catalog", {"inputValue": "catalog"}]}}, {"if": {"cond":
          {"isPresent": "schema"}, "then": ["--schema", {"inputValue": "schema"}]}},
          "--dataframe", {"outputPath": "dataframe"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef Load_Dataframe_via_Trino(\n    query,\n    dataframe_file,\n    columns=None,\n    columns_query
          = None,\n    host = \"trino.trino\",\n    port = 8080,\n    user = \"anybody\",\n    catalog
          = None,\n    schema = None,\n):\n    \"\"\"\n    Load a Pandas Dataframe
          using Trino as SQL client.\n\n            Parameters:\n                    query:
          An ANSI SQL compliant query for data, as supported by Trino. Queries can
          either use explicit or implicit references to schemata and catalogs. In
          the implicit case, the parameters catalog and schema must be set. Example:
          \"SELECT * FROM transactions OFFSET 20\".\n                    columns:
          List of column names of the resulting Dataframe.\n                    columns_query:
          An ANSI SQL compliant \"SHOW COLUMNS\" query for data columns, as supported
          by Trino. Queries can either use explicit or implicit references to schemata
          and catalogs. In the implicit case, the parameters catalog and schema must
          be set. If not set, generic column names are used. Example: \"SHOW COLUMNS
          FROM postgresql.public.transactions\".\n                    host: Host of
          the trino installation, typically the trino service in the trino namespace.
          Example:  \"trino.trino\".\n                    port: Trino service port.
          Example: \"8080\".\n                    user: Sets the query context to
          the given user. The user needs permissions to access the targeted catalog
          and schema. Example: \"anybody\".\n                    catalog: Sets the
          query context to the given catalog. If None, the query must explicitly reference
          to schemata and catalogs. If set, also a schema must be set. Example: \"postgresql\".\n                    schema:
          Sets the query context to the given schema. If None, the query must explicitly
          reference to schemata and catalogs. If set, also a catalog must be set.
          Example: \"public\".\n            Returns:\n                    dataframe_file:
          A Pandas dataframe containing the query results.\n    \"\"\"\n    import
          logging\n    import pandas as pd\n    import sys\n    from trino.dbapi import
          Connection\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n\n    if (catalog is not None and schema
          is None) or (\n        catalog is None and schema is not None\n    ):\n        raise
          Exception(\n            f\"If you set one, you need to set both: catalog={catalog}
          but schema={schema}!\"\n        )\n\n    logging.info(\"Establishing Trino
          connection...\")\n    with Connection(\n        host=host,\n        port=port,\n        user=user,\n        catalog=catalog,\n        schema=schema,\n    )
          as conn:\n        cursor = conn.cursor()\n\n        logging.info(\"Querying
          data...\")\n        cursor.execute(query)\n        dataframe = pd.DataFrame(cursor.fetchall())\n        logging.info(f\"Retrieved
          {len(dataframe)} rows.\")\n\n        if columns is not None:\n            logging.info(\"Using
          given column names...\")\n        elif columns_query is not None:\n            logging.info(\"Querying
          column names...\")\n            cursor.execute(columns_query)\n            columns_dataframe
          = pd.DataFrame(cursor.fetchall())\n            columns = columns_dataframe[0].values.tolist()\n        else:\n            logging.info(\"Creating
          generic column names...\")\n            columns = []\n            for column
          in range(dataframe.columns.size):\n                columns.append(f\"column_{column}\")\n\n        dataframe.columns
          = columns\n        logging.info(f\"Using columns: {columns}\")\n\n    #
          Feather outperforms Pickle & Parquet\n    # See https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d\n    dataframe.to_feather(dataframe_file)\n    logging.info(\"Finished.\")\n\nimport
          json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Load Dataframe
          via Trino'', description=''Load a Pandas Dataframe using Trino as SQL client.'')\n_parser.add_argument(\"--query\",
          dest=\"query\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--columns\",
          dest=\"columns\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--columns-query\",
          dest=\"columns_query\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--host\",
          dest=\"host\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--port\",
          dest=\"port\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--user\",
          dest=\"user\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--catalog\",
          dest=\"catalog\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--schema\",
          dest=\"schema\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataframe\",
          dest=\"dataframe_file\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = Load_Dataframe_via_Trino(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}},
          "inputs": [{"description": "An ANSI SQL compliant query for data, as supported
          by Trino. Queries can either use explicit or implicit references to schemata
          and catalogs. In the implicit case, the parameters catalog and schema must
          be set. Example: \"SELECT * FROM transactions OFFSET 20\".", "name": "query",
          "type": "String"}, {"description": "List of column names of the resulting
          Dataframe.", "name": "columns", "type": "typing.List[str]"}, {"description":
          "An ANSI SQL compliant \"SHOW COLUMNS\" query for data columns, as supported
          by Trino. Queries can either use explicit or implicit references to schemata
          and catalogs. In the implicit case, the parameters catalog and schema must
          be set. If not set, generic column names are used. Example: \"SHOW COLUMNS
          FROM postgresql.public.transactions\".", "name": "columns_query", "optional":
          true, "type": "String"}, {"default": "trino.trino", "description": "Host
          of the trino installation, typically the trino service in the trino namespace.
          Example:  \"trino.trino\".", "name": "host", "optional": true, "type": "String"},
          {"default": "8080", "description": "Trino service port. Example: \"8080\".",
          "name": "port", "optional": true, "type": "Integer"}, {"default": "anybody",
          "description": "Sets the query context to the given user. The user needs
          permissions to access the targeted catalog and schema. Example: \"anybody\".",
          "name": "user", "optional": true, "type": "String"}, {"description": "Sets
          the query context to the given catalog. If None, the query must explicitly
          reference to schemata and catalogs. If set, also a schema must be set. Example:
          \"postgresql\".", "name": "catalog", "optional": true, "type": "String"},
          {"description": "Sets the query context to the given schema. If None, the
          query must explicitly reference to schemata and catalogs. If set, also a
          catalog must be set. Example: \"public\".", "name": "schema", "optional":
          true, "type": "String"}], "name": "Load Dataframe via Trino", "outputs":
          [{"name": "dataframe", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "e4273a3c6660ff095407a725eeb09570263026854aed8638a077a5e3a3c3bc44", "url":
          "trino/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"columns_query":
          "SHOW COLUMNS FROM jtopen.demo.fraud", "host": "trino.trino", "port": "8080",
          "query": "SELECT * FROM  jtopen.demo.fraud limit 1000000", "user": "anybody"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: monitor-training
    container:
      args: [--monitor-bucket, mlpipeline, --monitor-folder, tensorboard, --mlpipeline-ui-metadata,
        /tmp/outputs/mlpipeline_ui_metadata/data, '----output-paths', /tmp/outputs/tensorboard_s3_address/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def Monitor_Training(
            mlpipeline_ui_metadata_path,
            monitor_bucket = "mlpipeline",
            monitor_folder = "tensorboard",
        ):
            """
            Monitors a training job based on Tensorboard logs. Logs are expected to be written to the tensorboard_s3_address.

                    Parameters:
                            monitor_bucket: Optional bucket name of the targeted s3 store. Example: "my-bucket".
                            monitor_folder: Optional folder name inside the targeted s3 store. Example: "my-monitor".
                    Returns:
                            mlpipeline_ui_metadata_path: OutputTensorboard visualization for the pipelines UI.
                            tensorboard_s3_address: The tensorboard_s3_address were logs are expected to be written to.
            """
            from collections import namedtuple
            import json
            from kubernetes import client, config, watch
            import logging
            import sys
            import os
            import yaml

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )
            logger = logging.getLogger()

            logger.info("Preparing monitor...")
            tensorboard_name = os.environ["HOSTNAME"]
            with open("/var/run/secrets/kubernetes.io/serviceaccount/namespace") as f:
                namespace = f.read()
            tensorboard_s3_address = (
                f"s3://{monitor_bucket}/{monitor_folder}/{tensorboard_name}"
            )
            # See: https://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/tensorboards/frontend/src/app/pages/index/index.component.ts
            # window.open(`/tensorboard/${tensorboard.namespace}/${tensorboard.name}/`);
            ui_address = f"/tensorboard/{namespace}/{tensorboard_name}/#scalars"

            logger.info("Creating monitor...")
            config.load_incluster_config()
            api_client = client.ApiClient()
            apps_api = client.AppsV1Api(api_client)
            custom_object_api = client.CustomObjectsApi(api_client)

            tensorboard_spec = f"""apiVersion: tensorboard.kubeflow.org/v1alpha1
        kind: Tensorboard
        metadata:
          name: {tensorboard_name}
          namespace: {namespace}
        spec:
          logspath: {tensorboard_s3_address}
        """
            custom_object_api.create_namespaced_custom_object(
                group="tensorboard.kubeflow.org",
                version="v1alpha1",
                plural="tensorboards",
                namespace=namespace,
                body=yaml.safe_load(tensorboard_spec),
            )

            applied_s3_patch = False
            tensorboard_watch = watch.Watch()
            for tensorboard_event in tensorboard_watch.stream(
                custom_object_api.list_namespaced_custom_object,
                group="tensorboard.kubeflow.org",
                version="v1alpha1",
                plural="tensorboards",
                namespace=namespace,
                field_selector=f"metadata.name={tensorboard_name}",
                timeout_seconds=0,
            ):
                logger.info(f"tensorboard_event: {tensorboard_event}")
                tensorboard = tensorboard_event["object"]

                if "status" not in tensorboard:
                    logger.info("Skipping event (no status information found)...")
                    continue

                if not applied_s3_patch:
                    # TODO Use pod defaults once available
                    # see: https://github.com/kubeflow/kubeflow/pull/6874
                    body = client.V1Deployment()
                    body.spec = {
                        "template": {
                            "spec": {
                                "containers": [
                                    {
                                        "name": "tensorboard",
                                        "env": [
                                            {
                                                "name": "S3_ENDPOINT",
                                                "value": "http://minio-service.kubeflow:9000",
                                            },
                                            {"name": "AWS_ACCESS_KEY_ID", "value": "minio"},
                                            {
                                                "name": "AWS_SECRET_ACCESS_KEY",
                                                "value": "minio123",
                                            },
                                            {
                                                "name": "AWS_S3_SIGNATURE_VERSION",
                                                "value": "s3v4",
                                            },
                                        ],
                                    }
                                ]
                            }
                        }
                    }
                    apps_api.patch_namespaced_deployment(
                        name=tensorboard_name, namespace=namespace, body=body
                    )
                    applied_s3_patch = True

                deployment_state = "Progressing"
                if "conditions" in tensorboard["status"]:
                    deployment_state = tensorboard["status"]["conditions"][-1][
                        "deploymentState"
                    ]

                if deployment_state == "Progressing":
                    logger.info("Tensorboard deployment is progressing...")
                elif deployment_state == "Available":
                    logger.info("Tensorboard is now available.")
                    tensorboard_watch.stop()
                    break
                elif deployment_state == "ReplicaFailure":
                    tensorboard_watch.stop()
                    raise Exception("Tensorboard deployment failed with a ReplicaFailure!")
                else:
                    raise Exception(f"Unknown deployment state: {deployment_state}")

            button_style = "align-items: center; appearance: none; background-color: rgb(26, 115, 232); border: 0px none rgb(255, 255, 255); border-radius: 3px; box-sizing: border-box; color: rgb(255, 255, 255); cursor: pointer; display: inline-flex; font-family: 'Google Sans', 'Helvetica Neue', sans-serif; font-size: 14px; font-stretch: 100%; font-style: normal; font-weight: 700; justify-content: center; letter-spacing: normal; line-height: 24.5px; margin: 0px 10px 2px 0px; min-height: 25px; min-width: 64px; padding: 2px 6px 2px 6px; position: relative; tab-size: 4; text-align: center; text-indent: 0px; text-rendering: auto; text-shadow: none; text-size-adjust: 100%; text-transform: none; user-select: none; vertical-align: middle; word-spacing: 0px; writing-mode: horizontal-tb;"
            markdown = f'# Tensorboard\n- <a href="{ui_address}" style="{button_style}" target="_blank">Connect</a>\n- <a href="/_/tensorboards/" style="{button_style}" target="_blank">Manage all</a>'
            markdown_output = {
                "type": "markdown",
                "storage": "inline",
                "source": markdown,
            }

            ui_metadata = {"outputs": [markdown_output]}
            with open(mlpipeline_ui_metadata_path, "w") as metadata_file:
                json.dump(ui_metadata, metadata_file)

            output = namedtuple("MonitorOutputs", ["tensorboard_s3_address"])
            logging.info("Finished.")
            return output(tensorboard_s3_address)

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Monitor Training', description='Monitors a training job based on Tensorboard logs. Logs are expected to be written to the tensorboard_s3_address.')
        _parser.add_argument("--monitor-bucket", dest="monitor_bucket", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--monitor-folder", dest="monitor_folder", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = Monitor_Training(**_parsed_args)

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    outputs:
      parameters:
      - name: monitor-training-tensorboard_s3_address
        valueFrom: {path: /tmp/outputs/tensorboard_s3_address/data}
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: monitor-training-tensorboard_s3_address, path: /tmp/outputs/tensorboard_s3_address/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Monitors
          a training job based on Tensorboard logs. Logs are expected to be written
          to the tensorboard_s3_address.", "implementation": {"container": {"args":
          [{"if": {"cond": {"isPresent": "monitor_bucket"}, "then": ["--monitor-bucket",
          {"inputValue": "monitor_bucket"}]}}, {"if": {"cond": {"isPresent": "monitor_folder"},
          "then": ["--monitor-folder", {"inputValue": "monitor_folder"}]}}, "--mlpipeline-ui-metadata",
          {"outputPath": "mlpipeline_ui_metadata"}, "----output-paths", {"outputPath":
          "tensorboard_s3_address"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef Monitor_Training(\n    mlpipeline_ui_metadata_path,\n    monitor_bucket
          = \"mlpipeline\",\n    monitor_folder = \"tensorboard\",\n):\n    \"\"\"\n    Monitors
          a training job based on Tensorboard logs. Logs are expected to be written
          to the tensorboard_s3_address.\n\n            Parameters:\n                    monitor_bucket:
          Optional bucket name of the targeted s3 store. Example: \"my-bucket\".\n                    monitor_folder:
          Optional folder name inside the targeted s3 store. Example: \"my-monitor\".\n            Returns:\n                    mlpipeline_ui_metadata_path:
          OutputTensorboard visualization for the pipelines UI.\n                    tensorboard_s3_address:
          The tensorboard_s3_address were logs are expected to be written to.\n    \"\"\"\n    from
          collections import namedtuple\n    import json\n    from kubernetes import
          client, config, watch\n    import logging\n    import sys\n    import os\n    import
          yaml\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    logger.info(\"Preparing
          monitor...\")\n    tensorboard_name = os.environ[\"HOSTNAME\"]\n    with
          open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n        namespace
          = f.read()\n    tensorboard_s3_address = (\n        f\"s3://{monitor_bucket}/{monitor_folder}/{tensorboard_name}\"\n    )\n    #
          See: https://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/tensorboards/frontend/src/app/pages/index/index.component.ts\n    #
          window.open(`/tensorboard/${tensorboard.namespace}/${tensorboard.name}/`);\n    ui_address
          = f\"/tensorboard/{namespace}/{tensorboard_name}/#scalars\"\n\n    logger.info(\"Creating
          monitor...\")\n    config.load_incluster_config()\n    api_client = client.ApiClient()\n    apps_api
          = client.AppsV1Api(api_client)\n    custom_object_api = client.CustomObjectsApi(api_client)\n\n    tensorboard_spec
          = f\"\"\"apiVersion: tensorboard.kubeflow.org/v1alpha1\nkind: Tensorboard\nmetadata:\n  name:
          {tensorboard_name}\n  namespace: {namespace}\nspec:\n  logspath: {tensorboard_s3_address}\n\"\"\"\n    custom_object_api.create_namespaced_custom_object(\n        group=\"tensorboard.kubeflow.org\",\n        version=\"v1alpha1\",\n        plural=\"tensorboards\",\n        namespace=namespace,\n        body=yaml.safe_load(tensorboard_spec),\n    )\n\n    applied_s3_patch
          = False\n    tensorboard_watch = watch.Watch()\n    for tensorboard_event
          in tensorboard_watch.stream(\n        custom_object_api.list_namespaced_custom_object,\n        group=\"tensorboard.kubeflow.org\",\n        version=\"v1alpha1\",\n        plural=\"tensorboards\",\n        namespace=namespace,\n        field_selector=f\"metadata.name={tensorboard_name}\",\n        timeout_seconds=0,\n    ):\n        logger.info(f\"tensorboard_event:
          {tensorboard_event}\")\n        tensorboard = tensorboard_event[\"object\"]\n\n        if
          \"status\" not in tensorboard:\n            logger.info(\"Skipping event
          (no status information found)...\")\n            continue\n\n        if
          not applied_s3_patch:\n            # TODO Use pod defaults once available\n            #
          see: https://github.com/kubeflow/kubeflow/pull/6874\n            body =
          client.V1Deployment()\n            body.spec = {\n                \"template\":
          {\n                    \"spec\": {\n                        \"containers\":
          [\n                            {\n                                \"name\":
          \"tensorboard\",\n                                \"env\": [\n                                    {\n                                        \"name\":
          \"S3_ENDPOINT\",\n                                        \"value\": \"http://minio-service.kubeflow:9000\",\n                                    },\n                                    {\"name\":
          \"AWS_ACCESS_KEY_ID\", \"value\": \"minio\"},\n                                    {\n                                        \"name\":
          \"AWS_SECRET_ACCESS_KEY\",\n                                        \"value\":
          \"minio123\",\n                                    },\n                                    {\n                                        \"name\":
          \"AWS_S3_SIGNATURE_VERSION\",\n                                        \"value\":
          \"s3v4\",\n                                    },\n                                ],\n                            }\n                        ]\n                    }\n                }\n            }\n            apps_api.patch_namespaced_deployment(\n                name=tensorboard_name,
          namespace=namespace, body=body\n            )\n            applied_s3_patch
          = True\n\n        deployment_state = \"Progressing\"\n        if \"conditions\"
          in tensorboard[\"status\"]:\n            deployment_state = tensorboard[\"status\"][\"conditions\"][-1][\n                \"deploymentState\"\n            ]\n\n        if
          deployment_state == \"Progressing\":\n            logger.info(\"Tensorboard
          deployment is progressing...\")\n        elif deployment_state == \"Available\":\n            logger.info(\"Tensorboard
          is now available.\")\n            tensorboard_watch.stop()\n            break\n        elif
          deployment_state == \"ReplicaFailure\":\n            tensorboard_watch.stop()\n            raise
          Exception(\"Tensorboard deployment failed with a ReplicaFailure!\")\n        else:\n            raise
          Exception(f\"Unknown deployment state: {deployment_state}\")\n\n    button_style
          = \"align-items: center; appearance: none; background-color: rgb(26, 115,
          232); border: 0px none rgb(255, 255, 255); border-radius: 3px; box-sizing:
          border-box; color: rgb(255, 255, 255); cursor: pointer; display: inline-flex;
          font-family: ''Google Sans'', ''Helvetica Neue'', sans-serif; font-size:
          14px; font-stretch: 100%; font-style: normal; font-weight: 700; justify-content:
          center; letter-spacing: normal; line-height: 24.5px; margin: 0px 10px 2px
          0px; min-height: 25px; min-width: 64px; padding: 2px 6px 2px 6px; position:
          relative; tab-size: 4; text-align: center; text-indent: 0px; text-rendering:
          auto; text-shadow: none; text-size-adjust: 100%; text-transform: none; user-select:
          none; vertical-align: middle; word-spacing: 0px; writing-mode: horizontal-tb;\"\n    markdown
          = f''# Tensorboard\\n- <a href=\"{ui_address}\" style=\"{button_style}\"
          target=\"_blank\">Connect</a>\\n- <a href=\"/_/tensorboards/\" style=\"{button_style}\"
          target=\"_blank\">Manage all</a>''\n    markdown_output = {\n        \"type\":
          \"markdown\",\n        \"storage\": \"inline\",\n        \"source\": markdown,\n    }\n\n    ui_metadata
          = {\"outputs\": [markdown_output]}\n    with open(mlpipeline_ui_metadata_path,
          \"w\") as metadata_file:\n        json.dump(ui_metadata, metadata_file)\n\n    output
          = namedtuple(\"MonitorOutputs\", [\"tensorboard_s3_address\"])\n    logging.info(\"Finished.\")\n    return
          output(tensorboard_s3_address)\n\ndef _serialize_str(str_value: str) ->
          str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(\n            str(str_value),
          str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Monitor Training'', description=''Monitors
          a training job based on Tensorboard logs. Logs are expected to be written
          to the tensorboard_s3_address.'')\n_parser.add_argument(\"--monitor-bucket\",
          dest=\"monitor_bucket\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--monitor-folder\",
          dest=\"monitor_folder\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = Monitor_Training(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}}, "inputs":
          [{"default": "mlpipeline", "description": "Optional bucket name of the targeted
          s3 store. Example: \"my-bucket\".", "name": "monitor_bucket", "optional":
          true, "type": "String"}, {"default": "tensorboard", "description": "Optional
          folder name inside the targeted s3 store. Example: \"my-monitor\".", "name":
          "monitor_folder", "optional": true, "type": "String"}], "name": "Monitor
          Training", "outputs": [{"name": "mlpipeline_ui_metadata"}, {"name": "tensorboard_s3_address",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "391d3799f8792e3722be6ec370073bcf8bf1b0281989d256e746c39392cd9001", "url":
          "/home/jovyan/components/model-building/monitor-training/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"monitor_bucket": "mlpipeline",
          "monitor_folder": "tensorboard"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: plot-confusion-matrix
    container:
      args: [--predictions-dir, /tmp/inputs/predictions_dir/data, --mlpipeline-ui-metadata,
        /tmp/outputs/mlpipeline_ui_metadata/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def plot_confusion_matrix(
            predictions_dir, mlpipeline_ui_metadata_path
        ):
            """
            Plots a confusion matrix based on a list of labels and predictions for one target variable

                    Parameters:
                            predictions_dir: Path to directory with 2 files for true labels (ytrue.txt) and predicted labels (ypred.txt).
                                            The files should have one label formatted as string per row.
                    Returns:
                            mlpipeline_ui_metadata_path: Data to plot a confusion matrix. The plotted confusion matrix can be viewed via Kubeflow UI's Vizualization for this component inside a pipeline run.
            """
            import json
            import logging
            import pandas as pd
            import sys
            import tensorflow as tf
            import os

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )
            logger = logging.getLogger()

            def load_data(file):
                with open(os.path.join(predictions_dir, file)) as f:
                    y = f.readlines()
                try:
                    y = list(map(lambda x: int(x.strip("\n")), y))
                    logger.info(f"Reading {file} as numbers.")
                except ValueError:
                    y = list(map(lambda x: x.strip("\n"), y))
                    logger.info(f"Reading {file} as strings.")
                return y

            y_true = load_data("ytrue.txt")
            y_pred = load_data("ypred.txt")

            if len(y_true) != len(y_pred):
                logger.error("Labels and Predictions have different lengths.")

            labels = list(set(y_true).union(set(y_pred)))
            logging.info(f"Using the labels {labels}")

            confusion_matrices = []
            confusion_matrix = tf.math.confusion_matrix(
                labels=y_true,
                predictions=y_pred,
                num_classes=len(labels),
            )

            data = []
            for target_index, target_row in enumerate(confusion_matrix):
                for predicted_index, count in enumerate(target_row):
                    data.append((labels[target_index], labels[predicted_index], count.numpy()))

            df = pd.DataFrame(data, columns=["target", "predicted", "count"])

            confusion_matrices.append(
                {
                    "type": "confusion_matrix",
                    "format": "csv",
                    "schema": [
                        {"name": "target", "type": "CATEGORY"},
                        {"name": "predicted", "type": "CATEGORY"},
                        {"name": "count", "type": "NUMBER"},
                    ],
                    "storage": "inline",
                    "source": df.to_csv(
                        columns=["target", "predicted", "count"], header=False, index=False
                    ),
                    "labels": labels,
                }
            )

            metadata = {"outputs": confusion_matrices}

            logger.info("Dumping mlpipeline_ui_metadata...")
            with open(mlpipeline_ui_metadata_path, "w") as metadata_file:
                json.dump(metadata, metadata_file)

            logger.info("Finished.")

        import argparse
        _parser = argparse.ArgumentParser(prog='Plot confusion matrix', description='Plots a confusion matrix based on a list of labels and predictions for one target variable')
        _parser.add_argument("--predictions-dir", dest="predictions_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = plot_confusion_matrix(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      artifacts:
      - {name: predict-predictions_dir, path: /tmp/inputs/predictions_dir/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Plots
          a confusion matrix based on a list of labels and predictions for one target
          variable", "implementation": {"container": {"args": ["--predictions-dir",
          {"inputPath": "predictions_dir"}, "--mlpipeline-ui-metadata", {"outputPath":
          "mlpipeline_ui_metadata"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef plot_confusion_matrix(\n    predictions_dir,
          mlpipeline_ui_metadata_path\n):\n    \"\"\"\n    Plots a confusion matrix
          based on a list of labels and predictions for one target variable\n\n            Parameters:\n                    predictions_dir:
          Path to directory with 2 files for true labels (ytrue.txt) and predicted
          labels (ypred.txt).\n                                    The files should
          have one label formatted as string per row.\n            Returns:\n                    mlpipeline_ui_metadata_path:
          Data to plot a confusion matrix. The plotted confusion matrix can be viewed
          via Kubeflow UI''s Vizualization for this component inside a pipeline run.\n    \"\"\"\n    import
          json\n    import logging\n    import pandas as pd\n    import sys\n    import
          tensorflow as tf\n    import os\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    def
          load_data(file):\n        with open(os.path.join(predictions_dir, file))
          as f:\n            y = f.readlines()\n        try:\n            y = list(map(lambda
          x: int(x.strip(\"\\n\")), y))\n            logger.info(f\"Reading {file}
          as numbers.\")\n        except ValueError:\n            y = list(map(lambda
          x: x.strip(\"\\n\"), y))\n            logger.info(f\"Reading {file} as strings.\")\n        return
          y\n\n    y_true = load_data(\"ytrue.txt\")\n    y_pred = load_data(\"ypred.txt\")\n\n    if
          len(y_true) != len(y_pred):\n        logger.error(\"Labels and Predictions
          have different lengths.\")\n\n    labels = list(set(y_true).union(set(y_pred)))\n    logging.info(f\"Using
          the labels {labels}\")\n\n    confusion_matrices = []\n    confusion_matrix
          = tf.math.confusion_matrix(\n        labels=y_true,\n        predictions=y_pred,\n        num_classes=len(labels),\n    )\n\n    data
          = []\n    for target_index, target_row in enumerate(confusion_matrix):\n        for
          predicted_index, count in enumerate(target_row):\n            data.append((labels[target_index],
          labels[predicted_index], count.numpy()))\n\n    df = pd.DataFrame(data,
          columns=[\"target\", \"predicted\", \"count\"])\n\n    confusion_matrices.append(\n        {\n            \"type\":
          \"confusion_matrix\",\n            \"format\": \"csv\",\n            \"schema\":
          [\n                {\"name\": \"target\", \"type\": \"CATEGORY\"},\n                {\"name\":
          \"predicted\", \"type\": \"CATEGORY\"},\n                {\"name\": \"count\",
          \"type\": \"NUMBER\"},\n            ],\n            \"storage\": \"inline\",\n            \"source\":
          df.to_csv(\n                columns=[\"target\", \"predicted\", \"count\"],
          header=False, index=False\n            ),\n            \"labels\": labels,\n        }\n    )\n\n    metadata
          = {\"outputs\": confusion_matrices}\n\n    logger.info(\"Dumping mlpipeline_ui_metadata...\")\n    with
          open(mlpipeline_ui_metadata_path, \"w\") as metadata_file:\n        json.dump(metadata,
          metadata_file)\n\n    logger.info(\"Finished.\")\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Plot confusion matrix'', description=''Plots
          a confusion matrix based on a list of labels and predictions for one target
          variable'')\n_parser.add_argument(\"--predictions-dir\", dest=\"predictions_dir\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = plot_confusion_matrix(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}},
          "inputs": [{"description": "Path to directory with 2 files for true labels
          (ytrue.txt) and predicted labels (ypred.txt).\nThe files should have one
          label formatted as string per row.", "name": "predictions_dir", "type":
          "String"}], "name": "Plot confusion matrix", "outputs": [{"name": "mlpipeline_ui_metadata",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "23e6f06680e7325fb3389fa664f033343afeb7c5acfb615e278c71d79f4a8047", "url":
          "/home/jovyan/components/model-building/plot-confusion-matrix-from-predictions/component.yaml"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: predict
    container:
      args: [--model-dir, /tmp/inputs/model_dir/data, --test-dataset-dir, /tmp/inputs/test_dataset_dir/data,
        --seq-len, '7', --predictions-dir, /tmp/outputs/predictions_dir/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def predict(
            model_dir,
            test_dataset_dir,
            predictions_dir,
            seq_len = 7,
        ):

            from tensorflow import keras
            import numpy as np
            import os

            data = np.load(os.path.join(test_dataset_dir, "data.npz"), allow_pickle=True)
            x, y = data["x"], data["y"]
            x = np.asarray(x).astype(np.float32)
            y = np.asarray(y).astype(np.int_)
            test_dataset = keras.preprocessing.timeseries_dataset_from_array(
                x, y, sequence_length=seq_len, batch_size=128
            )
            y = np.concatenate([b for a, b in test_dataset], axis=0)
            model = keras.models.load_model(model_dir)
            preds = model.predict(test_dataset)
            preds = [str(int(pred[0] > 0.5)) + "\n" for pred in preds]
            y = [str(x[0].item()) + "\n" for x in y]

            if not os.path.exists(predictions_dir):
                os.makedirs(predictions_dir)
            with open(os.path.join(predictions_dir, "ytrue.txt"), "w") as f:
                f.writelines(y)
            with open(os.path.join(predictions_dir, "ypred.txt"), "w") as f:
                f.writelines(preds)

        import argparse
        _parser = argparse.ArgumentParser(prog='Predict', description='')
        _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-dataset-dir", dest="test_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--seq-len", dest="seq_len", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--predictions-dir", dest="predictions_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = predict(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      artifacts:
      - {name: train-model-job-model_dir, path: /tmp/inputs/model_dir/data}
      - {name: preprocess-dataset-validation_dataset_dir, path: /tmp/inputs/test_dataset_dir/data}
    outputs:
      artifacts:
      - {name: predict-predictions_dir, path: /tmp/outputs/predictions_dir/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--model-dir", {"inputPath": "model_dir"}, "--test-dataset-dir",
          {"inputPath": "test_dataset_dir"}, {"if": {"cond": {"isPresent": "seq_len"},
          "then": ["--seq-len", {"inputValue": "seq_len"}]}}, "--predictions-dir",
          {"outputPath": "predictions_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef predict(\n    model_dir,\n    test_dataset_dir,\n    predictions_dir,\n    seq_len
          = 7,\n):\n\n    from tensorflow import keras\n    import numpy as np\n    import
          os\n\n    data = np.load(os.path.join(test_dataset_dir, \"data.npz\"), allow_pickle=True)\n    x,
          y = data[\"x\"], data[\"y\"]\n    x = np.asarray(x).astype(np.float32)\n    y
          = np.asarray(y).astype(np.int_)\n    test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n        x,
          y, sequence_length=seq_len, batch_size=128\n    )\n    y = np.concatenate([b
          for a, b in test_dataset], axis=0)\n    model = keras.models.load_model(model_dir)\n    preds
          = model.predict(test_dataset)\n    preds = [str(int(pred[0] > 0.5)) + \"\\n\"
          for pred in preds]\n    y = [str(x[0].item()) + \"\\n\" for x in y]\n\n    if
          not os.path.exists(predictions_dir):\n        os.makedirs(predictions_dir)\n    with
          open(os.path.join(predictions_dir, \"ytrue.txt\"), \"w\") as f:\n        f.writelines(y)\n    with
          open(os.path.join(predictions_dir, \"ypred.txt\"), \"w\") as f:\n        f.writelines(preds)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Predict'', description='''')\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-dataset-dir\",
          dest=\"test_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--seq-len\",
          dest=\"seq_len\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions-dir\",
          dest=\"predictions_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = predict(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}},
          "inputs": [{"name": "model_dir", "type": "String"}, {"name": "test_dataset_dir",
          "type": "String"}, {"default": "7", "name": "seq_len", "optional": true,
          "type": "Integer"}], "name": "Predict", "outputs": [{"name": "predictions_dir",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"seq_len":
          "7"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: preprocess-dataset
    container:
      args: [--dataframe, /tmp/inputs/dataframe/data, --validation-dataset-dir, /tmp/outputs/validation_dataset_dir/data,
        --train-dataset-dir, /tmp/outputs/train_dataset_dir/data, --dataset-encoder-dir,
        /tmp/outputs/dataset_encoder_dir/data, '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'imbalanced-learn' 'scikit-learn' 'sklearn-pandas' 'dill' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'imbalanced-learn'
        'scikit-learn' 'sklearn-pandas' 'dill' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def preprocess_dataset(
            dataframe,
            validation_dataset_dir,
            train_dataset_dir,
            dataset_encoder_dir,
        ):

            from imblearn.over_sampling import RandomOverSampler
            import math
            import numpy as np
            import os
            import pandas as pd
            import dill

            from sklearn.impute import SimpleImputer
            from sklearn.preprocessing import (
                LabelEncoder,
                OneHotEncoder,
                FunctionTransformer,
                MinMaxScaler,
                LabelBinarizer,
            )
            from sklearn_pandas import DataFrameMapper

            def save_to_dir(x, y, directory):
                if not os.path.exists(directory):
                    os.makedirs(directory)
                np.savez(os.path.join(directory, "data.npz"), x=x, y=y)

            def split_dataset(n, df):
                test = df.iloc[:n, :]
                train = df.iloc[n:, :]
                return test, train

            def merge_splits(frauds, non_frauds, split):
                print(
                    f"{split} ratio fraud ({len(frauds)}) / non-fraud ({len(non_frauds)}):",
                    len(frauds) / len(non_frauds),
                )
                df = pd.concat([frauds, non_frauds])
                df.sort_values("year_month_day_time", inplace=True)

                x, y = df.drop(["is fraud?"], axis=1), df["is fraud?"]
                min_ind = math.floor(len(x) / 128)
                x, y = x[-min_ind * 128 :], y[-min_ind * 128 :]
                y = y.astype("int")
                return x, y

            def timeEncoder(X):
                X_hm = X["time"].str.split(":", expand=True)
                d = pd.to_datetime(
                    dict(
                        year=X["year"],
                        month=X["month"],
                        day=X["day"],
                        hour=X_hm[0],
                        minute=X_hm[1],
                    )
                ).astype(int)
                return pd.DataFrame(d)

            def amtEncoder(X):
                amt = (
                    X.apply(lambda x: x[1:])
                    .astype(float)
                    .map(lambda amt: max(1, amt))
                    .map(math.log)
                )
                return pd.DataFrame(amt)

            def decimalEncoder(X, length=5):
                dnew = pd.DataFrame()
                for i in range(length):
                    dnew[i] = np.mod(X, 10)
                    X = np.floor_divide(X, 10)
                return dnew

            def fraudEncoder(X):
                return np.where(X == "Yes", 1, 0).astype(int)

            def whitespace_remover(dataframe):
                # Identify columns with data type 'object'
                str_columns = dataframe.select_dtypes(["object"]).columns

                # Apply strip operation only to string objects, skip others
                dataframe[str_columns] = dataframe[str_columns].applymap(
                    lambda x: x.strip() if isinstance(x, str) else x
                )

                return dataframe

            # df_nf = pd.read_csv(f"{os.getenv('HOME')}/card_transactions_non-frauds.csv")
            # df_f = pd.read_csv(f"{os.getenv('HOME')}/card_transactions_frauds.csv")
            # tdf = pd.concat([df_nf, df_f])
            print("read in raw data")
            tdf = pd.read_feather(dataframe).iloc[20:]
            tdf.columns = map(str.lower, tdf.columns)
            tdf["merchant name"] = tdf["merchant name"].astype(str)
            tdf.drop(["mcc", "zip", "merchant state"], axis=1, inplace=True)
            tdf.sort_values(by=["user", "card"], inplace=True)
            tdf.reset_index(inplace=True, drop=True)
            whitespace_remover(tdf)

            encoders = {
                "merchant_name": LabelEncoder().fit(tdf["merchant name"]),
                "merchant_city": LabelEncoder().fit(tdf["merchant city"]),
            }

            mapper = DataFrameMapper(
                [
                    ("is fraud?", FunctionTransformer(fraudEncoder)),
                    (
                        "merchant name",
                        [
                            encoders["merchant_name"],
                            FunctionTransformer(decimalEncoder),
                            OneHotEncoder(handle_unknown="ignore"),
                        ],
                    ),
                    (
                        "merchant city",
                        [
                            encoders["merchant_city"],
                            FunctionTransformer(decimalEncoder),
                            OneHotEncoder(handle_unknown="ignore"),
                        ],
                    ),
                    (["use chip"], [SimpleImputer(strategy="constant"), LabelBinarizer()]),
                    (["errors?"], [SimpleImputer(strategy="constant"), LabelBinarizer()]),
                    (
                        ["year", "month", "day", "time"],
                        [FunctionTransformer(timeEncoder), MinMaxScaler()],
                    ),
                    ("amount", [FunctionTransformer(amtEncoder), MinMaxScaler()]),
                ],
                input_df=True,
                df_out=True,
            )

            print("fit and transform dataframe")
            mapper.fit(tdf)
            tdf = mapper.transform(tdf)

            # save encoders to disk
            if not os.path.exists(dataset_encoder_dir):
                print("creating encoder dir")
                os.makedirs(dataset_encoder_dir)
            with open(os.path.join(dataset_encoder_dir, "mapper.pkl"), "wb") as f:
                dill.dump(mapper, f)

            print("encoders successfully archived")

            dataset = tdf
            dataset = dataset.sample(frac=1)  # shuffle randomly

            frauds = dataset[dataset["is fraud?"] == 1]
            non_frauds = dataset[dataset["is fraud?"] == 0]
            ratio = len(frauds) / len(non_frauds)
            print(
                f"{len(frauds)} Frauds ({len(frauds)/len(dataset)*100}%) and {len(non_frauds)} Non-Frauds ({len(non_frauds)/len(dataset)*100}%) - ratio: {ratio})."
            )

            test_ratio = 0.1
            n_test_frauds = int(test_ratio * len(frauds))
            n_test_non_frauds = int(test_ratio * len(non_frauds))
            n_train_frauds = len(frauds) - n_test_frauds
            n_train_non_frauds = len(non_frauds) - n_test_non_frauds
            # n_frauds = int(0.001 * len(dataset))
            # n_non_frauds = int(len(dataset) * 0.2 - n_frauds)

            print(f"Frauds in test split: {n_test_frauds}")
            test_frauds, train_frauds = split_dataset(n_test_frauds, frauds)

            print(f"Non-Frauds in test split: {n_test_non_frauds}")
            test_non_frauds, train_non_frauds = split_dataset(n_test_non_frauds, non_frauds)

            x_train, y_train = merge_splits(train_frauds, train_non_frauds, "Train")
            x_test, y_test = merge_splits(test_frauds, test_non_frauds, "Test")
            print(
                f"Using the following y-label: {y_train.name} and x-features: {x_train.columns}"
            )

            over_sampler = RandomOverSampler(random_state=37, sampling_strategy=0.1)
            train_input, train_target = over_sampler.fit_resample(x_train, y_train)
            # train_input, train_target = x_train, y_train # use this if you don't want to oversample
            print(
                sum(train_target == 0),
                "negative &",
                sum(train_target == 1),
                "positive training samples (after upsampling)",
            )
            print(
                sum(y_test == 0),
                "negative &",
                sum(y_test == 1),
                "positive test samples",
            )
            train = pd.concat([pd.DataFrame(train_target), pd.DataFrame(train_input)], axis=1)
            train.columns = dataset.columns
            train.sort_values("year_month_day_time", inplace=True)
            train_input, train_target = train.drop(["is fraud?"], axis=1), train["is fraud?"]

            train_target = train_target.to_numpy().reshape(len(train_target), 1)
            y_test = y_test.to_numpy().reshape(len(y_test), 1)

            save_to_dir(train_input.to_numpy(), train_target, train_dataset_dir)
            save_to_dir(x_test.to_numpy(), y_test, validation_dataset_dir)

            print(f"Pre-processed train dataset saved. Contents of '{train_dataset_dir}':")
            print(os.listdir("/".join(str(train_dataset_dir).split("/")[:-1])))
            print(f"Pre-processed test dataset saved. Contents of '{validation_dataset_dir}':")
            print(os.listdir("/".join(str(validation_dataset_dir).split("/")[:-1])))

            print(train_input.columns)
            return list(train_input.columns)

        def _serialize_json(obj) -> str:
            if isinstance(obj, str):
                return obj
            import json

            def default_serializer(obj):
                if hasattr(obj, 'to_struct'):
                    return obj.to_struct()
                else:
                    raise TypeError(
                        "Object of type '%s' is not JSON serializable and does not have .to_struct() method."
                        % obj.__class__.__name__)

            return json.dumps(obj, default=default_serializer, sort_keys=True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Preprocess dataset', description='')
        _parser.add_argument("--dataframe", dest="dataframe", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--validation-dataset-dir", dest="validation_dataset_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-dataset-dir", dest="train_dataset_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--dataset-encoder-dir", dest="dataset_encoder_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = preprocess_dataset(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_json,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      artifacts:
      - {name: load-dataframe-via-trino-dataframe, path: /tmp/inputs/dataframe/data}
    outputs:
      artifacts:
      - {name: preprocess-dataset-Output, path: /tmp/outputs/Output/data}
      - {name: preprocess-dataset-dataset_encoder_dir, path: /tmp/outputs/dataset_encoder_dir/data}
      - {name: preprocess-dataset-train_dataset_dir, path: /tmp/outputs/train_dataset_dir/data}
      - {name: preprocess-dataset-validation_dataset_dir, path: /tmp/outputs/validation_dataset_dir/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--dataframe", {"inputPath": "dataframe"}, "--validation-dataset-dir",
          {"outputPath": "validation_dataset_dir"}, "--train-dataset-dir", {"outputPath":
          "train_dataset_dir"}, "--dataset-encoder-dir", {"outputPath": "dataset_encoder_dir"},
          "----output-paths", {"outputPath": "Output"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''imbalanced-learn''
          ''scikit-learn'' ''sklearn-pandas'' ''dill'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''imbalanced-learn''
          ''scikit-learn'' ''sklearn-pandas'' ''dill'' --user) && \"$0\" \"$@\"",
          "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef preprocess_dataset(\n    dataframe,\n    validation_dataset_dir,\n    train_dataset_dir,\n    dataset_encoder_dir,\n):\n\n    from
          imblearn.over_sampling import RandomOverSampler\n    import math\n    import
          numpy as np\n    import os\n    import pandas as pd\n    import dill\n\n    from
          sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import
          (\n        LabelEncoder,\n        OneHotEncoder,\n        FunctionTransformer,\n        MinMaxScaler,\n        LabelBinarizer,\n    )\n    from
          sklearn_pandas import DataFrameMapper\n\n    def save_to_dir(x, y, directory):\n        if
          not os.path.exists(directory):\n            os.makedirs(directory)\n        np.savez(os.path.join(directory,
          \"data.npz\"), x=x, y=y)\n\n    def split_dataset(n, df):\n        test
          = df.iloc[:n, :]\n        train = df.iloc[n:, :]\n        return test, train\n\n    def
          merge_splits(frauds, non_frauds, split):\n        print(\n            f\"{split}
          ratio fraud ({len(frauds)}) / non-fraud ({len(non_frauds)}):\",\n            len(frauds)
          / len(non_frauds),\n        )\n        df = pd.concat([frauds, non_frauds])\n        df.sort_values(\"year_month_day_time\",
          inplace=True)\n\n        x, y = df.drop([\"is fraud?\"], axis=1), df[\"is
          fraud?\"]\n        min_ind = math.floor(len(x) / 128)\n        x, y = x[-min_ind
          * 128 :], y[-min_ind * 128 :]\n        y = y.astype(\"int\")\n        return
          x, y\n\n    def timeEncoder(X):\n        X_hm = X[\"time\"].str.split(\":\",
          expand=True)\n        d = pd.to_datetime(\n            dict(\n                year=X[\"year\"],\n                month=X[\"month\"],\n                day=X[\"day\"],\n                hour=X_hm[0],\n                minute=X_hm[1],\n            )\n        ).astype(int)\n        return
          pd.DataFrame(d)\n\n    def amtEncoder(X):\n        amt = (\n            X.apply(lambda
          x: x[1:])\n            .astype(float)\n            .map(lambda amt: max(1,
          amt))\n            .map(math.log)\n        )\n        return pd.DataFrame(amt)\n\n    def
          decimalEncoder(X, length=5):\n        dnew = pd.DataFrame()\n        for
          i in range(length):\n            dnew[i] = np.mod(X, 10)\n            X
          = np.floor_divide(X, 10)\n        return dnew\n\n    def fraudEncoder(X):\n        return
          np.where(X == \"Yes\", 1, 0).astype(int)\n\n    def whitespace_remover(dataframe):\n        #
          Identify columns with data type ''object''\n        str_columns = dataframe.select_dtypes([\"object\"]).columns\n\n        #
          Apply strip operation only to string objects, skip others\n        dataframe[str_columns]
          = dataframe[str_columns].applymap(\n            lambda x: x.strip() if isinstance(x,
          str) else x\n        )\n\n        return dataframe\n\n    # df_nf = pd.read_csv(f\"{os.getenv(''HOME'')}/card_transactions_non-frauds.csv\")\n    #
          df_f = pd.read_csv(f\"{os.getenv(''HOME'')}/card_transactions_frauds.csv\")\n    #
          tdf = pd.concat([df_nf, df_f])\n    print(\"read in raw data\")\n    tdf
          = pd.read_feather(dataframe).iloc[20:]\n    tdf.columns = map(str.lower,
          tdf.columns)\n    tdf[\"merchant name\"] = tdf[\"merchant name\"].astype(str)\n    tdf.drop([\"mcc\",
          \"zip\", \"merchant state\"], axis=1, inplace=True)\n    tdf.sort_values(by=[\"user\",
          \"card\"], inplace=True)\n    tdf.reset_index(inplace=True, drop=True)\n    whitespace_remover(tdf)\n\n    encoders
          = {\n        \"merchant_name\": LabelEncoder().fit(tdf[\"merchant name\"]),\n        \"merchant_city\":
          LabelEncoder().fit(tdf[\"merchant city\"]),\n    }\n\n    mapper = DataFrameMapper(\n        [\n            (\"is
          fraud?\", FunctionTransformer(fraudEncoder)),\n            (\n                \"merchant
          name\",\n                [\n                    encoders[\"merchant_name\"],\n                    FunctionTransformer(decimalEncoder),\n                    OneHotEncoder(handle_unknown=\"ignore\"),\n                ],\n            ),\n            (\n                \"merchant
          city\",\n                [\n                    encoders[\"merchant_city\"],\n                    FunctionTransformer(decimalEncoder),\n                    OneHotEncoder(handle_unknown=\"ignore\"),\n                ],\n            ),\n            ([\"use
          chip\"], [SimpleImputer(strategy=\"constant\"), LabelBinarizer()]),\n            ([\"errors?\"],
          [SimpleImputer(strategy=\"constant\"), LabelBinarizer()]),\n            (\n                [\"year\",
          \"month\", \"day\", \"time\"],\n                [FunctionTransformer(timeEncoder),
          MinMaxScaler()],\n            ),\n            (\"amount\", [FunctionTransformer(amtEncoder),
          MinMaxScaler()]),\n        ],\n        input_df=True,\n        df_out=True,\n    )\n\n    print(\"fit
          and transform dataframe\")\n    mapper.fit(tdf)\n    tdf = mapper.transform(tdf)\n\n    #
          save encoders to disk\n    if not os.path.exists(dataset_encoder_dir):\n        print(\"creating
          encoder dir\")\n        os.makedirs(dataset_encoder_dir)\n    with open(os.path.join(dataset_encoder_dir,
          \"mapper.pkl\"), \"wb\") as f:\n        dill.dump(mapper, f)\n\n    print(\"encoders
          successfully archived\")\n\n    dataset = tdf\n    dataset = dataset.sample(frac=1)  #
          shuffle randomly\n\n    frauds = dataset[dataset[\"is fraud?\"] == 1]\n    non_frauds
          = dataset[dataset[\"is fraud?\"] == 0]\n    ratio = len(frauds) / len(non_frauds)\n    print(\n        f\"{len(frauds)}
          Frauds ({len(frauds)/len(dataset)*100}%) and {len(non_frauds)} Non-Frauds
          ({len(non_frauds)/len(dataset)*100}%) - ratio: {ratio}).\"\n    )\n\n    test_ratio
          = 0.1\n    n_test_frauds = int(test_ratio * len(frauds))\n    n_test_non_frauds
          = int(test_ratio * len(non_frauds))\n    n_train_frauds = len(frauds) -
          n_test_frauds\n    n_train_non_frauds = len(non_frauds) - n_test_non_frauds\n    #
          n_frauds = int(0.001 * len(dataset))\n    # n_non_frauds = int(len(dataset)
          * 0.2 - n_frauds)\n\n    print(f\"Frauds in test split: {n_test_frauds}\")\n    test_frauds,
          train_frauds = split_dataset(n_test_frauds, frauds)\n\n    print(f\"Non-Frauds
          in test split: {n_test_non_frauds}\")\n    test_non_frauds, train_non_frauds
          = split_dataset(n_test_non_frauds, non_frauds)\n\n    x_train, y_train =
          merge_splits(train_frauds, train_non_frauds, \"Train\")\n    x_test, y_test
          = merge_splits(test_frauds, test_non_frauds, \"Test\")\n    print(\n        f\"Using
          the following y-label: {y_train.name} and x-features: {x_train.columns}\"\n    )\n\n    over_sampler
          = RandomOverSampler(random_state=37, sampling_strategy=0.1)\n    train_input,
          train_target = over_sampler.fit_resample(x_train, y_train)\n    # train_input,
          train_target = x_train, y_train # use this if you don''t want to oversample\n    print(\n        sum(train_target
          == 0),\n        \"negative &\",\n        sum(train_target == 1),\n        \"positive
          training samples (after upsampling)\",\n    )\n    print(\n        sum(y_test
          == 0),\n        \"negative &\",\n        sum(y_test == 1),\n        \"positive
          test samples\",\n    )\n    train = pd.concat([pd.DataFrame(train_target),
          pd.DataFrame(train_input)], axis=1)\n    train.columns = dataset.columns\n    train.sort_values(\"year_month_day_time\",
          inplace=True)\n    train_input, train_target = train.drop([\"is fraud?\"],
          axis=1), train[\"is fraud?\"]\n\n    train_target = train_target.to_numpy().reshape(len(train_target),
          1)\n    y_test = y_test.to_numpy().reshape(len(y_test), 1)\n\n    save_to_dir(train_input.to_numpy(),
          train_target, train_dataset_dir)\n    save_to_dir(x_test.to_numpy(), y_test,
          validation_dataset_dir)\n\n    print(f\"Pre-processed train dataset saved.
          Contents of ''{train_dataset_dir}'':\")\n    print(os.listdir(\"/\".join(str(train_dataset_dir).split(\"/\")[:-1])))\n    print(f\"Pre-processed
          test dataset saved. Contents of ''{validation_dataset_dir}'':\")\n    print(os.listdir(\"/\".join(str(validation_dataset_dir).split(\"/\")[:-1])))\n\n    print(train_input.columns)\n    return
          list(train_input.columns)\n\ndef _serialize_json(obj) -> str:\n    if isinstance(obj,
          str):\n        return obj\n    import json\n\n    def default_serializer(obj):\n        if
          hasattr(obj, ''to_struct''):\n            return obj.to_struct()\n        else:\n            raise
          TypeError(\n                \"Object of type ''%s'' is not JSON serializable
          and does not have .to_struct() method.\"\n                % obj.__class__.__name__)\n\n    return
          json.dumps(obj, default=default_serializer, sort_keys=True)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Preprocess dataset'', description='''')\n_parser.add_argument(\"--dataframe\",
          dest=\"dataframe\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--validation-dataset-dir\",
          dest=\"validation_dataset_dir\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-dataset-dir\",
          dest=\"train_dataset_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataset-encoder-dir\",
          dest=\"dataset_encoder_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\",
          type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = preprocess_dataset(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_json,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}}, "inputs":
          [{"name": "dataframe", "type": "String"}], "name": "Preprocess dataset",
          "outputs": [{"name": "validation_dataset_dir", "type": "String"}, {"name":
          "train_dataset_dir", "type": "String"}, {"name": "dataset_encoder_dir",
          "type": "String"}, {"name": "Output", "type": "typing.List[str]"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: train-model-job
    container:
      args:
      - --train-dataset-dir
      - /tmp/inputs/train_dataset_dir/data
      - --validation-dataset-dir
      - /tmp/inputs/validation_dataset_dir/data
      - --train-specification
      - |
        name: Train model
        inputs:
        - {name: train_dataset_dir, type: String}
        - {name: validation_dataset_dir, type: String}
        - {name: epochs, type: Integer, default: '10', optional: true}
        - {name: seqlen, type: Integer, default: '7', optional: true}
        outputs:
        - {name: model_dir, type: String}
        implementation:
          container:
            image: python:3.7
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_model(
                  model_dir,
                  train_dataset_dir,
                  validation_dataset_dir,
                  epochs = 10,
                  seqlen = 7,
              ):
                  import numpy as np
                  import os
                  from tensorflow import keras
                  from tensorflow.keras.callbacks import (
                      EarlyStopping,
                      ModelCheckpoint,
                      ReduceLROnPlateau,
                      TensorBoard,
                  )
                  from tensorflow.keras.layers import Input, LSTM, Dense
                  from tensorflow.keras.metrics import (
                      TruePositives,
                      FalsePositives,
                      FalseNegatives,
                      TrueNegatives,
                  )

                  def load_dataset(path):
                      data = np.load(os.path.join(path, "data.npz"), allow_pickle=True)
                      x, y = data["x"], data["y"]
                      x = np.asarray(x).astype(np.float32)
                      y = np.asarray(y).astype(np.int_)
                      dataset = keras.preprocessing.timeseries_dataset_from_array(
                          x, y, sequence_length=seqlen, batch_size=128
                      )
                      return dataset

                  if not os.path.exists(model_dir):
                      os.makedirs(model_dir)

                  train_dataset = load_dataset(train_dataset_dir)
                  test_dataset = load_dataset(validation_dataset_dir)

                  for batch in train_dataset.take(1):
                      input_d, targets = batch
                  print("Input shape:", input_d.numpy().shape, "Target shape:", targets.numpy().shape)

                  input_shape = (input_d.shape[1], input_d.shape[2])
                  inputs = Input(shape=input_shape)
                  lstm_in = LSTM(100, batch_size=7, return_sequences=True)(inputs)
                  lstm_out = LSTM(100, batch_size=7)(lstm_in)
                  outputs = Dense(1, activation="sigmoid")(lstm_out)
                  model = keras.Model(inputs=inputs, outputs=outputs)

                  metrics = [
                      "accuracy",
                      TruePositives(name="tp"),
                      FalsePositives(name="fp"),
                      FalseNegatives(name="fn"),
                      TrueNegatives(name="tn"),
                  ]
                  # loss = keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True)
                  model.compile(optimizer="adam", loss="binary_crossentropy", metrics=metrics)
                  print(model.summary())

                  print("Initializing training callbacks...")
                  callbacks = [
                      EarlyStopping(monitor="loss", patience=20, verbose=0, mode="min"),
                      ModelCheckpoint(
                          f"{model_dir}/best_model.keras",
                          monitor="loss",
                          save_best_only=True,
                          save_weights_only=True,
                          mode="min",
                      ),
                      ReduceLROnPlateau(
                          monitor="loss",
                          factor=0.1,
                          patience=7,
                          verbose=1,
                          min_delta=0.0001,
                          mode="min",
                      ),
                      TensorBoard(
                          log_dir=os.environ["TENSORBOARD_S3_ADDRESS"],
                          histogram_freq=1,
                      ),
                  ]

                  model.fit(
                      train_dataset,
                      epochs=epochs,
                      verbose=3,
                      callbacks=callbacks,
                  )

                  results = model.evaluate(test_dataset)
                  print("Evaluation Loss, Accuracy, TP, FP, FN, TN:", results)
                  TP, FP, FN, TN = results[2:]
                  if TP != 0:
                      PR = TP / (FP + TP)
                      RE = TP / (FN + TP)
                      print("F1 Measure:", 2 * (PR * RE / (PR + RE)))

                  model.save(model_dir)

              import argparse
              _parser = argparse.ArgumentParser(prog='Train model', description='')
              _parser.add_argument("--train-dataset-dir", dest="train_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--validation-dataset-dir", dest="validation_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--epochs", dest="epochs", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--seqlen", dest="seqlen", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model-dir", dest="model_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_model(**_parsed_args)
            args:
            - --train-dataset-dir
            - {inputPath: train_dataset_dir}
            - --validation-dataset-dir
            - {inputPath: validation_dataset_dir}
            - if:
                cond: {isPresent: epochs}
                then:
                - --epochs
                - {inputValue: epochs}
            - if:
                cond: {isPresent: seqlen}
                then:
                - --seqlen
                - {inputValue: seqlen}
            - --model-dir
            - {outputPath: model_dir}
      - --train-parameters
      - '{"epochs": "2", "model_dir": "model_dir", "seqlen": "4", "train_dataset_dir":
        "train_dataset_dir", "validation_dataset_dir": "validation_dataset_dir"}'
      - --train-mount
      - /train
      - --model-name
      - '{{inputs.parameters.model_name}}'
      - --base-image
      - quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
      - --node-selector
      - '{{inputs.parameters.training_node_selector}}'
      - --pvc-name
      - ''
      - --pvc-size
      - 10Gi
      - --cpus
      - ''
      - --gpus
      - '{{inputs.parameters.training_gpus}}'
      - --memory
      - ''
      - --tensorboard-s3-address
      - '{{inputs.parameters.monitor-training-tensorboard_s3_address}}'
      - --cluster-configuration-secret
      - '{{inputs.parameters.cluster_configuration_secret}}'
      - --model-dir
      - /tmp/outputs/model_dir/data
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def Train_Model_Job(
            train_dataset_dir,
            validation_dataset_dir,
            train_specification,
            train_parameters,
            model_dir,
            train_mount = "/train",
            model_name = "my-model",
            base_image = "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest",
            node_selector = "",
            pvc_name = "",
            pvc_size = "10Gi",
            cpus = "",
            gpus = 0,
            memory = "",
            tensorboard_s3_address = "",
            cluster_configuration_secret = "",
            distribution_specification = None,
        ):
            """
            Trains a model. Once trained, the model is persisted to model_dir.

                    Parameters:
                            train_dataset_dir: Path to the directory with training data.
                            validation_dataset_dir: Path to the directory with validation data to be used during training.
                            train_specification: Training command as generated from a Python function using kfp.components.func_to_component_text.
                            train_parameters: Dictionary mapping formal to actual parameters for the training spacification.
                            model_dir: Target path where the model will be stored.
                            train_mount: Optional mounting point for training data of an existing PVC. Example: "/train".
                            model_name: Optional name of the model. Must be unique for the targeted namespace and conform Kubernetes naming conventions. Example: my-model.
                            base_image: Optional base image for model training. Example: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest.
                            node_selector: Optional node selector for worker nodes. Example: nvidia.com/gpu.product: "Tesla-V100-SXM2-32GB".
                            pvc_name: Optional name to an existing persistent volume claim (pvc). If given, this pvc is mounted into the training job. Example: "music-genre-classification-j4ssf-training-pvc".
                            pvc_size: Optional size of the storage during model training. Storage is mounted into to the Job based on a persitent volume claim of the given size. Example: 10Gi.
                            cpus: Optional CPU limit for the job. Leave empty for cluster defaults (typically no limit). Example: "1000m".
                            gpus: Optional number of GPUs for the job. Example: 2.
                            memory: Optional memory limit for the job. Leave empty for cluster defaults (typically no limit). Example: "1Gi".
                            tensorboard_s3_address: Optional s3 address where Tensorboard logs shall be stored. Example: "s3://mlpipeline/tensorboard/my-train-job".
                            cluster_configuration_secret: Optional secret name configuring a (remote) Kubernetes cluster to run the job in and the backing MinIO object store. All secret's data values are optional and appropriate defaults are chosen if not present. The secret may provide a suitable kubernetes bearer token, the associated namespace, a host, etc. Example: "remote-power-cluster".
                            distribution_specification: Optional dictionary specifiying the distribution behavior. By default, no distributed training is executed, which results in an ordinary Kubernetes Job  for training. Otherwise, dictionary entries determine the distribution behavior. The "distribution_type" entry determines the distribution type: "Job" (no distribution; ordinary Kubernetes job), "MPI" (all-reduce style distribution via Horovod), or "TF" (parameter-server style distribution via distributed training with TensorFlow). Depending on the distribution type, additional dictionary entries can be processed. For distributed training jobs, the "number_of_workers" (e.g., 2) determines the number of worker replicas for training. Individual resource limits can be controlled via "worker_cpus" (e.g., "1000m") and "worker_memory" (e.g., "1Gi"). MPI additionally provides a fine-grained control of launcher cpu and memory limits via "launcher_cpus" (e.g., "1000m") and "launcher_memory" (e.g., "1Gi"). Full example with MPI: {"distribution_type": "MPI", "number_of_workers": 2, "worker_cpus": "8", "worker_memory": "32Gi", "launcher_cpus": "2", "launcher_memory": "8Gi"}
            """
            from datetime import datetime
            import errno
            import json
            import kfp
            from kubernetes import client, config, utils, watch
            import logging
            import os
            import shutil
            import sys
            import yaml

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )
            logger = logging.getLogger()

            ###########################################################################
            # Helper Functions
            ###########################################################################

            def establish_local_cluster_connection():
                config.load_incluster_config()
                return client.ApiClient()

            def get_cluster_configuration(api_client, cluster_configuration_secret):
                import base64
                from kubernetes.client.rest import ApiException

                def decode(secret, key):
                    data = secret.data[key]
                    decoded_data = base64.b64decode(data)
                    return decoded_data.decode("utf-8")

                def update_with_secret(secret, dictionary):
                    for key in dictionary:
                        if key in secret.data:
                            dictionary[key] = decode(secret, key)

                cluster_configuration = {
                    "access-mode": "ReadWriteMany",
                    "minio-accesskey": "minio",
                    "minio-bucket": "mlpipeline",
                    "minio-job-folder": "jobs",
                    "minio-secretkey": "minio123",
                    "minio-url": "http://minio-service.kubeflow:9000",
                    "remote-host": "",
                    "remote-namespace": "",
                    "remote-token": "",
                }

                try:
                    default_minio_secret = client.CoreV1Api(api_client).read_namespaced_secret(
                        "mlpipeline-minio-artifact", get_current_namespace()
                    )

                    if default_minio_secret.data is None:
                        logger.info(
                            "MinIO secret (mlpipeline-minio-artifact) includes no data - progressing with default values."
                        )
                    else:
                        logger.info(
                            "Found default MinIO secret (mlpipeline-minio-artifact) - updating cluster configuration accordingly."
                        )
                        cluster_configuration["minio-accesskey"] = decode(
                            default_minio_secret, "accesskey"
                        )
                        cluster_configuration["minio-secretkey"] = decode(
                            default_minio_secret, "secretkey"
                        )
                except ApiException as e:
                    if e.status == 404:
                        logger.info(
                            "Found no default MinIO secret (mlpipeline-minio-artifact) - progressing with default values."
                        )

                if cluster_configuration_secret == "":
                    logger.info(
                        "No cluster configuration secret specified - progressing with default values."
                    )
                    return cluster_configuration

                try:
                    secret = client.CoreV1Api(api_client).read_namespaced_secret(
                        cluster_configuration_secret, get_current_namespace()
                    )
                    if secret.data is None:
                        logger.info(
                            f"Cluster configuration secret ({cluster_configuration_secret}) includes no data - progressing with default values."
                        )
                    else:
                        logger.info(
                            f"Found cluster configuration secret ({cluster_configuration_secret}) - updating cluster configuration accordingly."
                        )
                        update_with_secret(secret, cluster_configuration)
                except ApiException as e:
                    if e.status == 404:
                        logger.info(
                            f"Found no cluster configuration secret ({cluster_configuration_secret}) - progressing with default values."
                        )

                return cluster_configuration

            def establish_training_cluster_connection(local_api_client, cluster_configuration):
                is_remote = False
                if (
                    cluster_configuration["remote-host"] == ""
                    or cluster_configuration["remote-token"] == ""
                ):
                    logger.info(
                        "Remote cluster not configured. Using in-cluster configuration..."
                    )
                    logger.info(
                        "Note: assign the name of a secret to the 'cluster_configuration_secret' pipeline argument and add the secret to your cluster."
                    )
                    logger.info("Example secret:")
                    logger.info("---")
                    logger.info("apiVersion: v1")
                    logger.info("kind: Secret")
                    logger.info("metadata:")
                    logger.info("  name: my-remote-cluster")
                    logger.info("stringData:")
                    logger.info("  access-mode: ReadWriteOnce")
                    logger.info("  minio-accesskey: minio")
                    logger.info("  minio-bucket: mlpipeline")
                    logger.info("  minio-job-folder: jobs")
                    logger.info("  minio-secretkey: minio123")
                    logger.info("  minio-url: http://minio-service.kubeflow:9000")
                    logger.info(
                        "  remote-host: https://istio-ingressgateway-istio-system.apps.mydomain.ai:6443"
                    )
                    logger.info("  remote-namespace: default")
                    logger.info("  remote-token: eyJh...")
                    logger.info("---")
                    logger.info(
                        "Where you get the remote-token from your remote cluster as described here:"
                    )
                    logger.info(
                        "https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#without-kubectl-proxy"
                    )

                    api_client = local_api_client
                    if not os.path.exists(train_mount):
                        logger.warning(
                            f"No local mount to {train_mount} found. Therefore, switching to remote data synchronization mode via MinIO. This will work but is slower compared to local mounts. Consider adding a mount to '{train_mount}' for this component by using a PVC inside your pipeline."
                        )
                        is_remote = True
                else:
                    # see: https://github.com/kubernetes-client/python/blob/6d4587e18064288d031ed9bbf5ab5b8245460b3c/examples/remote_cluster.py
                    logger.info(
                        "Remote host and token found. Using remote cluster configuration..."
                    )
                    configuration = client.Configuration()
                    configuration.host = cluster_configuration["remote-host"]
                    configuration.verify_ssl = False
                    configuration.api_key = {
                        "authorization": "Bearer " + cluster_configuration["remote-token"]
                    }
                    api_client = client.ApiClient(configuration)
                    is_remote = True

                return (api_client, is_remote)

            def clone_path(source, target):
                try:
                    logger.info(f"Cloning source path {source} to {target} of training job...")
                    shutil.copytree(source, target)
                    logger.info("Cloning finished. Target path contents:")
                    logger.info(os.listdir(target))
                except OSError as e:
                    if e.errno in (errno.ENOTDIR, errno.EINVAL):
                        shutil.copy(source, target)
                    else:
                        raise

            def sync_with_minio(
                cluster_configuration,
                inputs,
                job_name,
                is_upload,
                remove_minio_files = False,
            ):
                import boto3
                import botocore
                from botocore.client import Config
                import json
                import logging
                import os
                import sys
                import tarfile

                logging.basicConfig(
                    stream=sys.stdout,
                    level=logging.INFO,
                    format="%(levelname)s %(asctime)s: %(message)s",
                )
                logger = logging.getLogger()

                def establish_minio_connection(cluster_configuration):
                    if ("minio-accesskey" in cluster_configuration) and (
                        "minio-secretkey" in cluster_configuration
                    ):
                        minio_user = cluster_configuration["minio-accesskey"]
                        minio_pass = cluster_configuration["minio-secretkey"]
                    else:
                        minio_user = os.getenv("MINIO_USER")
                        minio_pass = os.getenv("MINIO_PASS")

                    if minio_user == "" or minio_pass == "":
                        err = "Environment variables MINIO_USER and MINIO_PASS need externally to be provided to this component using k8s_secret_key_to_env!"
                        raise Exception(err)

                    return boto3.session.Session().resource(
                        service_name="s3",
                        endpoint_url=cluster_configuration["minio-url"],
                        aws_access_key_id=minio_user,
                        aws_secret_access_key=minio_pass,
                        config=Config(signature_version="s3v4"),
                    )

                def path_to_tarfilename(pathname):
                    return f"{pathname.replace(os.sep, '-')}.tar.gz"

                def make_tarfile(output_filename, source_dir):
                    with tarfile.open(output_filename, "w:gz") as tar:
                        tar.add(source_dir, arcname=".")

                # see: https://stackoverflow.com/a/47565719/2625096
                def bucket_exists(minio_client, bucket):
                    try:
                        minio_client.meta.client.head_bucket(Bucket=bucket.name)
                        return True
                    except botocore.exceptions.ClientError as e:
                        error_code = int(e.response["Error"]["Code"])
                        if error_code == 403:
                            # Forbidden Access -> Private Bucket
                            return True
                        elif error_code == 404:
                            return False

                def upload_to_minio(file, upload_bucket, job_folder, job_name, minio_client):
                    bucket = minio_client.Bucket(upload_bucket)

                    if not bucket_exists(minio_client, bucket):
                        minio_client.create_bucket(Bucket=bucket.name)

                    bucket.upload_file(file, f"{job_folder}/{job_name}/{file}")

                def download_from_minio(
                    file, upload_bucket, job_folder, job_name, minio_client, remove_minio_file
                ):
                    bucket = minio_client.Bucket(upload_bucket)
                    key = f"{job_folder}/{job_name}/{file}"

                    bucket.download_file(key, file)

                    if remove_minio_file:
                        bucket.Object(key).delete()

                def extract_tarfile(tarfile_name, target):
                    with tarfile.open(tarfile_name, "r:gz") as tar_gz_ref:
                        tar_gz_ref.extractall(target)

                if isinstance(cluster_configuration, str):
                    cluster_configuration = json.loads(cluster_configuration)

                if isinstance(inputs, str):
                    inputs = json.loads(inputs)

                if isinstance(is_upload, str):
                    if is_upload == "True":
                        is_upload = True
                    else:
                        is_upload = False

                logger.info("Establishing MinIO connection...")
                minio_client = establish_minio_connection(cluster_configuration)

                for (source, target) in inputs:
                    tarfilename = path_to_tarfilename(source)

                    if is_upload:
                        logger.info(f"Tar.gz input {source} into {tarfilename}...")
                        make_tarfile(tarfilename, source)

                        logger.info(
                            f'Uploading {tarfilename} to {cluster_configuration["minio-bucket"]}/{cluster_configuration["minio-job-folder"]}/{job_name}/{tarfilename}...'
                        )
                        upload_to_minio(
                            tarfilename,
                            cluster_configuration["minio-bucket"],
                            cluster_configuration["minio-job-folder"],
                            job_name,
                            minio_client,
                        )
                    else:
                        logger.info(
                            f'Downloading {cluster_configuration["minio-bucket"]}/{cluster_configuration["minio-job-folder"]}/{job_name}/{tarfilename} to {tarfilename}...'
                        )
                        download_from_minio(
                            tarfilename,
                            cluster_configuration["minio-bucket"],
                            cluster_configuration["minio-job-folder"],
                            job_name,
                            minio_client,
                            remove_minio_files,
                        )

                        logger.info(f"Extracting {tarfilename} to {target}...")
                        extract_tarfile(tarfilename, target)

                        logger.info("Result:")
                        logger.info(os.listdir(target))

            def generate_unique_job_name(model_name):
                epoch = datetime.today().strftime("%Y%m%d%H%M%S")
                return f"job-{model_name}-{epoch}"

            def get_current_namespace():
                SA_NAMESPACE = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
                with open(SA_NAMESPACE) as f:
                    return f.read()

            def initialize_namespace(namespace):
                if namespace == "":
                    namespace = get_current_namespace()
                namespace_spec = f"namespace: {namespace}"

                return (namespace, namespace_spec)

            def initialize_nodeselector(node_selector):
                if node_selector != "":
                    node_selector = f"nodeSelector:\n        {node_selector}"
                return node_selector

            def initialize_init_container(
                base_image,
                cluster_configuration,
                inputs,
                is_remote,
                job_name,
                minio_secret,
                mount_path,
            ):
                if not is_remote:
                    return ""

                command_specification = kfp.components.func_to_component_text(
                    func=sync_with_minio
                )

                # inner components loose type information as needed by lists/dicts
                # -> cluster_configuration & inputs need to be a string (using json)
                cluster_configuration_json = json.dumps(
                    {
                        "minio-bucket": cluster_configuration["minio-bucket"],
                        "minio-job-folder": cluster_configuration["minio-job-folder"],
                        "minio-url": cluster_configuration["minio-url"],
                    }
                )
                inputs_json = json.dumps(inputs)
                parameters = {
                    "cluster_configuration": cluster_configuration_json,
                    "inputs": inputs_json,
                    "job_name": job_name,
                    "is_upload": "False",
                }

                command, _, _ = initialize_command(command_specification, parameters)

                init_container = f"""initContainers:
                  - name: init-inputs
                    image: {base_image}
                    command: {command}
                    volumeMounts:
                    - mountPath: {mount_path}
                      name: training
                    env:
                    - name: MINIO_USER
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: accesskey
                          optional: false
                    - name: MINIO_PASS
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: secretkey
                          optional: false
        """
                return init_container

            def initialize_command(
                specification,
                parameters,
                path_parameters = {},
                mount_path = "/tmp",
            ):
                component_yaml = yaml.safe_load(specification)
                container_yaml = component_yaml["implementation"]["container"]
                command = container_yaml["command"]
                args = container_yaml["args"]

                actual_args = list()
                inputs = list()
                outputs = list()
                for idx, arg in enumerate(args):
                    if type(arg) is dict:
                        if "inputValue" in arg:
                            # required parameter (value)
                            key = arg["inputValue"]
                            if key in parameters:
                                actual_args.append(parameters[key])
                            else:
                                err = f"Required parameter '{key}' missing in component input!"
                                raise Exception(err)
                        elif "if" in arg:
                            # optional parameter
                            key = arg["if"]["cond"]["isPresent"]
                            if key in parameters:
                                actual_args.append(f"--{key}")
                                actual_args.append(parameters[key])
                        elif "inputPath" in arg:
                            # required InputPath
                            key = arg["inputPath"]
                            if key in parameters:
                                path_key = parameters[key]
                                if path_key in path_parameters:
                                    mount = f"{mount_path}{path_parameters[path_key]}"
                                    inputs.append((path_parameters[path_key], mount))
                                    actual_args.append(mount)
                                else:
                                    err = f"InputPath '{path_key}' unavailable in training component!"
                                    raise Exception(err)
                            else:
                                err = f"Required parameter '{key}' missing in component input!"
                                raise Exception(err)
                        elif "outputPath" in arg:
                            # required OutputPath
                            key = arg["outputPath"]
                            if key in parameters:
                                path_key = parameters[key]
                                if path_key in path_parameters:
                                    mount = f"{mount_path}{path_parameters[path_key]}"
                                    outputs.append((mount, path_parameters[path_key]))
                                    actual_args.append(mount)
                                else:
                                    err = f"OutputPath '{path_key}' unavailable in training component!"
                                    raise Exception(err)
                            else:
                                err = f"Required parameter '{key}' missing in component input!"
                                raise Exception(err)
                    else:
                        # required parameter (key)
                        actual_args.append(arg)

                command_with_initialized_args = json.dumps(command + actual_args)

                return command_with_initialized_args, inputs, outputs

            def initialize_fetch_command(
                cluster_configuration,
                job_name,
                outputs,
            ):
                command_specification = kfp.components.func_to_component_text(
                    func=sync_with_minio
                )

                # inner components loose type information as needed by lists/dicts
                # -> cluster_configuration & inputs need to be a string (using json)
                cluster_configuration_json = json.dumps(
                    {
                        "minio-bucket": cluster_configuration["minio-bucket"],
                        "minio-job-folder": cluster_configuration["minio-job-folder"],
                        "minio-url": cluster_configuration["minio-url"],
                    }
                )
                outputs_json = json.dumps(outputs)
                parameters = {
                    "cluster_configuration": cluster_configuration_json,
                    "inputs": outputs_json,
                    "job_name": job_name,
                    "is_upload": "True",
                }
                command, _, _ = initialize_command(command_specification, parameters)
                return command

            def create_pvc_spec(pvc_name, namespace_spec, access_mode, pvc_size):
                pvc_spec = f"""apiVersion: batch/v1
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: {pvc_name}
          {namespace_spec}
        spec:
          accessModes:
          - {access_mode}
          resources:
            requests:
              storage: {pvc_size}
        """
                return yaml.safe_load(pvc_spec)

            def create_minio_secret_spec(cluster_configuration, minio_secret, namespace_spec):
                minio_secret_spec = f"""apiVersion: v1
        kind: Secret
        metadata:
          name: {minio_secret}
          {namespace_spec}
        stringData:
          accesskey: {cluster_configuration["minio-accesskey"]}
          secretkey: {cluster_configuration["minio-secretkey"]}
        """
                return yaml.safe_load(minio_secret_spec)

            def create_train_job_configuration(
                job_name,
                namespace_spec,
                node_selector,
                base_image,
                train_command,
                train_mount,
                cpus,
                memory,
                gpus,
                init_container,
                pvc_name,
                distribution_specification,
                minio_url,
                minio_secret,
                tensorboard_s3_address,
            ):
                if cpus:
                    cpu_spec = f"cpu: {cpus}"
                else:
                    cpu_spec = ""

                if memory:
                    memory_spec = f"memory: {memory}"
                else:
                    memory_spec = ""

                if gpus:
                    gpu_spec = f"nvidia.com/gpu: {gpus}"
                else:
                    gpu_spec = ""

                if distribution_specification is None:
                    distribution_specification = dict()

                if "distribution_type" not in distribution_specification:
                    distribution_specification["distribution_type"] = "Job"

                if gpus < 1:
                    slots_per_worker = 1
                else:
                    slots_per_worker = gpus

                if "number_of_workers" in distribution_specification:
                    number_of_workers = distribution_specification["number_of_workers"]
                else:
                    number_of_workers = 2

                number_of_processes = number_of_workers * slots_per_worker

                if "launcher_cpus" in distribution_specification:
                    launcher_cpu_spec = f"cpu: {distribution_specification['launcher_cpus']}"
                else:
                    launcher_cpu_spec = ""

                if "launcher_memory" in distribution_specification:
                    launcher_memory_spec = (
                        f"memory: {distribution_specification['launcher_memory']}"
                    )
                else:
                    launcher_memory_spec = ""

                if "worker_cpus" in distribution_specification:
                    worker_cpu_spec = f"cpu: {distribution_specification['worker_cpus']}"
                else:
                    worker_cpu_spec = ""

                if "worker_memory" in distribution_specification:
                    worker_memory_spec = (
                        f"memory: {distribution_specification['worker_memory']}"
                    )
                else:
                    worker_memory_spec = ""

                if distribution_specification["distribution_type"] == "Job":
                    job_spec = f"""apiVersion: batch/v1
        kind: Job
        metadata:
          name: {job_name}
          labels:
            train-model-job: {job_name}
          {namespace_spec}
        spec:
          template:
            metadata:
              annotations:
                sidecar.istio.io/inject: "false"
            spec:
              {node_selector}
              containers:
                - name: training-container
                  image: {base_image}
                  command: {train_command}
                  volumeMounts:
                    - mountPath: {train_mount}
                      name: training
                  restartPolicy: Never
                  env:
                    - name: S3_ENDPOINT
                      value: {minio_url}
                    - name: AWS_ACCESS_KEY_ID
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: accesskey
                          optional: false
                    - name: AWS_SECRET_ACCESS_KEY
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: secretkey
                          optional: false
                    - name: AWS_S3_SIGNATURE_VERSION
                      value: "s3v4"
                    - name: TENSORBOARD_S3_ADDRESS
                      value: {tensorboard_s3_address}
                  resources:
                    limits:
                      {cpu_spec}
                      {memory_spec}
                      {gpu_spec}
              {init_container}
              volumes:
                - name: training
                  persistentVolumeClaim:
                    claimName: {pvc_name}
              restartPolicy: Never
        """
                    job_config = {
                        "group": "batch",
                        "version": "v1",
                        "plural": "jobs",
                        "label": "job-name",
                    }
                elif distribution_specification["distribution_type"] == "MPI":
                    job_spec = f"""apiVersion: kubeflow.org/v1
        kind: MPIJob
        metadata:
          name: {job_name}
          labels:
            train-model-job: {job_name}
          {namespace_spec}
        spec:
          slotsPerWorker: {slots_per_worker}
          runPolicy:
            cleanPodPolicy: Running
          mpiReplicaSpecs:
            Launcher:
              replicas: 1
              template:
                metadata:
                  annotations:
                    sidecar.istio.io/inject: "false"
                spec:
                  {init_container}
                  volumes:
                    - name: training
                      persistentVolumeClaim:
                        claimName: {pvc_name}
                  containers:
                  - image: {base_image}
                    name: mpi-launcher
                    command:
                    - mpirun
                    - -np
                    - "{number_of_processes}"
                    - --allow-run-as-root
                    - -bind-to
                    - none
                    - -map-by
                    - slot
                    - --prefix
                    - /opt/conda
                    - -mca
                    - pml
                    - ob1
                    - -mca
                    - btl
                    - ^openib
                    - -x
                    - NCCL_DEBUG=INFO
                    args: {train_command}
                    resources:
                      limits:
                        {launcher_cpu_spec}
                        {launcher_memory_spec}
            Worker:
              replicas: {number_of_workers}
              template:
                metadata:
                  annotations:
                    sidecar.istio.io/inject: "false"
                spec:
                  {node_selector}
                  containers:
                  - image: {base_image}
                    name: mpi-worker
                    env:
                    - name: S3_ENDPOINT
                      value: {minio_url}
                    - name: AWS_ACCESS_KEY_ID
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: accesskey
                          optional: false
                    - name: AWS_SECRET_ACCESS_KEY
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: secretkey
                          optional: false
                    - name: AWS_S3_SIGNATURE_VERSION
                      value: "s3v4"
                    - name: TENSORBOARD_S3_ADDRESS
                      value: {tensorboard_s3_address}
                    volumeMounts:
                      - mountPath: /train
                        name: training
                    resources:
                      limits:
                        {worker_cpu_spec}
                        {worker_memory_spec}
                        {gpu_spec}
                  volumes:
                    - name: training
                      persistentVolumeClaim:
                        claimName: {pvc_name}
        """
                    job_config = {
                        "group": "kubeflow.org",
                        "version": "v1",
                        "plural": "mpijobs",
                        "label": "training.kubeflow.org/replica-type=launcher,training.kubeflow.org/job-name",
                    }
                elif distribution_specification["distribution_type"] == "TF":
                    job_spec = f"""apiVersion: kubeflow.org/v1
        kind: TFJob
        metadata:
          name: {job_name}
          labels:
            train-model-job: {job_name}
          {namespace_spec}
        spec:
          runPolicy:
            cleanPodPolicy: None
          tfReplicaSpecs:
            Worker:
              replicas: {number_of_workers}
              restartPolicy: OnFailure
              template:
                metadata:
                  annotations:
                    sidecar.istio.io/inject: "false"
                spec:
                  {node_selector}
                  containers:
                    - name: tensorflow
                      image: {base_image}
                      command: {train_command}
                      env:
                      - name: S3_ENDPOINT
                        value: {minio_url}
                      - name: AWS_ACCESS_KEY_ID
                        valueFrom:
                          secretKeyRef:
                            name: {minio_secret}
                            key: accesskey
                            optional: false
                      - name: AWS_SECRET_ACCESS_KEY
                        valueFrom:
                          secretKeyRef:
                            name: {minio_secret}
                            key: secretkey
                            optional: false
                      - name: AWS_S3_SIGNATURE_VERSION
                        value: "s3v4"
                      - name: TENSORBOARD_S3_ADDRESS
                        value: {tensorboard_s3_address}
                      volumeMounts:
                        - mountPath: /train
                          name: training
                      resources:
                        limits:
                          {worker_cpu_spec}
                          {worker_memory_spec}
                          {gpu_spec}
                  volumes:
                    - name: training
                      persistentVolumeClaim:
                        claimName: {pvc_name}
        """
                    job_config = {
                        "group": "kubeflow.org",
                        "version": "v1",
                        "plural": "tfjobs",
                        "label": "tf-job-name",
                    }
                else:
                    err = f"Job failed while executing - unknown distribution_type: {distribution_specification['distribution_type']}"
                    raise Exception(err)

                job_config["job_spec"] = yaml.safe_load(job_spec)
                return job_config

            def create_fetch_job_configuration(
                job_name,
                namespace_spec,
                base_image,
                fetch_command,
                train_mount,
                minio_secret,
                pvc_name,
            ):
                job_spec = f"""apiVersion: batch/v1
        kind: Job
        metadata:
          name: {job_name}
          labels:
            train-model-job: {job_name}
          {namespace_spec}
        spec:
          template:
            metadata:
              annotations:
                sidecar.istio.io/inject: "false"
            spec:
              containers:
                - name: training-container
                  image: {base_image}
                  command: {fetch_command}
                  volumeMounts:
                    - mountPath: {train_mount}
                      name: training
                  restartPolicy: Never
                  env:
                  - name: MINIO_USER
                    valueFrom:
                      secretKeyRef:
                        name: {minio_secret}
                        key: accesskey
                        optional: false
                  - name: MINIO_PASS
                    valueFrom:
                      secretKeyRef:
                        name: {minio_secret}
                        key: secretkey
                        optional: false
              volumes:
                - name: training
                  persistentVolumeClaim:
                    claimName: {pvc_name}
              restartPolicy: Never
        """
                job_config = {
                    "group": "batch",
                    "version": "v1",
                    "plural": "jobs",
                    "job_spec": yaml.safe_load(job_spec),
                    "label": "job-name",
                }
                return job_config

            def submit_and_monitor_job(
                api_client, job_config, namespace, additional_job_resources=[]
            ):
                job_spec = job_config["job_spec"]
                job_resource = custom_object_api.create_namespaced_custom_object(
                    group=job_config["group"],
                    version=job_config["version"],
                    namespace=namespace,
                    plural=job_config["plural"],
                    body=job_spec,
                )
                job_name = job_resource["metadata"]["name"]
                job_uid = job_resource["metadata"]["uid"]

                logger.info("Creating additional job resource...")
                if additional_job_resources:
                    for resource in additional_job_resources:
                        resource["metadata"]["ownerReferences"] = [
                            {
                                "apiVersion": job_spec["apiVersion"],
                                "kind": job_spec["kind"],
                                "name": job_name,
                                "uid": job_uid,
                            }
                        ]
                    utils.create_from_yaml(api_client, yaml_objects=additional_job_resources)

                logger.info("Waiting for job to succeed...")
                job_is_monitored = False
                pods_being_monitored = set()
                job_watch = watch.Watch()
                for job_event in job_watch.stream(
                    custom_object_api.list_namespaced_custom_object,
                    group=job_config["group"],
                    version=job_config["version"],
                    plural=job_config["plural"],
                    namespace=namespace,
                    label_selector=f"train-model-job={job_name}",
                    timeout_seconds=0,
                ):
                    logger.info(f"job_event: {job_event}")
                    job = job_event["object"]
                    if "status" not in job and "items" in job:
                        job = job["items"][0]

                    if "status" not in job:
                        logger.info("Skipping event (no status information found)...")
                        continue

                    job_status = dict()
                    if "active" in job["status"]:
                        job_status["active"] = job["status"]["active"]
                    else:
                        job_status["active"] = 0
                    if "completionTime" in job["status"]:
                        job_status["completionTime"] = job["status"]["completionTime"]
                    if "failed" in job["status"]:
                        job_status["failed"] = job["status"]["failed"]
                    else:
                        job_status["failed"] = 0
                    if "ready" in job["status"]:
                        job_status["ready"] = job["status"]["ready"]
                    else:
                        job_status["ready"] = 0
                    if "startTime" in job["status"]:
                        job_status["startTime"] = job["status"]["startTime"]
                    if "succeeded" in job["status"]:
                        job_status["succeeded"] = job["status"]["succeeded"]
                    else:
                        job_status["succeeded"] = 0

                    # MPI
                    job_status["Complete"] = "False"
                    job_status["Created"] = "False"
                    job_status["Failed"] = "False"
                    job_status["Running"] = "False"
                    job_status["Succeeded"] = "False"
                    if "conditions" in job["status"]:
                        for condition in job["status"]["conditions"]:
                            job_status[condition["type"]] = condition["status"]

                    logger.info(f"Job status: {job_status}")

                    def start_monitoring(job_name, job_status):
                        return (not job_is_monitored) and (
                            job_status["active"] > 0
                            or job_status["Running"] == "True"
                            or job_status["failed"] > 0
                            or job_status["Failed"] == "True"
                            or job_status["ready"] > 0
                            or job_status["Complete"] == "True"
                            or job_status["Succeeded"] == "True"
                        )

                    if start_monitoring(job_name, job_status):
                        job_is_monitored = True
                        logger.info("Monitoring pods of job...")

                        # See https://stackoverflow.com/questions/65938572/kubernetes-python-client-equivalent-of-kubectl-wait-for-command
                        pod_watch = watch.Watch()
                        for pod_event in pod_watch.stream(
                            func=core_api.list_namespaced_pod,
                            namespace=namespace,
                            label_selector=f"{job_config['label']}={job_name}",
                            timeout_seconds=0,
                        ):
                            pod = pod_event["object"]
                            pod_name = pod.metadata.name

                            logger.info(
                                f"Pod {pod_name}: {pod_event['type']} - {pod.status.phase}"
                            )

                            if pod_name in pods_being_monitored:
                                pod_watch.stop()
                            elif pod_name not in pods_being_monitored and (
                                pod.status.phase == "Running"
                                or pod.status.phase == "Succeeded"
                                or pod.status.phase == "Failed"
                            ):
                                pods_being_monitored.add(pod_name)
                                logger.info(
                                    "=============================================================================="
                                )
                                logger.info(
                                    "=============================================================================="
                                )
                                logger.info(f"=== Streaming logs of pod {pod_name}...")
                                logger.info(
                                    "=============================================================================="
                                )
                                logger.info(
                                    "=============================================================================="
                                )

                                log_watch = watch.Watch()
                                for log_event in log_watch.stream(
                                    core_api.read_namespaced_pod_log,
                                    name=pod_name,
                                    namespace=namespace,
                                    follow=True,
                                    _return_http_data_only=True,
                                    _preload_content=False,
                                ):
                                    print(log_event)
                                logger.info(
                                    "=============================================================================="
                                )
                                logger.info(
                                    "=============================================================================="
                                )

                                pod_watch.stop()

                                if pod.status.phase == "Failed":
                                    err = "Job failed while executing."
                                    raise Exception(err)
                                break
                            if pod_event["type"] == "DELETED":
                                err = "Pod was deleted while we where waiting for it to start."
                                raise Exception(err)
                    elif (
                        job_status["succeeded"] > 0
                        or job_status["Complete"] == "True"
                        or job_status["Succeeded"] == "True"
                    ):
                        job_watch.stop()
                        logger.info("Job finished successfully.")
                        break
                    elif not (job_status["active"] > 0 or job_status["Running"] == "True") and (
                        job_status["failed"] > 0 or job_status["Failed"] == "True"
                    ):
                        job_watch.stop()
                        raise Exception("Job failed!")
                    else:
                        logger.info(f"Waiting for job updates. Current status: {job_status}")

            ###########################################################################
            # Main Workflow
            ###########################################################################

            logger.info("Establishing local cluster connection...")
            local_api_client = establish_local_cluster_connection()

            logger.info("Receiving training cluster configuration...")
            cluster_configuration = get_cluster_configuration(
                local_api_client, cluster_configuration_secret
            )

            logger.info("Establishing training cluster connection...")
            api_client, is_remote = establish_training_cluster_connection(
                local_api_client, cluster_configuration
            )
            batch_api = client.BatchV1Api(api_client)
            core_api = client.CoreV1Api(api_client)
            custom_object_api = client.CustomObjectsApi(api_client)

            logger.info("Initializing resources...")
            job_name = generate_unique_job_name(model_name)
            job_minio_secret = f"{job_name}-minio-secret"
            namespace, namespace_spec = initialize_namespace(
                cluster_configuration["remote-namespace"]
            )
            pvc_name = f"{job_name}-pvc"
            node_selector = initialize_nodeselector(node_selector)

            path_parameters = {
                "train_dataset_dir": train_dataset_dir,
                "validation_dataset_dir": validation_dataset_dir,
                "model_dir": model_dir,
            }
            train_command, inputs, outputs = initialize_command(
                train_specification, train_parameters, path_parameters, train_mount
            )

            init_container = initialize_init_container(
                base_image,
                cluster_configuration,
                inputs,
                is_remote,
                job_name,
                job_minio_secret,
                train_mount,
            )

            logger.info("=======================================")
            logger.info("Derived configurations")
            logger.info("=======================================")
            logger.info(f"job_name: {job_name}")
            logger.info(f"namespace: {namespace}")
            logger.info(f"is_remote: {is_remote}")
            logger.info(f"minio_url: {cluster_configuration['minio-url']}")
            logger.info(f"job_minio_secret: {job_minio_secret}")
            logger.info("inputs (input paths send to job):")
            for source, target in inputs:
                logger.info(
                    f"- {source} -> {cluster_configuration['minio-bucket']}/{cluster_configuration['minio-job-folder']}/{job_name}/{target}"
                )
            logger.info("outputs (output paths returning from job):")
            for source, target in outputs:
                logger.info(
                    f"- {target} <- {cluster_configuration['minio-bucket']}/{cluster_configuration['minio-job-folder']}/{job_name}/{source}"
                )
            logger.info(f"distribution_specification: {distribution_specification}")
            logger.info(f"train_command: {train_command}")
            logger.info("=======================================")

            additional_job_resources = []

            if is_remote:
                logger.info("Using MinIO to sync data with a new remote PVC for the job...")
                sync_with_minio(cluster_configuration, inputs, job_name, is_upload=True)
                additional_job_resources.append(
                    create_pvc_spec(
                        pvc_name, namespace_spec, cluster_configuration["access-mode"], pvc_size
                    )
                )
                additional_job_resources.append(
                    create_minio_secret_spec(
                        cluster_configuration, job_minio_secret, namespace_spec
                    )
                )
            else:
                logger.info(
                    f"Pushing inputs to local {train_mount} mount as shared with job environment..."
                )
                for (source, target) in inputs:
                    clone_path(source, target)

            logger.info("Creating train job configuration...")
            train_job_config = create_train_job_configuration(
                job_name,
                namespace_spec,
                node_selector,
                base_image,
                train_command,
                train_mount,
                cpus,
                memory,
                gpus,
                init_container,
                pvc_name,
                distribution_specification,
                cluster_configuration["minio-url"],
                job_minio_secret,
                tensorboard_s3_address,
            )

            logger.info(f"Starting train job '{namespace}.{job_name}'...")
            submit_and_monitor_job(
                api_client,
                train_job_config,
                namespace,
                additional_job_resources,
            )

            logger.info("Receiving training outputs...")
            if not os.path.exists(model_dir):
                os.makedirs(model_dir)
            if is_remote:
                fetch_command = initialize_fetch_command(
                    cluster_configuration, job_name, outputs
                )
                fetch_job_name = f"{job_name}-fetch"

                logger.info("Creating fetch job configuration...")
                fetch_job_config = create_fetch_job_configuration(
                    fetch_job_name,
                    namespace_spec,
                    base_image,
                    fetch_command,
                    train_mount,
                    job_minio_secret,
                    pvc_name,
                )

                logger.info(f"Starting fetch job '{namespace}.{fetch_job_name}'...")
                submit_and_monitor_job(api_client, fetch_job_config, namespace)

                logger.info("Fetching output data from MinIO & deleting it afterwards...")
                sync_with_minio(
                    cluster_configuration,
                    outputs,
                    job_name,
                    is_upload=False,
                    remove_minio_files=True,
                )

                logger.info(f"Deleting Job {fetch_job_name}...")
                batch_api.delete_namespaced_job(fetch_job_name, namespace)
            else:
                logger.info(
                    f"Fetching outputs to local {train_mount} mount as shared with job environment..."
                )
                for (source, target) in outputs:
                    clone_path(source, target)

            logger.info(f"Deleting Job {job_name}...")
            custom_object_api.delete_namespaced_custom_object(
                train_job_config["group"],
                train_job_config["version"],
                namespace,
                train_job_config["plural"],
                job_name,
            )

            logger.info("Finished.")

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Train Model Job', description='Trains a model. Once trained, the model is persisted to model_dir.')
        _parser.add_argument("--train-dataset-dir", dest="train_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--validation-dataset-dir", dest="validation_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-specification", dest="train_specification", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-parameters", dest="train_parameters", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-mount", dest="train_mount", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-name", dest="model_name", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--base-image", dest="base_image", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--node-selector", dest="node_selector", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--pvc-name", dest="pvc_name", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--pvc-size", dest="pvc_size", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--cpus", dest="cpus", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--gpus", dest="gpus", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--memory", dest="memory", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--tensorboard-s3-address", dest="tensorboard_s3_address", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--cluster-configuration-secret", dest="cluster_configuration_secret", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--distribution-specification", dest="distribution_specification", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-dir", dest="model_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = Train_Model_Job(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      parameters:
      - {name: cluster_configuration_secret}
      - {name: model_name}
      - {name: monitor-training-tensorboard_s3_address}
      - {name: training_gpus}
      - {name: training_node_selector}
      artifacts:
      - {name: preprocess-dataset-train_dataset_dir, path: /tmp/inputs/train_dataset_dir/data}
      - {name: preprocess-dataset-validation_dataset_dir, path: /tmp/inputs/validation_dataset_dir/data}
    outputs:
      artifacts:
      - {name: train-model-job-model_dir, path: /tmp/outputs/model_dir/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Trains
          a model. Once trained, the model is persisted to model_dir.", "implementation":
          {"container": {"args": ["--train-dataset-dir", {"inputPath": "train_dataset_dir"},
          "--validation-dataset-dir", {"inputPath": "validation_dataset_dir"}, "--train-specification",
          {"inputValue": "train_specification"}, "--train-parameters", {"inputValue":
          "train_parameters"}, {"if": {"cond": {"isPresent": "train_mount"}, "then":
          ["--train-mount", {"inputValue": "train_mount"}]}}, {"if": {"cond": {"isPresent":
          "model_name"}, "then": ["--model-name", {"inputValue": "model_name"}]}},
          {"if": {"cond": {"isPresent": "base_image"}, "then": ["--base-image", {"inputValue":
          "base_image"}]}}, {"if": {"cond": {"isPresent": "node_selector"}, "then":
          ["--node-selector", {"inputValue": "node_selector"}]}}, {"if": {"cond":
          {"isPresent": "pvc_name"}, "then": ["--pvc-name", {"inputValue": "pvc_name"}]}},
          {"if": {"cond": {"isPresent": "pvc_size"}, "then": ["--pvc-size", {"inputValue":
          "pvc_size"}]}}, {"if": {"cond": {"isPresent": "cpus"}, "then": ["--cpus",
          {"inputValue": "cpus"}]}}, {"if": {"cond": {"isPresent": "gpus"}, "then":
          ["--gpus", {"inputValue": "gpus"}]}}, {"if": {"cond": {"isPresent": "memory"},
          "then": ["--memory", {"inputValue": "memory"}]}}, {"if": {"cond": {"isPresent":
          "tensorboard_s3_address"}, "then": ["--tensorboard-s3-address", {"inputValue":
          "tensorboard_s3_address"}]}}, {"if": {"cond": {"isPresent": "cluster_configuration_secret"},
          "then": ["--cluster-configuration-secret", {"inputValue": "cluster_configuration_secret"}]}},
          {"if": {"cond": {"isPresent": "distribution_specification"}, "then": ["--distribution-specification",
          {"inputValue": "distribution_specification"}]}}, "--model-dir", {"outputPath":
          "model_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef Train_Model_Job(\n    train_dataset_dir,\n    validation_dataset_dir,\n    train_specification,\n    train_parameters,\n    model_dir,\n    train_mount
          = \"/train\",\n    model_name = \"my-model\",\n    base_image = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\",\n    node_selector
          = \"\",\n    pvc_name = \"\",\n    pvc_size = \"10Gi\",\n    cpus = \"\",\n    gpus
          = 0,\n    memory = \"\",\n    tensorboard_s3_address = \"\",\n    cluster_configuration_secret
          = \"\",\n    distribution_specification = None,\n):\n    \"\"\"\n    Trains
          a model. Once trained, the model is persisted to model_dir.\n\n            Parameters:\n                    train_dataset_dir:
          Path to the directory with training data.\n                    validation_dataset_dir:
          Path to the directory with validation data to be used during training.\n                    train_specification:
          Training command as generated from a Python function using kfp.components.func_to_component_text.\n                    train_parameters:
          Dictionary mapping formal to actual parameters for the training spacification.\n                    model_dir:
          Target path where the model will be stored.\n                    train_mount:
          Optional mounting point for training data of an existing PVC. Example: \"/train\".\n                    model_name:
          Optional name of the model. Must be unique for the targeted namespace and
          conform Kubernetes naming conventions. Example: my-model.\n                    base_image:
          Optional base image for model training. Example: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest.\n                    node_selector:
          Optional node selector for worker nodes. Example: nvidia.com/gpu.product:
          \"Tesla-V100-SXM2-32GB\".\n                    pvc_name: Optional name to
          an existing persistent volume claim (pvc). If given, this pvc is mounted
          into the training job. Example: \"music-genre-classification-j4ssf-training-pvc\".\n                    pvc_size:
          Optional size of the storage during model training. Storage is mounted into
          to the Job based on a persitent volume claim of the given size. Example:
          10Gi.\n                    cpus: Optional CPU limit for the job. Leave empty
          for cluster defaults (typically no limit). Example: \"1000m\".\n                    gpus:
          Optional number of GPUs for the job. Example: 2.\n                    memory:
          Optional memory limit for the job. Leave empty for cluster defaults (typically
          no limit). Example: \"1Gi\".\n                    tensorboard_s3_address:
          Optional s3 address where Tensorboard logs shall be stored. Example: \"s3://mlpipeline/tensorboard/my-train-job\".\n                    cluster_configuration_secret:
          Optional secret name configuring a (remote) Kubernetes cluster to run the
          job in and the backing MinIO object store. All secret''s data values are
          optional and appropriate defaults are chosen if not present. The secret
          may provide a suitable kubernetes bearer token, the associated namespace,
          a host, etc. Example: \"remote-power-cluster\".\n                    distribution_specification:
          Optional dictionary specifiying the distribution behavior. By default, no
          distributed training is executed, which results in an ordinary Kubernetes
          Job  for training. Otherwise, dictionary entries determine the distribution
          behavior. The \"distribution_type\" entry determines the distribution type:
          \"Job\" (no distribution; ordinary Kubernetes job), \"MPI\" (all-reduce
          style distribution via Horovod), or \"TF\" (parameter-server style distribution
          via distributed training with TensorFlow). Depending on the distribution
          type, additional dictionary entries can be processed. For distributed training
          jobs, the \"number_of_workers\" (e.g., 2) determines the number of worker
          replicas for training. Individual resource limits can be controlled via
          \"worker_cpus\" (e.g., \"1000m\") and \"worker_memory\" (e.g., \"1Gi\").
          MPI additionally provides a fine-grained control of launcher cpu and memory
          limits via \"launcher_cpus\" (e.g., \"1000m\") and \"launcher_memory\" (e.g.,
          \"1Gi\"). Full example with MPI: {\"distribution_type\": \"MPI\", \"number_of_workers\":
          2, \"worker_cpus\": \"8\", \"worker_memory\": \"32Gi\", \"launcher_cpus\":
          \"2\", \"launcher_memory\": \"8Gi\"}\n    \"\"\"\n    from datetime import
          datetime\n    import errno\n    import json\n    import kfp\n    from kubernetes
          import client, config, utils, watch\n    import logging\n    import os\n    import
          shutil\n    import sys\n    import yaml\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    ###########################################################################\n    #
          Helper Functions\n    ###########################################################################\n\n    def
          establish_local_cluster_connection():\n        config.load_incluster_config()\n        return
          client.ApiClient()\n\n    def get_cluster_configuration(api_client, cluster_configuration_secret):\n        import
          base64\n        from kubernetes.client.rest import ApiException\n\n        def
          decode(secret, key):\n            data = secret.data[key]\n            decoded_data
          = base64.b64decode(data)\n            return decoded_data.decode(\"utf-8\")\n\n        def
          update_with_secret(secret, dictionary):\n            for key in dictionary:\n                if
          key in secret.data:\n                    dictionary[key] = decode(secret,
          key)\n\n        cluster_configuration = {\n            \"access-mode\":
          \"ReadWriteMany\",\n            \"minio-accesskey\": \"minio\",\n            \"minio-bucket\":
          \"mlpipeline\",\n            \"minio-job-folder\": \"jobs\",\n            \"minio-secretkey\":
          \"minio123\",\n            \"minio-url\": \"http://minio-service.kubeflow:9000\",\n            \"remote-host\":
          \"\",\n            \"remote-namespace\": \"\",\n            \"remote-token\":
          \"\",\n        }\n\n        try:\n            default_minio_secret = client.CoreV1Api(api_client).read_namespaced_secret(\n                \"mlpipeline-minio-artifact\",
          get_current_namespace()\n            )\n\n            if default_minio_secret.data
          is None:\n                logger.info(\n                    \"MinIO secret
          (mlpipeline-minio-artifact) includes no data - progressing with default
          values.\"\n                )\n            else:\n                logger.info(\n                    \"Found
          default MinIO secret (mlpipeline-minio-artifact) - updating cluster configuration
          accordingly.\"\n                )\n                cluster_configuration[\"minio-accesskey\"]
          = decode(\n                    default_minio_secret, \"accesskey\"\n                )\n                cluster_configuration[\"minio-secretkey\"]
          = decode(\n                    default_minio_secret, \"secretkey\"\n                )\n        except
          ApiException as e:\n            if e.status == 404:\n                logger.info(\n                    \"Found
          no default MinIO secret (mlpipeline-minio-artifact) - progressing with default
          values.\"\n                )\n\n        if cluster_configuration_secret
          == \"\":\n            logger.info(\n                \"No cluster configuration
          secret specified - progressing with default values.\"\n            )\n            return
          cluster_configuration\n\n        try:\n            secret = client.CoreV1Api(api_client).read_namespaced_secret(\n                cluster_configuration_secret,
          get_current_namespace()\n            )\n            if secret.data is None:\n                logger.info(\n                    f\"Cluster
          configuration secret ({cluster_configuration_secret}) includes no data -
          progressing with default values.\"\n                )\n            else:\n                logger.info(\n                    f\"Found
          cluster configuration secret ({cluster_configuration_secret}) - updating
          cluster configuration accordingly.\"\n                )\n                update_with_secret(secret,
          cluster_configuration)\n        except ApiException as e:\n            if
          e.status == 404:\n                logger.info(\n                    f\"Found
          no cluster configuration secret ({cluster_configuration_secret}) - progressing
          with default values.\"\n                )\n\n        return cluster_configuration\n\n    def
          establish_training_cluster_connection(local_api_client, cluster_configuration):\n        is_remote
          = False\n        if (\n            cluster_configuration[\"remote-host\"]
          == \"\"\n            or cluster_configuration[\"remote-token\"] == \"\"\n        ):\n            logger.info(\n                \"Remote
          cluster not configured. Using in-cluster configuration...\"\n            )\n            logger.info(\n                \"Note:
          assign the name of a secret to the ''cluster_configuration_secret'' pipeline
          argument and add the secret to your cluster.\"\n            )\n            logger.info(\"Example
          secret:\")\n            logger.info(\"---\")\n            logger.info(\"apiVersion:
          v1\")\n            logger.info(\"kind: Secret\")\n            logger.info(\"metadata:\")\n            logger.info(\"  name:
          my-remote-cluster\")\n            logger.info(\"stringData:\")\n            logger.info(\"  access-mode:
          ReadWriteOnce\")\n            logger.info(\"  minio-accesskey: minio\")\n            logger.info(\"  minio-bucket:
          mlpipeline\")\n            logger.info(\"  minio-job-folder: jobs\")\n            logger.info(\"  minio-secretkey:
          minio123\")\n            logger.info(\"  minio-url: http://minio-service.kubeflow:9000\")\n            logger.info(\n                \"  remote-host:
          https://istio-ingressgateway-istio-system.apps.mydomain.ai:6443\"\n            )\n            logger.info(\"  remote-namespace:
          default\")\n            logger.info(\"  remote-token: eyJh...\")\n            logger.info(\"---\")\n            logger.info(\n                \"Where
          you get the remote-token from your remote cluster as described here:\"\n            )\n            logger.info(\n                \"https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#without-kubectl-proxy\"\n            )\n\n            api_client
          = local_api_client\n            if not os.path.exists(train_mount):\n                logger.warning(\n                    f\"No
          local mount to {train_mount} found. Therefore, switching to remote data
          synchronization mode via MinIO. This will work but is slower compared to
          local mounts. Consider adding a mount to ''{train_mount}'' for this component
          by using a PVC inside your pipeline.\"\n                )\n                is_remote
          = True\n        else:\n            # see: https://github.com/kubernetes-client/python/blob/6d4587e18064288d031ed9bbf5ab5b8245460b3c/examples/remote_cluster.py\n            logger.info(\n                \"Remote
          host and token found. Using remote cluster configuration...\"\n            )\n            configuration
          = client.Configuration()\n            configuration.host = cluster_configuration[\"remote-host\"]\n            configuration.verify_ssl
          = False\n            configuration.api_key = {\n                \"authorization\":
          \"Bearer \" + cluster_configuration[\"remote-token\"]\n            }\n            api_client
          = client.ApiClient(configuration)\n            is_remote = True\n\n        return
          (api_client, is_remote)\n\n    def clone_path(source, target):\n        try:\n            logger.info(f\"Cloning
          source path {source} to {target} of training job...\")\n            shutil.copytree(source,
          target)\n            logger.info(\"Cloning finished. Target path contents:\")\n            logger.info(os.listdir(target))\n        except
          OSError as e:\n            if e.errno in (errno.ENOTDIR, errno.EINVAL):\n                shutil.copy(source,
          target)\n            else:\n                raise\n\n    def sync_with_minio(\n        cluster_configuration,\n        inputs,\n        job_name,\n        is_upload,\n        remove_minio_files
          = False,\n    ):\n        import boto3\n        import botocore\n        from
          botocore.client import Config\n        import json\n        import logging\n        import
          os\n        import sys\n        import tarfile\n\n        logging.basicConfig(\n            stream=sys.stdout,\n            level=logging.INFO,\n            format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n        )\n        logger = logging.getLogger()\n\n        def
          establish_minio_connection(cluster_configuration):\n            if (\"minio-accesskey\"
          in cluster_configuration) and (\n                \"minio-secretkey\" in
          cluster_configuration\n            ):\n                minio_user = cluster_configuration[\"minio-accesskey\"]\n                minio_pass
          = cluster_configuration[\"minio-secretkey\"]\n            else:\n                minio_user
          = os.getenv(\"MINIO_USER\")\n                minio_pass = os.getenv(\"MINIO_PASS\")\n\n            if
          minio_user == \"\" or minio_pass == \"\":\n                err = \"Environment
          variables MINIO_USER and MINIO_PASS need externally to be provided to this
          component using k8s_secret_key_to_env!\"\n                raise Exception(err)\n\n            return
          boto3.session.Session().resource(\n                service_name=\"s3\",\n                endpoint_url=cluster_configuration[\"minio-url\"],\n                aws_access_key_id=minio_user,\n                aws_secret_access_key=minio_pass,\n                config=Config(signature_version=\"s3v4\"),\n            )\n\n        def
          path_to_tarfilename(pathname):\n            return f\"{pathname.replace(os.sep,
          ''-'')}.tar.gz\"\n\n        def make_tarfile(output_filename, source_dir):\n            with
          tarfile.open(output_filename, \"w:gz\") as tar:\n                tar.add(source_dir,
          arcname=\".\")\n\n        # see: https://stackoverflow.com/a/47565719/2625096\n        def
          bucket_exists(minio_client, bucket):\n            try:\n                minio_client.meta.client.head_bucket(Bucket=bucket.name)\n                return
          True\n            except botocore.exceptions.ClientError as e:\n                error_code
          = int(e.response[\"Error\"][\"Code\"])\n                if error_code ==
          403:\n                    # Forbidden Access -> Private Bucket\n                    return
          True\n                elif error_code == 404:\n                    return
          False\n\n        def upload_to_minio(file, upload_bucket, job_folder, job_name,
          minio_client):\n            bucket = minio_client.Bucket(upload_bucket)\n\n            if
          not bucket_exists(minio_client, bucket):\n                minio_client.create_bucket(Bucket=bucket.name)\n\n            bucket.upload_file(file,
          f\"{job_folder}/{job_name}/{file}\")\n\n        def download_from_minio(\n            file,
          upload_bucket, job_folder, job_name, minio_client, remove_minio_file\n        ):\n            bucket
          = minio_client.Bucket(upload_bucket)\n            key = f\"{job_folder}/{job_name}/{file}\"\n\n            bucket.download_file(key,
          file)\n\n            if remove_minio_file:\n                bucket.Object(key).delete()\n\n        def
          extract_tarfile(tarfile_name, target):\n            with tarfile.open(tarfile_name,
          \"r:gz\") as tar_gz_ref:\n                tar_gz_ref.extractall(target)\n\n        if
          isinstance(cluster_configuration, str):\n            cluster_configuration
          = json.loads(cluster_configuration)\n\n        if isinstance(inputs, str):\n            inputs
          = json.loads(inputs)\n\n        if isinstance(is_upload, str):\n            if
          is_upload == \"True\":\n                is_upload = True\n            else:\n                is_upload
          = False\n\n        logger.info(\"Establishing MinIO connection...\")\n        minio_client
          = establish_minio_connection(cluster_configuration)\n\n        for (source,
          target) in inputs:\n            tarfilename = path_to_tarfilename(source)\n\n            if
          is_upload:\n                logger.info(f\"Tar.gz input {source} into {tarfilename}...\")\n                make_tarfile(tarfilename,
          source)\n\n                logger.info(\n                    f''Uploading
          {tarfilename} to {cluster_configuration[\"minio-bucket\"]}/{cluster_configuration[\"minio-job-folder\"]}/{job_name}/{tarfilename}...''\n                )\n                upload_to_minio(\n                    tarfilename,\n                    cluster_configuration[\"minio-bucket\"],\n                    cluster_configuration[\"minio-job-folder\"],\n                    job_name,\n                    minio_client,\n                )\n            else:\n                logger.info(\n                    f''Downloading
          {cluster_configuration[\"minio-bucket\"]}/{cluster_configuration[\"minio-job-folder\"]}/{job_name}/{tarfilename}
          to {tarfilename}...''\n                )\n                download_from_minio(\n                    tarfilename,\n                    cluster_configuration[\"minio-bucket\"],\n                    cluster_configuration[\"minio-job-folder\"],\n                    job_name,\n                    minio_client,\n                    remove_minio_files,\n                )\n\n                logger.info(f\"Extracting
          {tarfilename} to {target}...\")\n                extract_tarfile(tarfilename,
          target)\n\n                logger.info(\"Result:\")\n                logger.info(os.listdir(target))\n\n    def
          generate_unique_job_name(model_name):\n        epoch = datetime.today().strftime(\"%Y%m%d%H%M%S\")\n        return
          f\"job-{model_name}-{epoch}\"\n\n    def get_current_namespace():\n        SA_NAMESPACE
          = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n        with
          open(SA_NAMESPACE) as f:\n            return f.read()\n\n    def initialize_namespace(namespace):\n        if
          namespace == \"\":\n            namespace = get_current_namespace()\n        namespace_spec
          = f\"namespace: {namespace}\"\n\n        return (namespace, namespace_spec)\n\n    def
          initialize_nodeselector(node_selector):\n        if node_selector != \"\":\n            node_selector
          = f\"nodeSelector:\\n        {node_selector}\"\n        return node_selector\n\n    def
          initialize_init_container(\n        base_image,\n        cluster_configuration,\n        inputs,\n        is_remote,\n        job_name,\n        minio_secret,\n        mount_path,\n    ):\n        if
          not is_remote:\n            return \"\"\n\n        command_specification
          = kfp.components.func_to_component_text(\n            func=sync_with_minio\n        )\n\n        #
          inner components loose type information as needed by lists/dicts\n        #
          -> cluster_configuration & inputs need to be a string (using json)\n        cluster_configuration_json
          = json.dumps(\n            {\n                \"minio-bucket\": cluster_configuration[\"minio-bucket\"],\n                \"minio-job-folder\":
          cluster_configuration[\"minio-job-folder\"],\n                \"minio-url\":
          cluster_configuration[\"minio-url\"],\n            }\n        )\n        inputs_json
          = json.dumps(inputs)\n        parameters = {\n            \"cluster_configuration\":
          cluster_configuration_json,\n            \"inputs\": inputs_json,\n            \"job_name\":
          job_name,\n            \"is_upload\": \"False\",\n        }\n\n        command,
          _, _ = initialize_command(command_specification, parameters)\n\n        init_container
          = f\"\"\"initContainers:\n          - name: init-inputs\n            image:
          {base_image}\n            command: {command}\n            volumeMounts:\n            -
          mountPath: {mount_path}\n              name: training\n            env:\n            -
          name: MINIO_USER\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: accesskey\n                  optional:
          false\n            - name: MINIO_PASS\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: secretkey\n                  optional:
          false\n\"\"\"\n        return init_container\n\n    def initialize_command(\n        specification,\n        parameters,\n        path_parameters
          = {},\n        mount_path = \"/tmp\",\n    ):\n        component_yaml =
          yaml.safe_load(specification)\n        container_yaml = component_yaml[\"implementation\"][\"container\"]\n        command
          = container_yaml[\"command\"]\n        args = container_yaml[\"args\"]\n\n        actual_args
          = list()\n        inputs = list()\n        outputs = list()\n        for
          idx, arg in enumerate(args):\n            if type(arg) is dict:\n                if
          \"inputValue\" in arg:\n                    # required parameter (value)\n                    key
          = arg[\"inputValue\"]\n                    if key in parameters:\n                        actual_args.append(parameters[key])\n                    else:\n                        err
          = f\"Required parameter ''{key}'' missing in component input!\"\n                        raise
          Exception(err)\n                elif \"if\" in arg:\n                    #
          optional parameter\n                    key = arg[\"if\"][\"cond\"][\"isPresent\"]\n                    if
          key in parameters:\n                        actual_args.append(f\"--{key}\")\n                        actual_args.append(parameters[key])\n                elif
          \"inputPath\" in arg:\n                    # required InputPath\n                    key
          = arg[\"inputPath\"]\n                    if key in parameters:\n                        path_key
          = parameters[key]\n                        if path_key in path_parameters:\n                            mount
          = f\"{mount_path}{path_parameters[path_key]}\"\n                            inputs.append((path_parameters[path_key],
          mount))\n                            actual_args.append(mount)\n                        else:\n                            err
          = f\"InputPath ''{path_key}'' unavailable in training component!\"\n                            raise
          Exception(err)\n                    else:\n                        err =
          f\"Required parameter ''{key}'' missing in component input!\"\n                        raise
          Exception(err)\n                elif \"outputPath\" in arg:\n                    #
          required OutputPath\n                    key = arg[\"outputPath\"]\n                    if
          key in parameters:\n                        path_key = parameters[key]\n                        if
          path_key in path_parameters:\n                            mount = f\"{mount_path}{path_parameters[path_key]}\"\n                            outputs.append((mount,
          path_parameters[path_key]))\n                            actual_args.append(mount)\n                        else:\n                            err
          = f\"OutputPath ''{path_key}'' unavailable in training component!\"\n                            raise
          Exception(err)\n                    else:\n                        err =
          f\"Required parameter ''{key}'' missing in component input!\"\n                        raise
          Exception(err)\n            else:\n                # required parameter
          (key)\n                actual_args.append(arg)\n\n        command_with_initialized_args
          = json.dumps(command + actual_args)\n\n        return command_with_initialized_args,
          inputs, outputs\n\n    def initialize_fetch_command(\n        cluster_configuration,\n        job_name,\n        outputs,\n    ):\n        command_specification
          = kfp.components.func_to_component_text(\n            func=sync_with_minio\n        )\n\n        #
          inner components loose type information as needed by lists/dicts\n        #
          -> cluster_configuration & inputs need to be a string (using json)\n        cluster_configuration_json
          = json.dumps(\n            {\n                \"minio-bucket\": cluster_configuration[\"minio-bucket\"],\n                \"minio-job-folder\":
          cluster_configuration[\"minio-job-folder\"],\n                \"minio-url\":
          cluster_configuration[\"minio-url\"],\n            }\n        )\n        outputs_json
          = json.dumps(outputs)\n        parameters = {\n            \"cluster_configuration\":
          cluster_configuration_json,\n            \"inputs\": outputs_json,\n            \"job_name\":
          job_name,\n            \"is_upload\": \"True\",\n        }\n        command,
          _, _ = initialize_command(command_specification, parameters)\n        return
          command\n\n    def create_pvc_spec(pvc_name, namespace_spec, access_mode,
          pvc_size):\n        pvc_spec = f\"\"\"apiVersion: batch/v1\napiVersion:
          v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: {pvc_name}\n  {namespace_spec}\nspec:\n  accessModes:\n  -
          {access_mode}\n  resources:\n    requests:\n      storage: {pvc_size}\n\"\"\"\n        return
          yaml.safe_load(pvc_spec)\n\n    def create_minio_secret_spec(cluster_configuration,
          minio_secret, namespace_spec):\n        minio_secret_spec = f\"\"\"apiVersion:
          v1\nkind: Secret\nmetadata:\n  name: {minio_secret}\n  {namespace_spec}\nstringData:\n  accesskey:
          {cluster_configuration[\"minio-accesskey\"]}\n  secretkey: {cluster_configuration[\"minio-secretkey\"]}\n\"\"\"\n        return
          yaml.safe_load(minio_secret_spec)\n\n    def create_train_job_configuration(\n        job_name,\n        namespace_spec,\n        node_selector,\n        base_image,\n        train_command,\n        train_mount,\n        cpus,\n        memory,\n        gpus,\n        init_container,\n        pvc_name,\n        distribution_specification,\n        minio_url,\n        minio_secret,\n        tensorboard_s3_address,\n    ):\n        if
          cpus:\n            cpu_spec = f\"cpu: {cpus}\"\n        else:\n            cpu_spec
          = \"\"\n\n        if memory:\n            memory_spec = f\"memory: {memory}\"\n        else:\n            memory_spec
          = \"\"\n\n        if gpus:\n            gpu_spec = f\"nvidia.com/gpu: {gpus}\"\n        else:\n            gpu_spec
          = \"\"\n\n        if distribution_specification is None:\n            distribution_specification
          = dict()\n\n        if \"distribution_type\" not in distribution_specification:\n            distribution_specification[\"distribution_type\"]
          = \"Job\"\n\n        if gpus < 1:\n            slots_per_worker = 1\n        else:\n            slots_per_worker
          = gpus\n\n        if \"number_of_workers\" in distribution_specification:\n            number_of_workers
          = distribution_specification[\"number_of_workers\"]\n        else:\n            number_of_workers
          = 2\n\n        number_of_processes = number_of_workers * slots_per_worker\n\n        if
          \"launcher_cpus\" in distribution_specification:\n            launcher_cpu_spec
          = f\"cpu: {distribution_specification[''launcher_cpus'']}\"\n        else:\n            launcher_cpu_spec
          = \"\"\n\n        if \"launcher_memory\" in distribution_specification:\n            launcher_memory_spec
          = (\n                f\"memory: {distribution_specification[''launcher_memory'']}\"\n            )\n        else:\n            launcher_memory_spec
          = \"\"\n\n        if \"worker_cpus\" in distribution_specification:\n            worker_cpu_spec
          = f\"cpu: {distribution_specification[''worker_cpus'']}\"\n        else:\n            worker_cpu_spec
          = \"\"\n\n        if \"worker_memory\" in distribution_specification:\n            worker_memory_spec
          = (\n                f\"memory: {distribution_specification[''worker_memory'']}\"\n            )\n        else:\n            worker_memory_spec
          = \"\"\n\n        if distribution_specification[\"distribution_type\"] ==
          \"Job\":\n            job_spec = f\"\"\"apiVersion: batch/v1\nkind: Job\nmetadata:\n  name:
          {job_name}\n  labels:\n    train-model-job: {job_name}\n  {namespace_spec}\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject:
          \"false\"\n    spec:\n      {node_selector}\n      containers:\n        -
          name: training-container\n          image: {base_image}\n          command:
          {train_command}\n          volumeMounts:\n            - mountPath: {train_mount}\n              name:
          training\n          restartPolicy: Never\n          env:\n            -
          name: S3_ENDPOINT\n              value: {minio_url}\n            - name:
          AWS_ACCESS_KEY_ID\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: accesskey\n                  optional:
          false\n            - name: AWS_SECRET_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: secretkey\n                  optional:
          false\n            - name: AWS_S3_SIGNATURE_VERSION\n              value:
          \"s3v4\"\n            - name: TENSORBOARD_S3_ADDRESS\n              value:
          {tensorboard_s3_address}\n          resources:\n            limits:\n              {cpu_spec}\n              {memory_spec}\n              {gpu_spec}\n      {init_container}\n      volumes:\n        -
          name: training\n          persistentVolumeClaim:\n            claimName:
          {pvc_name}\n      restartPolicy: Never\n\"\"\"\n            job_config =
          {\n                \"group\": \"batch\",\n                \"version\": \"v1\",\n                \"plural\":
          \"jobs\",\n                \"label\": \"job-name\",\n            }\n        elif
          distribution_specification[\"distribution_type\"] == \"MPI\":\n            job_spec
          = f\"\"\"apiVersion: kubeflow.org/v1\nkind: MPIJob\nmetadata:\n  name: {job_name}\n  labels:\n    train-model-job:
          {job_name}\n  {namespace_spec}\nspec:\n  slotsPerWorker: {slots_per_worker}\n  runPolicy:\n    cleanPodPolicy:
          Running\n  mpiReplicaSpecs:\n    Launcher:\n      replicas: 1\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject:
          \"false\"\n        spec:\n          {init_container}\n          volumes:\n            -
          name: training\n              persistentVolumeClaim:\n                claimName:
          {pvc_name}\n          containers:\n          - image: {base_image}\n            name:
          mpi-launcher\n            command:\n            - mpirun\n            -
          -np\n            - \"{number_of_processes}\"\n            - --allow-run-as-root\n            -
          -bind-to\n            - none\n            - -map-by\n            - slot\n            -
          --prefix\n            - /opt/conda\n            - -mca\n            - pml\n            -
          ob1\n            - -mca\n            - btl\n            - ^openib\n            -
          -x\n            - NCCL_DEBUG=INFO\n            args: {train_command}\n            resources:\n              limits:\n                {launcher_cpu_spec}\n                {launcher_memory_spec}\n    Worker:\n      replicas:
          {number_of_workers}\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject:
          \"false\"\n        spec:\n          {node_selector}\n          containers:\n          -
          image: {base_image}\n            name: mpi-worker\n            env:\n            -
          name: S3_ENDPOINT\n              value: {minio_url}\n            - name:
          AWS_ACCESS_KEY_ID\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: accesskey\n                  optional:
          false\n            - name: AWS_SECRET_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: secretkey\n                  optional:
          false\n            - name: AWS_S3_SIGNATURE_VERSION\n              value:
          \"s3v4\"\n            - name: TENSORBOARD_S3_ADDRESS\n              value:
          {tensorboard_s3_address}\n            volumeMounts:\n              - mountPath:
          /train\n                name: training\n            resources:\n              limits:\n                {worker_cpu_spec}\n                {worker_memory_spec}\n                {gpu_spec}\n          volumes:\n            -
          name: training\n              persistentVolumeClaim:\n                claimName:
          {pvc_name}\n\"\"\"\n            job_config = {\n                \"group\":
          \"kubeflow.org\",\n                \"version\": \"v1\",\n                \"plural\":
          \"mpijobs\",\n                \"label\": \"training.kubeflow.org/replica-type=launcher,training.kubeflow.org/job-name\",\n            }\n        elif
          distribution_specification[\"distribution_type\"] == \"TF\":\n            job_spec
          = f\"\"\"apiVersion: kubeflow.org/v1\nkind: TFJob\nmetadata:\n  name: {job_name}\n  labels:\n    train-model-job:
          {job_name}\n  {namespace_spec}\nspec:\n  runPolicy:\n    cleanPodPolicy:
          None\n  tfReplicaSpecs:\n    Worker:\n      replicas: {number_of_workers}\n      restartPolicy:
          OnFailure\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject:
          \"false\"\n        spec:\n          {node_selector}\n          containers:\n            -
          name: tensorflow\n              image: {base_image}\n              command:
          {train_command}\n              env:\n              - name: S3_ENDPOINT\n                value:
          {minio_url}\n              - name: AWS_ACCESS_KEY_ID\n                valueFrom:\n                  secretKeyRef:\n                    name:
          {minio_secret}\n                    key: accesskey\n                    optional:
          false\n              - name: AWS_SECRET_ACCESS_KEY\n                valueFrom:\n                  secretKeyRef:\n                    name:
          {minio_secret}\n                    key: secretkey\n                    optional:
          false\n              - name: AWS_S3_SIGNATURE_VERSION\n                value:
          \"s3v4\"\n              - name: TENSORBOARD_S3_ADDRESS\n                value:
          {tensorboard_s3_address}\n              volumeMounts:\n                -
          mountPath: /train\n                  name: training\n              resources:\n                limits:\n                  {worker_cpu_spec}\n                  {worker_memory_spec}\n                  {gpu_spec}\n          volumes:\n            -
          name: training\n              persistentVolumeClaim:\n                claimName:
          {pvc_name}\n\"\"\"\n            job_config = {\n                \"group\":
          \"kubeflow.org\",\n                \"version\": \"v1\",\n                \"plural\":
          \"tfjobs\",\n                \"label\": \"tf-job-name\",\n            }\n        else:\n            err
          = f\"Job failed while executing - unknown distribution_type: {distribution_specification[''distribution_type'']}\"\n            raise
          Exception(err)\n\n        job_config[\"job_spec\"] = yaml.safe_load(job_spec)\n        return
          job_config\n\n    def create_fetch_job_configuration(\n        job_name,\n        namespace_spec,\n        base_image,\n        fetch_command,\n        train_mount,\n        minio_secret,\n        pvc_name,\n    ):\n        job_spec
          = f\"\"\"apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {job_name}\n  labels:\n    train-model-job:
          {job_name}\n  {namespace_spec}\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject:
          \"false\"\n    spec:\n      containers:\n        - name: training-container\n          image:
          {base_image}\n          command: {fetch_command}\n          volumeMounts:\n            -
          mountPath: {train_mount}\n              name: training\n          restartPolicy:
          Never\n          env:\n          - name: MINIO_USER\n            valueFrom:\n              secretKeyRef:\n                name:
          {minio_secret}\n                key: accesskey\n                optional:
          false\n          - name: MINIO_PASS\n            valueFrom:\n              secretKeyRef:\n                name:
          {minio_secret}\n                key: secretkey\n                optional:
          false\n      volumes:\n        - name: training\n          persistentVolumeClaim:\n            claimName:
          {pvc_name}\n      restartPolicy: Never\n\"\"\"\n        job_config = {\n            \"group\":
          \"batch\",\n            \"version\": \"v1\",\n            \"plural\": \"jobs\",\n            \"job_spec\":
          yaml.safe_load(job_spec),\n            \"label\": \"job-name\",\n        }\n        return
          job_config\n\n    def submit_and_monitor_job(\n        api_client, job_config,
          namespace, additional_job_resources=[]\n    ):\n        job_spec = job_config[\"job_spec\"]\n        job_resource
          = custom_object_api.create_namespaced_custom_object(\n            group=job_config[\"group\"],\n            version=job_config[\"version\"],\n            namespace=namespace,\n            plural=job_config[\"plural\"],\n            body=job_spec,\n        )\n        job_name
          = job_resource[\"metadata\"][\"name\"]\n        job_uid = job_resource[\"metadata\"][\"uid\"]\n\n        logger.info(\"Creating
          additional job resource...\")\n        if additional_job_resources:\n            for
          resource in additional_job_resources:\n                resource[\"metadata\"][\"ownerReferences\"]
          = [\n                    {\n                        \"apiVersion\": job_spec[\"apiVersion\"],\n                        \"kind\":
          job_spec[\"kind\"],\n                        \"name\": job_name,\n                        \"uid\":
          job_uid,\n                    }\n                ]\n            utils.create_from_yaml(api_client,
          yaml_objects=additional_job_resources)\n\n        logger.info(\"Waiting
          for job to succeed...\")\n        job_is_monitored = False\n        pods_being_monitored
          = set()\n        job_watch = watch.Watch()\n        for job_event in job_watch.stream(\n            custom_object_api.list_namespaced_custom_object,\n            group=job_config[\"group\"],\n            version=job_config[\"version\"],\n            plural=job_config[\"plural\"],\n            namespace=namespace,\n            label_selector=f\"train-model-job={job_name}\",\n            timeout_seconds=0,\n        ):\n            logger.info(f\"job_event:
          {job_event}\")\n            job = job_event[\"object\"]\n            if
          \"status\" not in job and \"items\" in job:\n                job = job[\"items\"][0]\n\n            if
          \"status\" not in job:\n                logger.info(\"Skipping event (no
          status information found)...\")\n                continue\n\n            job_status
          = dict()\n            if \"active\" in job[\"status\"]:\n                job_status[\"active\"]
          = job[\"status\"][\"active\"]\n            else:\n                job_status[\"active\"]
          = 0\n            if \"completionTime\" in job[\"status\"]:\n                job_status[\"completionTime\"]
          = job[\"status\"][\"completionTime\"]\n            if \"failed\" in job[\"status\"]:\n                job_status[\"failed\"]
          = job[\"status\"][\"failed\"]\n            else:\n                job_status[\"failed\"]
          = 0\n            if \"ready\" in job[\"status\"]:\n                job_status[\"ready\"]
          = job[\"status\"][\"ready\"]\n            else:\n                job_status[\"ready\"]
          = 0\n            if \"startTime\" in job[\"status\"]:\n                job_status[\"startTime\"]
          = job[\"status\"][\"startTime\"]\n            if \"succeeded\" in job[\"status\"]:\n                job_status[\"succeeded\"]
          = job[\"status\"][\"succeeded\"]\n            else:\n                job_status[\"succeeded\"]
          = 0\n\n            # MPI\n            job_status[\"Complete\"] = \"False\"\n            job_status[\"Created\"]
          = \"False\"\n            job_status[\"Failed\"] = \"False\"\n            job_status[\"Running\"]
          = \"False\"\n            job_status[\"Succeeded\"] = \"False\"\n            if
          \"conditions\" in job[\"status\"]:\n                for condition in job[\"status\"][\"conditions\"]:\n                    job_status[condition[\"type\"]]
          = condition[\"status\"]\n\n            logger.info(f\"Job status: {job_status}\")\n\n            def
          start_monitoring(job_name, job_status):\n                return (not job_is_monitored)
          and (\n                    job_status[\"active\"] > 0\n                    or
          job_status[\"Running\"] == \"True\"\n                    or job_status[\"failed\"]
          > 0\n                    or job_status[\"Failed\"] == \"True\"\n                    or
          job_status[\"ready\"] > 0\n                    or job_status[\"Complete\"]
          == \"True\"\n                    or job_status[\"Succeeded\"] == \"True\"\n                )\n\n            if
          start_monitoring(job_name, job_status):\n                job_is_monitored
          = True\n                logger.info(\"Monitoring pods of job...\")\n\n                #
          See https://stackoverflow.com/questions/65938572/kubernetes-python-client-equivalent-of-kubectl-wait-for-command\n                pod_watch
          = watch.Watch()\n                for pod_event in pod_watch.stream(\n                    func=core_api.list_namespaced_pod,\n                    namespace=namespace,\n                    label_selector=f\"{job_config[''label'']}={job_name}\",\n                    timeout_seconds=0,\n                ):\n                    pod
          = pod_event[\"object\"]\n                    pod_name = pod.metadata.name\n\n                    logger.info(\n                        f\"Pod
          {pod_name}: {pod_event[''type'']} - {pod.status.phase}\"\n                    )\n\n                    if
          pod_name in pods_being_monitored:\n                        pod_watch.stop()\n                    elif
          pod_name not in pods_being_monitored and (\n                        pod.status.phase
          == \"Running\"\n                        or pod.status.phase == \"Succeeded\"\n                        or
          pod.status.phase == \"Failed\"\n                    ):\n                        pods_being_monitored.add(pod_name)\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n                        logger.info(f\"===
          Streaming logs of pod {pod_name}...\")\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n\n                        log_watch
          = watch.Watch()\n                        for log_event in log_watch.stream(\n                            core_api.read_namespaced_pod_log,\n                            name=pod_name,\n                            namespace=namespace,\n                            follow=True,\n                            _return_http_data_only=True,\n                            _preload_content=False,\n                        ):\n                            print(log_event)\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n\n                        pod_watch.stop()\n\n                        if
          pod.status.phase == \"Failed\":\n                            err = \"Job
          failed while executing.\"\n                            raise Exception(err)\n                        break\n                    if
          pod_event[\"type\"] == \"DELETED\":\n                        err = \"Pod
          was deleted while we where waiting for it to start.\"\n                        raise
          Exception(err)\n            elif (\n                job_status[\"succeeded\"]
          > 0\n                or job_status[\"Complete\"] == \"True\"\n                or
          job_status[\"Succeeded\"] == \"True\"\n            ):\n                job_watch.stop()\n                logger.info(\"Job
          finished successfully.\")\n                break\n            elif not (job_status[\"active\"]
          > 0 or job_status[\"Running\"] == \"True\") and (\n                job_status[\"failed\"]
          > 0 or job_status[\"Failed\"] == \"True\"\n            ):\n                job_watch.stop()\n                raise
          Exception(\"Job failed!\")\n            else:\n                logger.info(f\"Waiting
          for job updates. Current status: {job_status}\")\n\n    ###########################################################################\n    #
          Main Workflow\n    ###########################################################################\n\n    logger.info(\"Establishing
          local cluster connection...\")\n    local_api_client = establish_local_cluster_connection()\n\n    logger.info(\"Receiving
          training cluster configuration...\")\n    cluster_configuration = get_cluster_configuration(\n        local_api_client,
          cluster_configuration_secret\n    )\n\n    logger.info(\"Establishing training
          cluster connection...\")\n    api_client, is_remote = establish_training_cluster_connection(\n        local_api_client,
          cluster_configuration\n    )\n    batch_api = client.BatchV1Api(api_client)\n    core_api
          = client.CoreV1Api(api_client)\n    custom_object_api = client.CustomObjectsApi(api_client)\n\n    logger.info(\"Initializing
          resources...\")\n    job_name = generate_unique_job_name(model_name)\n    job_minio_secret
          = f\"{job_name}-minio-secret\"\n    namespace, namespace_spec = initialize_namespace(\n        cluster_configuration[\"remote-namespace\"]\n    )\n    pvc_name
          = f\"{job_name}-pvc\"\n    node_selector = initialize_nodeselector(node_selector)\n\n    path_parameters
          = {\n        \"train_dataset_dir\": train_dataset_dir,\n        \"validation_dataset_dir\":
          validation_dataset_dir,\n        \"model_dir\": model_dir,\n    }\n    train_command,
          inputs, outputs = initialize_command(\n        train_specification, train_parameters,
          path_parameters, train_mount\n    )\n\n    init_container = initialize_init_container(\n        base_image,\n        cluster_configuration,\n        inputs,\n        is_remote,\n        job_name,\n        job_minio_secret,\n        train_mount,\n    )\n\n    logger.info(\"=======================================\")\n    logger.info(\"Derived
          configurations\")\n    logger.info(\"=======================================\")\n    logger.info(f\"job_name:
          {job_name}\")\n    logger.info(f\"namespace: {namespace}\")\n    logger.info(f\"is_remote:
          {is_remote}\")\n    logger.info(f\"minio_url: {cluster_configuration[''minio-url'']}\")\n    logger.info(f\"job_minio_secret:
          {job_minio_secret}\")\n    logger.info(\"inputs (input paths send to job):\")\n    for
          source, target in inputs:\n        logger.info(\n            f\"- {source}
          -> {cluster_configuration[''minio-bucket'']}/{cluster_configuration[''minio-job-folder'']}/{job_name}/{target}\"\n        )\n    logger.info(\"outputs
          (output paths returning from job):\")\n    for source, target in outputs:\n        logger.info(\n            f\"-
          {target} <- {cluster_configuration[''minio-bucket'']}/{cluster_configuration[''minio-job-folder'']}/{job_name}/{source}\"\n        )\n    logger.info(f\"distribution_specification:
          {distribution_specification}\")\n    logger.info(f\"train_command: {train_command}\")\n    logger.info(\"=======================================\")\n\n    additional_job_resources
          = []\n\n    if is_remote:\n        logger.info(\"Using MinIO to sync data
          with a new remote PVC for the job...\")\n        sync_with_minio(cluster_configuration,
          inputs, job_name, is_upload=True)\n        additional_job_resources.append(\n            create_pvc_spec(\n                pvc_name,
          namespace_spec, cluster_configuration[\"access-mode\"], pvc_size\n            )\n        )\n        additional_job_resources.append(\n            create_minio_secret_spec(\n                cluster_configuration,
          job_minio_secret, namespace_spec\n            )\n        )\n    else:\n        logger.info(\n            f\"Pushing
          inputs to local {train_mount} mount as shared with job environment...\"\n        )\n        for
          (source, target) in inputs:\n            clone_path(source, target)\n\n    logger.info(\"Creating
          train job configuration...\")\n    train_job_config = create_train_job_configuration(\n        job_name,\n        namespace_spec,\n        node_selector,\n        base_image,\n        train_command,\n        train_mount,\n        cpus,\n        memory,\n        gpus,\n        init_container,\n        pvc_name,\n        distribution_specification,\n        cluster_configuration[\"minio-url\"],\n        job_minio_secret,\n        tensorboard_s3_address,\n    )\n\n    logger.info(f\"Starting
          train job ''{namespace}.{job_name}''...\")\n    submit_and_monitor_job(\n        api_client,\n        train_job_config,\n        namespace,\n        additional_job_resources,\n    )\n\n    logger.info(\"Receiving
          training outputs...\")\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    if
          is_remote:\n        fetch_command = initialize_fetch_command(\n            cluster_configuration,
          job_name, outputs\n        )\n        fetch_job_name = f\"{job_name}-fetch\"\n\n        logger.info(\"Creating
          fetch job configuration...\")\n        fetch_job_config = create_fetch_job_configuration(\n            fetch_job_name,\n            namespace_spec,\n            base_image,\n            fetch_command,\n            train_mount,\n            job_minio_secret,\n            pvc_name,\n        )\n\n        logger.info(f\"Starting
          fetch job ''{namespace}.{fetch_job_name}''...\")\n        submit_and_monitor_job(api_client,
          fetch_job_config, namespace)\n\n        logger.info(\"Fetching output data
          from MinIO & deleting it afterwards...\")\n        sync_with_minio(\n            cluster_configuration,\n            outputs,\n            job_name,\n            is_upload=False,\n            remove_minio_files=True,\n        )\n\n        logger.info(f\"Deleting
          Job {fetch_job_name}...\")\n        batch_api.delete_namespaced_job(fetch_job_name,
          namespace)\n    else:\n        logger.info(\n            f\"Fetching outputs
          to local {train_mount} mount as shared with job environment...\"\n        )\n        for
          (source, target) in outputs:\n            clone_path(source, target)\n\n    logger.info(f\"Deleting
          Job {job_name}...\")\n    custom_object_api.delete_namespaced_custom_object(\n        train_job_config[\"group\"],\n        train_job_config[\"version\"],\n        namespace,\n        train_job_config[\"plural\"],\n        job_name,\n    )\n\n    logger.info(\"Finished.\")\n\nimport
          json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train Model
          Job'', description=''Trains a model. Once trained, the model is persisted
          to model_dir.'')\n_parser.add_argument(\"--train-dataset-dir\", dest=\"train_dataset_dir\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--validation-dataset-dir\",
          dest=\"validation_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-specification\",
          dest=\"train_specification\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-parameters\",
          dest=\"train_parameters\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-mount\",
          dest=\"train_mount\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
          dest=\"model_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--base-image\",
          dest=\"base_image\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--node-selector\",
          dest=\"node_selector\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pvc-name\",
          dest=\"pvc_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pvc-size\",
          dest=\"pvc_size\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--cpus\",
          dest=\"cpus\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--gpus\",
          dest=\"gpus\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--memory\",
          dest=\"memory\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--tensorboard-s3-address\",
          dest=\"tensorboard_s3_address\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--cluster-configuration-secret\",
          dest=\"cluster_configuration_secret\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--distribution-specification\",
          dest=\"distribution_specification\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = Train_Model_Job(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}},
          "inputs": [{"description": "Path to the directory with training data.",
          "name": "train_dataset_dir", "type": "String"}, {"description": "Path to
          the directory with validation data to be used during training.", "name":
          "validation_dataset_dir", "type": "String"}, {"description": "Training command
          as generated from a Python function using kfp.components.func_to_component_text.",
          "name": "train_specification", "type": "String"}, {"description": "Dictionary
          mapping formal to actual parameters for the training spacification.", "name":
          "train_parameters", "type": "typing.Dict[str, str]"}, {"default": "/train",
          "description": "Optional mounting point for training data of an existing
          PVC. Example: \"/train\".", "name": "train_mount", "optional": true, "type":
          "String"}, {"default": "my-model", "description": "Optional name of the
          model. Must be unique for the targeted namespace and conform Kubernetes
          naming conventions. Example: my-model.", "name": "model_name", "optional":
          true, "type": "String"}, {"default": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest",
          "description": "Optional base image for model training. Example: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest.",
          "name": "base_image", "optional": true, "type": "String"}, {"default": "",
          "description": "Optional node selector for worker nodes. Example: nvidia.com/gpu.product:
          \"Tesla-V100-SXM2-32GB\".", "name": "node_selector", "optional": true, "type":
          "String"}, {"default": "", "description": "Optional name to an existing
          persistent volume claim (pvc). If given, this pvc is mounted into the training
          job. Example: \"music-genre-classification-j4ssf-training-pvc\".", "name":
          "pvc_name", "optional": true, "type": "String"}, {"default": "10Gi", "description":
          "Optional size of the storage during model training. Storage is mounted
          into to the Job based on a persitent volume claim of the given size. Example:
          10Gi.", "name": "pvc_size", "optional": true, "type": "String"}, {"default":
          "", "description": "Optional CPU limit for the job. Leave empty for cluster
          defaults (typically no limit). Example: \"1000m\".", "name": "cpus", "optional":
          true, "type": "String"}, {"default": "0", "description": "Optional number
          of GPUs for the job. Example: 2.", "name": "gpus", "optional": true, "type":
          "Integer"}, {"default": "", "description": "Optional memory limit for the
          job. Leave empty for cluster defaults (typically no limit). Example: \"1Gi\".",
          "name": "memory", "optional": true, "type": "String"}, {"default": "", "description":
          "Optional s3 address where Tensorboard logs shall be stored. Example: \"s3://mlpipeline/tensorboard/my-train-job\".",
          "name": "tensorboard_s3_address", "optional": true, "type": "String"}, {"default":
          "", "description": "Optional secret name configuring a (remote) Kubernetes
          cluster to run the job in and the backing MinIO object store. All secret''s
          data values are optional and appropriate defaults are chosen if not present.
          The secret may provide a suitable kubernetes bearer token, the associated
          namespace, a host, etc. Example: \"remote-power-cluster\".", "name": "cluster_configuration_secret",
          "optional": true, "type": "String"}, {"description": "Optional dictionary
          specifiying the distribution behavior. By default, no distributed training
          is executed, which results in an ordinary Kubernetes Job  for training.
          Otherwise, dictionary entries determine the distribution behavior. The \"distribution_type\"
          entry determines the distribution type: \"Job\" (no distribution; ordinary
          Kubernetes job), \"MPI\" (all-reduce style distribution via Horovod), or
          \"TF\" (parameter-server style distribution via distributed training with
          TensorFlow). Depending on the distribution type, additional dictionary entries
          can be processed. For distributed training jobs, the \"number_of_workers\"
          (e.g., 2) determines the number of worker replicas for training. Individual
          resource limits can be controlled via \"worker_cpus\" (e.g., \"1000m\")
          and \"worker_memory\" (e.g., \"1Gi\"). MPI additionally provides a fine-grained
          control of launcher cpu and memory limits via \"launcher_cpus\" (e.g., \"1000m\")
          and \"launcher_memory\" (e.g., \"1Gi\"). Full example with MPI: {\"distribution_type\":
          \"MPI\", \"number_of_workers\": 2, \"worker_cpus\": \"8\", \"worker_memory\":
          \"32Gi\", \"launcher_cpus\": \"2\", \"launcher_memory\": \"8Gi\"}", "name":
          "distribution_specification", "optional": true, "type": "typing.Dict[str,
          str]"}], "name": "Train Model Job", "outputs": [{"description": "Target
          path where the model will be stored.", "name": "model_dir", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "8362489343cbebfdec127060590fb07123e0b94add6e855c0069a46523a2a265",
          "url": "/home/jovyan/components/model-building/train-model-job/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"base_image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest",
          "cluster_configuration_secret": "{{inputs.parameters.cluster_configuration_secret}}",
          "cpus": "", "gpus": "{{inputs.parameters.training_gpus}}", "memory": "",
          "model_name": "{{inputs.parameters.model_name}}", "node_selector": "{{inputs.parameters.training_node_selector}}",
          "pvc_name": "", "pvc_size": "10Gi", "tensorboard_s3_address": "{{inputs.parameters.monitor-training-tensorboard_s3_address}}",
          "train_mount": "/train", "train_parameters": "{\"epochs\": \"2\", \"model_dir\":
          \"model_dir\", \"seqlen\": \"4\", \"train_dataset_dir\": \"train_dataset_dir\",
          \"validation_dataset_dir\": \"validation_dataset_dir\"}", "train_specification":
          "name: Train model\ninputs:\n- {name: train_dataset_dir, type: String}\n-
          {name: validation_dataset_dir, type: String}\n- {name: epochs, type: Integer,
          default: ''10'', optional: true}\n- {name: seqlen, type: Integer, default:
          ''7'', optional: true}\noutputs:\n- {name: model_dir, type: String}\nimplementation:\n  container:\n    image:
          python:3.7\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf
          \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    -
          |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import
          os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return
          file_path\n\n      def train_model(\n          model_dir,\n          train_dataset_dir,\n          validation_dataset_dir,\n          epochs
          = 10,\n          seqlen = 7,\n      ):\n          import numpy as np\n          import
          os\n          from tensorflow import keras\n          from tensorflow.keras.callbacks
          import (\n              EarlyStopping,\n              ModelCheckpoint,\n              ReduceLROnPlateau,\n              TensorBoard,\n          )\n          from
          tensorflow.keras.layers import Input, LSTM, Dense\n          from tensorflow.keras.metrics
          import (\n              TruePositives,\n              FalsePositives,\n              FalseNegatives,\n              TrueNegatives,\n          )\n\n          def
          load_dataset(path):\n              data = np.load(os.path.join(path, \"data.npz\"),
          allow_pickle=True)\n              x, y = data[\"x\"], data[\"y\"]\n              x
          = np.asarray(x).astype(np.float32)\n              y = np.asarray(y).astype(np.int_)\n              dataset
          = keras.preprocessing.timeseries_dataset_from_array(\n                  x,
          y, sequence_length=seqlen, batch_size=128\n              )\n              return
          dataset\n\n          if not os.path.exists(model_dir):\n              os.makedirs(model_dir)\n\n          train_dataset
          = load_dataset(train_dataset_dir)\n          test_dataset = load_dataset(validation_dataset_dir)\n\n          for
          batch in train_dataset.take(1):\n              input_d, targets = batch\n          print(\"Input
          shape:\", input_d.numpy().shape, \"Target shape:\", targets.numpy().shape)\n\n          input_shape
          = (input_d.shape[1], input_d.shape[2])\n          inputs = Input(shape=input_shape)\n          lstm_in
          = LSTM(100, batch_size=7, return_sequences=True)(inputs)\n          lstm_out
          = LSTM(100, batch_size=7)(lstm_in)\n          outputs = Dense(1, activation=\"sigmoid\")(lstm_out)\n          model
          = keras.Model(inputs=inputs, outputs=outputs)\n\n          metrics = [\n              \"accuracy\",\n              TruePositives(name=\"tp\"),\n              FalsePositives(name=\"fp\"),\n              FalseNegatives(name=\"fn\"),\n              TrueNegatives(name=\"tn\"),\n          ]\n          #
          loss = keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True)\n          model.compile(optimizer=\"adam\",
          loss=\"binary_crossentropy\", metrics=metrics)\n          print(model.summary())\n\n          print(\"Initializing
          training callbacks...\")\n          callbacks = [\n              EarlyStopping(monitor=\"loss\",
          patience=20, verbose=0, mode=\"min\"),\n              ModelCheckpoint(\n                  f\"{model_dir}/best_model.keras\",\n                  monitor=\"loss\",\n                  save_best_only=True,\n                  save_weights_only=True,\n                  mode=\"min\",\n              ),\n              ReduceLROnPlateau(\n                  monitor=\"loss\",\n                  factor=0.1,\n                  patience=7,\n                  verbose=1,\n                  min_delta=0.0001,\n                  mode=\"min\",\n              ),\n              TensorBoard(\n                  log_dir=os.environ[\"TENSORBOARD_S3_ADDRESS\"],\n                  histogram_freq=1,\n              ),\n          ]\n\n          model.fit(\n              train_dataset,\n              epochs=epochs,\n              verbose=3,\n              callbacks=callbacks,\n          )\n\n          results
          = model.evaluate(test_dataset)\n          print(\"Evaluation Loss, Accuracy,
          TP, FP, FN, TN:\", results)\n          TP, FP, FN, TN = results[2:]\n          if
          TP != 0:\n              PR = TP / (FP + TP)\n              RE = TP / (FN
          + TP)\n              print(\"F1 Measure:\", 2 * (PR * RE / (PR + RE)))\n\n          model.save(model_dir)\n\n      import
          argparse\n      _parser = argparse.ArgumentParser(prog=''Train model'',
          description='''')\n      _parser.add_argument(\"--train-dataset-dir\", dest=\"train_dataset_dir\",
          type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--validation-dataset-dir\",
          dest=\"validation_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--epochs\",
          dest=\"epochs\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--seqlen\",
          dest=\"seqlen\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs
          = train_model(**_parsed_args)\n    args:\n    - --train-dataset-dir\n    -
          {inputPath: train_dataset_dir}\n    - --validation-dataset-dir\n    - {inputPath:
          validation_dataset_dir}\n    - if:\n        cond: {isPresent: epochs}\n        then:\n        -
          --epochs\n        - {inputValue: epochs}\n    - if:\n        cond: {isPresent:
          seqlen}\n        then:\n        - --seqlen\n        - {inputValue: seqlen}\n    -
          --model-dir\n    - {outputPath: model_dir}\n"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: upload-model
    container:
      args: [--file-dir, /tmp/inputs/file_dir/data, --project-name, '{{inputs.parameters.model_name}}',
        --minio-url, 'http://minio-service.kubeflow:9000', --minio-secret, mlpipeline-minio-artifact,
        --export-bucket, projects, --model-version, '0', --model-format, onnx, '----output-paths',
        /tmp/outputs/s3_address/data, /tmp/outputs/triton_s3_address/data, /tmp/outputs/model_version/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def upload_model(
            file_dir,
            project_name,
            minio_url = "http://minio-service.kubeflow:9000",
            minio_secret = "mlpipeline-minio-artifact",
            export_bucket = "projects",
            model_version = 0,
            model_format = "onnx",
        ):
            """Uploads a model file to MinIO artifact store."""

            import boto3
            import botocore
            from botocore.client import Config
            from collections import namedtuple
            from kubernetes import client, config
            import logging
            import sys

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )
            logger = logging.getLogger()

            def get_minio_client(minio_secret):
                import base64
                from kubernetes.client.rest import ApiException

                def get_current_namespace():
                    SA_NAMESPACE = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
                    with open(SA_NAMESPACE) as f:
                        return f.read()

                def decode(text):
                    return base64.b64decode(text).decode("utf-8")

                config.load_incluster_config()
                api_client = client.ApiClient()

                try:
                    secret = client.CoreV1Api(api_client).read_namespaced_secret(
                        minio_secret, get_current_namespace()
                    )

                    minio_user = decode(secret.data["accesskey"])
                    minio_pass = decode(secret.data["secretkey"])

                    return boto3.session.Session().resource(
                        service_name="s3",
                        endpoint_url=minio_url,
                        aws_access_key_id=minio_user,
                        aws_secret_access_key=minio_pass,
                        config=Config(signature_version="s3v4"),
                    )
                except ApiException as e:
                    if e.status == 404:
                        logger.error(
                            "Failed to get secret 'mlpipeline-minio-artifact', which is needed for communicating with MinIO!"
                        )
                    raise Exception(e)

            # see: https://stackoverflow.com/a/47565719/2625096
            def bucket_exists(minio_client, bucket):
                try:
                    minio_client.meta.client.head_bucket(Bucket=bucket.name)
                    return True
                except botocore.exceptions.ClientError as e:
                    error_code = int(e.response["Error"]["Code"])
                    if error_code == 403:
                        # Forbidden Access -> Private Bucket
                        return True
                    elif error_code == 404:
                        return False

            # see: https://stackoverflow.com/questions/57957585/how-to-check-if-a-particular-directory-exists-in-s3-bucket-using-python-and-boto
            def folder_exists(minio_client, bucket, path):
                path = path.rstrip("/")
                resp = minio_client.meta.client.list_objects_v2(
                    Bucket=bucket.name, Prefix=path, Delimiter="/", MaxKeys=1
                )
                return "CommonPrefixes" in resp

            def get_versions(minio_client, bucket, path):
                from pathlib import Path

                path = path.rstrip("/")
                response = minio_client.meta.client.list_objects_v2(
                    Bucket=bucket.name, Prefix=path
                )
                versions = set()
                for content in response.get("Contents", []):
                    versions.add(int(Path(content["Key"]).parts[2]))
                return versions

            logger.info(f"Establishing MinIO connection to '{minio_url}'...")
            minio_client = get_minio_client(minio_secret)

            # Create export bucket if it does not yet exist
            bucket = minio_client.Bucket(export_bucket)
            if not bucket_exists(minio_client, bucket):
                logger.info(f"Creating bucket '{export_bucket}'...")
                minio_client.create_bucket(Bucket=bucket.name)

            model_dir_name = "model"
            if model_version == 0:
                logger.info("Determinig model version...")

                if not folder_exists(minio_client, bucket, project_name):
                    logger.info("First-time deployment -> model version: 1")
                    model_version = 1
                else:
                    versions = get_versions(
                        minio_client, bucket, f"{project_name}/{model_dir_name}"
                    )
                    model_version = max(versions) + 1
                    logger.info(f"Model version: {model_version}")

            model_path = f"{model_dir_name}/{model_version}/model.{model_format}"
            s3_address = f"{minio_url}/{export_bucket}/{project_name}"
            triton_s3_address = f"{s3_address}/{model_path}"

            logger.info("Saving onnx file to MinIO...")
            logger.info(f"s3 address: {s3_address}")
            logger.info(f"triton address: {triton_s3_address}")
            # file name in bucket of Minio / for Triton name MUST be model.onnx!
            bucket.upload_file(file_dir, f"{project_name}/{model_path}")

            logger.info("Finished.")
            out_tuple = namedtuple(
                "UploadOutput", ["s3_address", "triton_s3_address", "model_version"]
            )
            return out_tuple(s3_address, triton_s3_address, model_version)

        def _serialize_int(int_value: int) -> str:
            if isinstance(int_value, str):
                return int_value
            if not isinstance(int_value, int):
                raise TypeError('Value "{}" has type "{}" instead of int.'.format(
                    str(int_value), str(type(int_value))))
            return str(int_value)

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Upload model', description='Uploads a model file to MinIO artifact store.')
        _parser.add_argument("--file-dir", dest="file_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--project-name", dest="project_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-url", dest="minio_url", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-secret", dest="minio_secret", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--export-bucket", dest="export_bucket", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-version", dest="model_version", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-format", dest="model_format", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = upload_model(**_parsed_args)

        _output_serializers = [
            _serialize_str,
            _serialize_str,
            _serialize_int,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      parameters:
      - {name: model_name}
      artifacts:
      - {name: convert-model-to-onnx-onnx_model_dir, path: /tmp/inputs/file_dir/data}
    outputs:
      parameters:
      - name: upload-model-model_version
        valueFrom: {path: /tmp/outputs/model_version/data}
      artifacts:
      - {name: upload-model-model_version, path: /tmp/outputs/model_version/data}
      - {name: upload-model-s3_address, path: /tmp/outputs/s3_address/data}
      - {name: upload-model-triton_s3_address, path: /tmp/outputs/triton_s3_address/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Uploads
          a model file to MinIO artifact store.", "implementation": {"container":
          {"args": ["--file-dir", {"inputPath": "file_dir"}, "--project-name", {"inputValue":
          "project_name"}, {"if": {"cond": {"isPresent": "minio_url"}, "then": ["--minio-url",
          {"inputValue": "minio_url"}]}}, {"if": {"cond": {"isPresent": "minio_secret"},
          "then": ["--minio-secret", {"inputValue": "minio_secret"}]}}, {"if": {"cond":
          {"isPresent": "export_bucket"}, "then": ["--export-bucket", {"inputValue":
          "export_bucket"}]}}, {"if": {"cond": {"isPresent": "model_version"}, "then":
          ["--model-version", {"inputValue": "model_version"}]}}, {"if": {"cond":
          {"isPresent": "model_format"}, "then": ["--model-format", {"inputValue":
          "model_format"}]}}, "----output-paths", {"outputPath": "s3_address"}, {"outputPath":
          "triton_s3_address"}, {"outputPath": "model_version"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def upload_model(\n    file_dir,\n    project_name,\n    minio_url
          = \"http://minio-service.kubeflow:9000\",\n    minio_secret = \"mlpipeline-minio-artifact\",\n    export_bucket
          = \"projects\",\n    model_version = 0,\n    model_format = \"onnx\",\n):\n    \"\"\"Uploads
          a model file to MinIO artifact store.\"\"\"\n\n    import boto3\n    import
          botocore\n    from botocore.client import Config\n    from collections import
          namedtuple\n    from kubernetes import client, config\n    import logging\n    import
          sys\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    def
          get_minio_client(minio_secret):\n        import base64\n        from kubernetes.client.rest
          import ApiException\n\n        def get_current_namespace():\n            SA_NAMESPACE
          = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n            with
          open(SA_NAMESPACE) as f:\n                return f.read()\n\n        def
          decode(text):\n            return base64.b64decode(text).decode(\"utf-8\")\n\n        config.load_incluster_config()\n        api_client
          = client.ApiClient()\n\n        try:\n            secret = client.CoreV1Api(api_client).read_namespaced_secret(\n                minio_secret,
          get_current_namespace()\n            )\n\n            minio_user = decode(secret.data[\"accesskey\"])\n            minio_pass
          = decode(secret.data[\"secretkey\"])\n\n            return boto3.session.Session().resource(\n                service_name=\"s3\",\n                endpoint_url=minio_url,\n                aws_access_key_id=minio_user,\n                aws_secret_access_key=minio_pass,\n                config=Config(signature_version=\"s3v4\"),\n            )\n        except
          ApiException as e:\n            if e.status == 404:\n                logger.error(\n                    \"Failed
          to get secret ''mlpipeline-minio-artifact'', which is needed for communicating
          with MinIO!\"\n                )\n            raise Exception(e)\n\n    #
          see: https://stackoverflow.com/a/47565719/2625096\n    def bucket_exists(minio_client,
          bucket):\n        try:\n            minio_client.meta.client.head_bucket(Bucket=bucket.name)\n            return
          True\n        except botocore.exceptions.ClientError as e:\n            error_code
          = int(e.response[\"Error\"][\"Code\"])\n            if error_code == 403:\n                #
          Forbidden Access -> Private Bucket\n                return True\n            elif
          error_code == 404:\n                return False\n\n    # see: https://stackoverflow.com/questions/57957585/how-to-check-if-a-particular-directory-exists-in-s3-bucket-using-python-and-boto\n    def
          folder_exists(minio_client, bucket, path):\n        path = path.rstrip(\"/\")\n        resp
          = minio_client.meta.client.list_objects_v2(\n            Bucket=bucket.name,
          Prefix=path, Delimiter=\"/\", MaxKeys=1\n        )\n        return \"CommonPrefixes\"
          in resp\n\n    def get_versions(minio_client, bucket, path):\n        from
          pathlib import Path\n\n        path = path.rstrip(\"/\")\n        response
          = minio_client.meta.client.list_objects_v2(\n            Bucket=bucket.name,
          Prefix=path\n        )\n        versions = set()\n        for content in
          response.get(\"Contents\", []):\n            versions.add(int(Path(content[\"Key\"]).parts[2]))\n        return
          versions\n\n    logger.info(f\"Establishing MinIO connection to ''{minio_url}''...\")\n    minio_client
          = get_minio_client(minio_secret)\n\n    # Create export bucket if it does
          not yet exist\n    bucket = minio_client.Bucket(export_bucket)\n    if not
          bucket_exists(minio_client, bucket):\n        logger.info(f\"Creating bucket
          ''{export_bucket}''...\")\n        minio_client.create_bucket(Bucket=bucket.name)\n\n    model_dir_name
          = \"model\"\n    if model_version == 0:\n        logger.info(\"Determinig
          model version...\")\n\n        if not folder_exists(minio_client, bucket,
          project_name):\n            logger.info(\"First-time deployment -> model
          version: 1\")\n            model_version = 1\n        else:\n            versions
          = get_versions(\n                minio_client, bucket, f\"{project_name}/{model_dir_name}\"\n            )\n            model_version
          = max(versions) + 1\n            logger.info(f\"Model version: {model_version}\")\n\n    model_path
          = f\"{model_dir_name}/{model_version}/model.{model_format}\"\n    s3_address
          = f\"{minio_url}/{export_bucket}/{project_name}\"\n    triton_s3_address
          = f\"{s3_address}/{model_path}\"\n\n    logger.info(\"Saving onnx file to
          MinIO...\")\n    logger.info(f\"s3 address: {s3_address}\")\n    logger.info(f\"triton
          address: {triton_s3_address}\")\n    # file name in bucket of Minio / for
          Triton name MUST be model.onnx!\n    bucket.upload_file(file_dir, f\"{project_name}/{model_path}\")\n\n    logger.info(\"Finished.\")\n    out_tuple
          = namedtuple(\n        \"UploadOutput\", [\"s3_address\", \"triton_s3_address\",
          \"model_version\"]\n    )\n    return out_tuple(s3_address, triton_s3_address,
          model_version)\n\ndef _serialize_int(int_value: int) -> str:\n    if isinstance(int_value,
          str):\n        return int_value\n    if not isinstance(int_value, int):\n        raise
          TypeError(''Value \"{}\" has type \"{}\" instead of int.''.format(\n            str(int_value),
          str(type(int_value))))\n    return str(int_value)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(\n            str(str_value),
          str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Upload model'', description=''Uploads a
          model file to MinIO artifact store.'')\n_parser.add_argument(\"--file-dir\",
          dest=\"file_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--project-name\",
          dest=\"project_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-url\",
          dest=\"minio_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-secret\",
          dest=\"minio_secret\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--export-bucket\",
          dest=\"export_bucket\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-version\",
          dest=\"model_version\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-format\",
          dest=\"model_format\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = upload_model(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n    _serialize_str,\n    _serialize_int,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}}, "inputs":
          [{"name": "file_dir", "type": "String"}, {"name": "project_name", "type":
          "String"}, {"default": "http://minio-service.kubeflow:9000", "name": "minio_url",
          "optional": true, "type": "String"}, {"default": "mlpipeline-minio-artifact",
          "name": "minio_secret", "optional": true, "type": "String"}, {"default":
          "projects", "name": "export_bucket", "optional": true, "type": "String"},
          {"default": "0", "name": "model_version", "optional": true, "type": "Integer"},
          {"default": "onnx", "name": "model_format", "optional": true, "type": "String"}],
          "name": "Upload model", "outputs": [{"name": "s3_address", "type": "String"},
          {"name": "triton_s3_address", "type": "String"}, {"name": "model_version",
          "type": "Integer"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "c9ec4efa41be502b16b74cc44648b5eb11aef139c8032199c8f08e86464be440", "url":
          "/home/jovyan/components/model-building/upload-model/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"export_bucket": "projects",
          "minio_secret": "mlpipeline-minio-artifact", "minio_url": "http://minio-service.kubeflow:9000",
          "model_format": "onnx", "model_version": "0", "project_name": "{{inputs.parameters.model_name}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  arguments:
    parameters:
    - {name: blackboard}
    - {name: model_name}
    - {name: cluster_configuration_secret}
    - {name: training_gpus}
    - {name: training_node_selector}
  serviceAccountName: pipeline-runner
